---
title: 读书笔记：《动手学深度学习》第 2,3,4 章：预备知识，简单网络的实现
date: 2022-01-22
categories: [科研]
tags: [读书笔记, 《动手学深度学习》, 技术]
img_path: /assets/img/
math: true
---

导师最近很推荐这本书。这是一本把深度学习从头开始讲的技术书，虽然大部分内容是会的，难得找到一本书在代码上讲得系统，看一遍也是很有好处的。我计划是利用寒假时间看一看，整理出一套笔记。

此笔记内容遵从书的编排方式，但内容不局限于书中内容，着重强调代码原理的理解，具体的代码手册我放在另一篇笔记：[Python 代码速查手册]()上。

此系列笔记可以作为 PyTorch 的学习笔记使用。


## 书籍信息 

### [Dive into Deep Learning (PyTorch 版)](https://d2l.ai)
- 作者：亚马逊团队
- 本节内容：第 1 章前言部分又把机器学习、深度学习的基础知识讲了一遍，直接跳过。第 2 章介绍 PyTorch 的预备知识，包括张量的基本操作、自动微分等，第 3、4 章开始搭建简单网络。


------------------------------



# 基本数据结构：Tensor

PyTorch 是深度学习框架，预备知识一定是基本的数据结构、数据操作。张量（Tensor）是 PyTorch 的基本数据结构，它的性质和用法就是数学上的张量，在[这篇文章]()已详细讲述。本书 2.1,2.3,2.4 等节大部分篇幅在讲述 Tensor 的基本用法，这些与 Numpy 也是一致的，就跳过了。 

[这篇文章]()也说，PyTorch 和 Numpy 的基本数据结构本质都是数学上的张量，而且 PyTorch 是基于 Numpy 的，为什么还要自己封装一个 Tensor 类型？第 2 章开头总结的不错，PyTorch 在 Tensor 中融入了深度学习相关的功能：
- 在 GPU 上加速计算（Numpy 只能在 CPU）；
- 储存梯度、计算图等信息，实现自动微分功能。


## 自动微分

[自动微分](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html)（求导）是深度学习框架的主要功能，顾名思义就是给出一个函数后，即可直接算出在某点的导数（梯度）值（注意，并不是求导函数的表达式）。

在书中 2.5 节，是放在张量这里介绍的，正如上述，张量中有个属性（`grad_fn`)是用来存函数（计算图）的。计算图、链式法则是自动微分基于的原理，但也不需要搞明白其底层实现方式，只要会用即可。

假设要求 $$\left.\frac{\operatorname{d} y}{\operatorname{d} \mathbf{x}}\right|_{\mathbf{x}=\mathbf{x}_{0}}
$$，以求 $$y  = 2\mathbf{x}^T \mathbf{x}$$ 在 $$x_0 = (0,1,2,3)^T$$ 点的梯度为例，完整的自动微分过程如下：

1. 定义 $$x_0$$：将 $$x_0$$ 点的值以 tensor 的形式赋给变量 `x`
```
    x = torch.arange(4.0)
```
2. 开启求导模式：把 tensor `x` 的 `requires_grad_` 属性设为 True
```   
    x.requires_grad_(True)
```  
3. 定义被求导函数 $$y$$：将含 `x` 的 torch 表达式赋给变量 `y` （此时 tensor `y` 存放了计算图）
```
    y = 2 * torch.dot(x, x)
```
4. 求导：调用 `y` 的 `backward` 方法，导数值存放在 `x` 的 `grad` 属性中（与 `x` 维数相同）
```
    y.backward()
```

注意点：
- y 必须是标量，而 x 可以是向量；
- 存放求导结果的 `grad` 属性是累加的：第一次求导前默认为0，求导后将结果叠加到 0 上，第二次求导后会叠加到第一次的结果上。所以如需反复求导一定要**清零**；
- 被求导函数可以额外打包成一个 Python 函数赋给 `y`，只要函数里面用的都是 torch 的表达式；
- 构建计算图极容易粗心，一定注意好求导模式的开关，不在不该的地方引入计算图。除了修改 `requires_grad_` 属性，还可以：
    - 全局地关闭求导模式，用以下代码包裹：
    ```
        with torch.no_grad():
    ```
    - 分离变量：即去掉 `grad_fn` 存放的计算图，只保留 tensor 值。以下代码将 `y` 分离成 `u`：
    ```
        u = y.detach()
    ```   


## 深度学习模型的 Pipeline


本书在第 3、4 章介绍了三个简单网络，每个模型都分从头开始实现和简单实现两部分，有点啰嗦。





以下是几个简单网络的实现方式。








