---
title: 科研笔记：IIRC 场景的模型与训练算法
date: 2022-04-27
categories: [科研]
tags: [科研笔记, 持续学习]
img_path: /assets/img/
math: true
---


近期我计划 follow [IIRC](https://pengxiang-wang.github.io/posts/papernotes_IIRC) 这篇论文的工作。该论文把粗细标签引入到持续学习场景中，允许标签有粗细粒度之分，且要求只能越来越细，把类别增量学习推广到了他们提出的 IIRC 场景。论文提出新场景后，只把现有的模型套用到新场景（把新场景看成普通持续学习）对比效果，没有提出模型与训练算法。我的目标就是设计针对 IIRC 场景的模型与训练算法。



# 算法框架

这个 IIRC 场景很宽泛，新来的任务包含的类
- 可以是旧任务的子类；
- 还可以是与其并列的超类。
此论文虽然关注前者，主要目的是解决越来越细这种关系，但也包含了并列关系。

这两种情况中，后者已有大量的持续学习算法，而前者少之又少，是重点关注对象。为解决前者，我们认为，提出一个能一次性解决并列关系和层级关系的模型与算法是比较困难的。我将单独考虑层级关系的场景，即考虑 **IIRC 的特殊情况**——越来越细，即新任务是上一个任务的子类，并提出适合此场景的模型与算法。


这个问题形式上看和普通的持续学习没什么区别：任务序列 $$\tau_1, \cdots, \tau_t$$（设只来一个类），设计模型 $$f$$ 使其可以正确分类；对新任务 $$\tau_{t+1}$$，设计训练算法 $$\mathcal{A}_{sub}$$，将模型由 $$f_t$$ 更新成 $$f_{t+1}$$。

> 我们知道的“$$\tau_{t+1}$$ 是 $$\tau_{t}$$ 的子类”这个信息，不会显式地体现在任何数据上，我们只是知道数据有这种特点，并针对这种数据有针对性地设计算法。
{: .prompt-tip }


有了 $$\mathcal{A}_{sub}$$ 后，**IIRC 场景的训练算法框架**就是：判断新任务是不是某个旧任务的子类，如果是，使用 $$\mathcal{A}_{sub}$$ 更新模型；否则，使用普通的持续学习算法 $$\mathcal{A}$$ 更新模型。

之前在 IIRC 论文笔记中强调过，训练阶段不知道新任务是不是某个旧任务的子类。如果把这些并列关系和层级关系看作一片森林的话，这意味着这个森林长什么样子在训练阶段算法不应该知道。所以，上述“判断”步骤不是简单地看看新任务在森林中是不是根节点（因为压根不知道森林），必须从数据本身出发，通过某种算法 $$\mathcal{GC}$$（我称为粒度判别器，granularity discriminator）去预测它是不是子类。



# 设计算法 $$\mathcal{A}_{sub}$$

我没有其他新颖的思路，先从持续学习的三大类方法中寻找灵感。设计 $$\mathcal{A}_{sub}$$ 在前两类方法中就是设计损失函数 $$\mathcal{L}^{REVIEW}$$。

## 重演方法

记忆中存储的是过去任务的重演数据。普通的持续学习场景下，不同任务的数据应是没什么关联的，因此这些重演方法将其一视同仁。而在越来越细的场景下，训练到任务 $$\tau_{t}$$ 时，之前任务数据 $$\tau_1, \cdots, \tau_{t-1}$$ 会有与 $$\tau_{t}$$ 相似的数据。想法是：在使用重演数据时，从之前的任务中提取这些相似的数据，更着重或者有区别地利用它们。

例如一个做法是，设有 3 个越来越细的任务，其数据为 $$D_{1}, D_2, D_3$$，训练到 $$\tau_3$$ 时，设 $$D_1, D_2$$ 构造的重演数据为 $$R_1, R_2$$，从中提取出的与 $$\tau_3$$ 相似的数据为 $$R_1^3, R_2^3$$。重演方法的损失函数 $$\mathcal{L}^{FINETUNE} +  \mathcal{L}^{REVIEW}$$ 是用重演数据构造的。普通的持续学习算法：
- $$\mathcal{L}^{FINETUNE}$$ 用 $$D_3$$；
- $$\mathcal{L}^{REVIEW}$$ 用 $$R_1, R_2$$。

这里将其调整为
 - $$\mathcal{L}^{FINETUNE}$$ 用 $$D_3, R_1^3, R_2^3$$；
 - $$\mathcal{L}^{REVIEW}$$ 用 $$R_1, R_2$$（也可从中挖去 $$ R_1^3, R_2^3$$）。 

宏观上说，这种从利用数据下手的思路关注两个方面：
- 如何提取相似的数据；
- 如何分配到损失函数中。
前者可以设计按最近邻抽取数据，或者数据增强等生成数据的方法；后者的工作就主要是设计针对不同数据细化的损失函数。


经讨论，此方法的严重问题是重演数据量是很少的，上面这些调整的影响是微乎其微的。一个解决方法是通过数据增强增大数据量，但这样做很容易喧宾夺主，冲淡新任务的数据。



## 正则化方法

正则化方法经常对梯度下手。参考的工作是 [Orthogonal Gradient Descent](https://proceedings.mlr.press/v108/farajtabar20a/farajtabar20a.pdf)。



## 网络结构方法

如果要对网络结构下手，一个最直观的想法是，让网络也具备越来越细的层级关系。

最简单的类比是将一个个小网络也以树形结构相连，新来的类如果是细类，则构造一个网络挂到父类网络下面，如果是粗类则并列地扩充网络（参见[论文]()）。这种方式的缺点是很难操作，而且网络规模与见过的所有类数成正比，空间代价很大。

另一个方式从一个大网络出发，让更新参数的范围越来越小。它固定了空间，但缺点是不知道一开始该取多大。

我采用二者结合的方式，这样能中和掉各自的缺点：粗类并列地扩充网络，细类则在粗类网络下划细参数范围，如下图。

![1](IIRC_my_architecture_method.pdf)

要点：
- 由于深度神经网络上面的层通常提取更抽象、笼统的特征，所以应尽量把下层网络划给细类；暂时采用简单的固定比例，例如图中类 4 划为下面一半；
- 由于同一个粗类可能有多个细类，细类部分参数的宽度上不能占满；但是又不知道有多少个细类，不能采用均分的方式；又由于细类之间也有可能重叠；我采用随机地选择固定宽度的方法（随机后固定）；
- 粗类如果把下层网络划给细类，则粗类和细类输出是类似的（子空间关系）；考虑使用还没开始分细类的那一层的输出，将其与最后一层输出结合使用。



还有很多其他方式。论文[HMC](https://openaccess.thecvf.com/content/CVPR2022/papers/Chen_Label_Relation_Graphs_Enhanced_Hierarchical_Residual_Network_for_Hierarchical_Multi-Granularity_CVPR_2022_paper.pdf)提供了另一种在网络中引入层级关系的方法。这个模型网络规模与见过的层级数（树的高度）成正比。



# 其他算法框架


上面这个算法框架很直观，它将层次关系和并列关系分开考虑，其中包含的三部分算法 $$\mathcal{GC},\mathcal{A}_{sub},\mathcal{A}$$ 也是相对独立的。这样其实是有很大问题的，因为 $$\mathcal{GC}$$ 和 $$\mathcal{A}_{sub},\mathcal{A}$$ 是串联的，后者要使用前者输出的结果，相当于前者一直为后者“背书”。如果前者判定错误，后者一切都无从谈起。直观上，这种结构会极大降低算法的鲁棒性。

因此，下一步应当考虑打破这种框架。为解决算法串联导致的鲁棒性问题，最好的办法当然是不串联，将粒度判别和更新模型两部分深度融合为一个 Black-box 模型，但这件事是比较困难的，先放在此，以后再考虑。




# 动手实现

IIRC 的代码已经公开在 GitHub，我已研读过，见另一篇[代码笔记]()。最好的方式是重构、改写此代码，构造出我的特殊化场景。


<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>


（可以看成一片森林）。这种层次信息是这个场景新出现的重要信息，模型现在只是知道有这种信息可以利用，但并不知道具体的。我们不能人工地告诉训练阶段，这是作弊。所以设计针对此场景的模型，关键是去学这种信息。ZGZ 师兄正在研究这个问题并与我们讨论。