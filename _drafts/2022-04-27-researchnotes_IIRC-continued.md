---
title: 科研笔记：越来越细的持续学习
date: 2022-04-27
categories: [科研]
tags: [科研笔记, 持续学习]
img_path: /assets/img/
math: true
---


这是导师提给我的想法，基于 [IIRC](https://pengxiang-wang.github.io/posts/papernotes_IIRC) 这篇论文。该论文把粗细标签引入到持续学习场景中，允许标签有粗细粒度之分，且要求只能越来越细，把类别增量学习推广到了他们提出的 IIRC 场景。

但是此论文工作较粗略：首先，他们的 IIRC 场景非常 general，新来的任务除了可以是旧任务的子类，还可以是与其并列的超类（参见[论文笔记](https://pengxiang-wang.github.io/posts/papernotes_IIRC)）。此论文初衷是解决越来越细这种关系，可是提出了一个更 general 的模型，也包含了并列关系。其次，论文提了新场景后，只把现有的模型套用到新场景下对比效果，没有提出针对新场景的模型。

我的工作应是，把这个 general 的场景进一步特殊化——只考虑越来越细，并提出适合此场景的模型。

## 场景定义：IIRC 特殊化

对于任务序列 $$\tau_1, \cdots, \tau_N$$，每个任务 $$\tau_k$$ 暂时简化成只来一个类。现在只考虑越来越细，即 $$\tau_t$$ 是 $$\tau_{t-1}$$ 的子类。

插播一个事情：IIRC 场景中一个很关键的问题是，训练阶段并不知道新来的类是超类还是之前超类的子类（可以看成一片森林）。这种层次信息是这个场景新出现的重要信息，模型现在只是知道有这种信息可以利用，但并不知道具体的。我们不能人工地告诉训练阶段，这是作弊。所以设计针对此场景的模型，关键是去学这种信息。ZGZ 师兄正在研究这个问题并与我们讨论。

我的场景不需要考虑这个问题。如果我和师兄分别解决了各自的问题，那么合起来：新来任务后，通过师兄的算法判断层次，如果是之前超类的子类，则用我的算法处理此数据；如果是并列的超类，则按普通的持续学习算法处理。


## 关于模型的想法

我先从持续学习的三大类方法中寻找灵感。*但切记，提想法不应该框死在这三类方法中。*

### 重演方法

记忆中存储的是过去任务的重演数据。普通的场景下，不同任务的数据应是没什么关联的。而在越来越细的场景下，训练到任务 $$\tau_{t}$$ 时，之前任务数据 $$\tau_1, \cdots, \tau_{t-1}$$ 会有与 $$\tau_{t}$$ 相似的数据。想法是：在使用重演数据时，从之前的任务中提取这些相似的数据，更着重或者有区别地利用它们。

具体来说，记有3个越来越细的任务，其数据为 $$D_{1}, D_2, D_3$$，训练到 $$\tau_3$$ 时，设 $$D_1, D_2$$ 构造的重演数据为 $$R_1, R_2$$，从中提取出的与 $$\tau_3$$ 相似的数据为 $$R_1^3, R_2^3$$。

 持续学习优化目标是 $$\mathcal{L}^{FINETUNE} +  \mathcal{L}^{REVIEW}$$，在重演方法中都是用数据构造的，前者是用 $$D_3$$，后者 $$R_1, R_2$$。现将其改为前者 $$D_3, R_1^3, R_2^3$$，后者 $$R_1, R_2$$（也可从中挖去 $$ R_1^3, R_2^3$$）。 





更有代表性的数据，



### 正则化方法：


### 结构方法：

一个很直观的想法是，对一个大网络，参数更新的集合也越来越细。


受一篇不断增加结构的论文启发，此论文每遇到一个新任务时，在网络右端扩充一列神经元，将前面的参数固定，训练新增的参数。

此方法是纵向地扩充，更适用于。




## 动手实现

IIRC 的代码已经公开在 GitHub，我已研读过，见另一篇[代码笔记]()。最好的方式是重构、改写此代码，构造出我的特殊化场景。




