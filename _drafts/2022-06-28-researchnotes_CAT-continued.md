---
title: 科研笔记：CAT 改进方向
date: 2022-06-28
categories: [科研]
tags: [论文笔记, 持续学习]
img_path: /assets/img/
math: true
---


## 论文信息 

### [Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks](https://proceedings.neurips.cc/paper/2020/file/d7488039246a405baf6a7cbc3613a56f-Paper.pdf)


- 会议：NIPS 2020
- 作者：
  - [Zixuan Ke](https://underline.io/speakers/97701-zixuan-ke)：伊利诺伊大学芝加哥分校，博士生，后者的学生。
  - [Bing Liu](https://www.cs.uic.edu/~liub/)：伊利诺伊大学芝加哥分校，教授。他是《终身机器学习》的作者，我有系列[读书笔记](https://pengxiang-wang.github.io/tags/终身机器学习/)。
  - [Xingchang Huang](https://people.mpi-inf.mpg.de/~xhuang/)：苏黎世联邦理工大学，博士生。
- 内容：提出了一个持续学习模型。它不只关注不相似任务的灾难性遗忘，还关注相似任务的知识迁移。


--------------

# CAT 改进

## 一、质疑 if...else... 判断任务相似度的模式

1. if...else...的方式肯定不好，是非黑即白的。它在拿自己的判断为后面的一切做担保，万一判断错误，全盘皆输。
2. if...else...判断中，断言了是否一个任务与前面的任务是否相似。但是后面有很多指标能反映相似程度，如学到 mask 的重合程度。if...else...的断言可能与 mask 重合程度反映的相似程度不一致！这是一个矛盾。

**解决方法**：

1. 修正：就是遇到矛盾时选择推翻之前的断言，有点多此一举；
2. 推倒重来：就是干脆不用之前那种比较判断，直接换一个指标作为相似度标准，例如 mask 重合程度。可能的做法：都先统一只训练 KB + TM，观察 mask 重合程度，如果大于某个阈值再去训练 KTA。

## 二、质疑比较两模型效果判断相似度的公平性

即使 if...else... 的模式合理，文章判断相似度的方法也只是单纯比较两个模型效果大小：迁移模型、参考模型。

我的感觉是，肯定是自己家训出来的（参考模型）效果好的可能性大一些，因为毕竟迁移模型用的是其他任务的知识。单纯地比较效果数值大小公平吗？这样天平会不会更向参考模型好——即不迁移知识上倾斜？

作者有无 “有KTA这部分，但故意不用” 的嫌疑？需要看他的实验：

- 看在文章机制下相似/不相似的比例是否平衡，统计一下；
- 如果比例无严重不平衡，比例与效果有无关系（例如 KTA 用的越多，效果越差，会说明问题）；
- 三是手动地调整比例，修改 if...else... 语句（极端情况：只相似或不相似），观察规律；

**解决方法**：修改判断相似度的方法，例如简单地引入一个天平系数的参数，调控天平平衡的条件，使其公平。这个参数可以是：

- 超参数，在训练前固定下来，常数或者与当前任务数相关的量；
- 可学习的参数。

## 三、（小问题）测试与训练不一致

测试时，如果 $$t$$ 与之前有的任务相似，文章的做法是 $$x$$ 通过 KB 在 $$\tau_{sim}$$ 任务上的 mask 得到一系列特征 $$h_{mask}^{(i_{sim})}$$，让它们过 KTA 和后面的分类头，得到分类结果，没有用到 $$h_{mask}^{(t)}$$ 过 KB 后面的分类头。但在训练时，无论 $$t$$ 与之前有无相似，都会训练 $$t$$ 的任务 mask。这就导致了训练与测试不一致的问题。

有两种解决办法：

- 让训练与测试一致：即修改损失函数 $$\frac{1}{N_{t}} \sum_{j=1}^{N_{t}} \mathcal{L}\left(f_{m a s k}\left(x_{j}^{(t)} ; \theta_{m a s k}\right), y_{j}^{(t)}\right)+\frac{1}{N_{t}} \sum_{j=1}^{N_{t}} \mathcal{L}\left(f_{K T A}\left(x_{j}^{(t)} ; \theta_{K T A}\right), y_{j}^{(t)}\right)$$，如果 $$t$$ 与之前有的任务相似，则去掉第一项。这样是不靠谱的，因为这一项必须训练。
- 让测试与训练一致：测试时，如果 $$t$$ 与之前有的任务相似，不仅用 $$h_{mask}^{(i_{sim})}$$，也用上 $$h_{mask}^{(t)}$$。需要根据训练时损失函数推导出来测试时改如何融合二者的信息。

## 四、预留的 Mask 空间不够用

持续学习的特点是不知道未来有几个任务。文章的 Mask 机制是为每个任务选择部分神经元，根据任务是否相似来决定 Mask 是否重叠。如果不采用动态扩充网络的方式，网络神经元数量是固定的，随着不相似任务（Mask 不能重叠）的增加，新任务 Mask 的可选范围会越来越小，直到没有。所以，每个任务应该 Mask 掉多少神经元合适需要确定。例如，第 1 个任务可以 Mask 所有的神经元，那么训练过程会学出 Mask 多少呢？需不需要人为地限制？限制的量如何规定？

即使是无论任务是否相似都允许重叠，空间不够用的问题也是存在的：后面的任务即使与前面所有任务都不相似，也会因为 Mask 空间都被前面任务占满，而被迫于前面某些任务相似。

现在的想法是无法指望从逻辑上彻底解决这一问题，但是可以通过一些手段缓解这一问题。

例如改变现有的为每个任务设定 $$\tau_{sim}, \tau_{dis}$$ 的模式，改为：构造一个不相似任务的集合，每来一个新任务判断与集合中任务是否相似，若相似，则 Mask 直接选用相似的任务；若都不相似，则加入集合。这样其实杜绝掉了一种情况：A 与 B 相似，B 与 C 不相似，但 A 与 C 相似。（可以画 3 个任务看看）





# 应用到 IIRC 场景中

文章的思路可以模仿到 IIRC 场景中。

IIRC 为任务之间引入了层级关系。层级关系其实是相似关系的一种特殊情况，大方向是：将任务相似度的概念细到层级性的概念，将文章判断相似度的方法细化为判断父子关系的方法。模型方面，KB + TM 的机制可以完美用上，而 KTA 需要作一些修改，以适应层级关系。

问题：和相似关系不一样的地方是，每一个子类只能有一个父类。