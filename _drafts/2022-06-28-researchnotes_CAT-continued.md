---
title: 论文笔记：Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks
date: 2022-06-28
categories: [科研]
tags: [论文笔记, 持续学习]
img_path: /assets/img/
math: true
---


## 论文信息 



### [Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks](https://proceedings.neurips.cc/paper/2020/file/d7488039246a405baf6a7cbc3613a56f-Paper.pdf)


- 会议：NIPS 2020
- 作者：
  - [Zixuan Ke](https://underline.io/speakers/97701-zixuan-ke)：伊利诺伊大学芝加哥分校，博士生，后者的学生。
  - [Bing Liu](https://www.cs.uic.edu/~liub/)：伊利诺伊大学芝加哥分校，教授。他是《终身机器学习》的作者，我有系列[读书笔记](https://pengxiang-wang.github.io/tags/终身机器学习/)。
  - [Xingchang Huang](https://people.mpi-inf.mpg.de/~xhuang/)：苏黎世联邦理工大学，博士生。
- 内容：提出了一个持续学习模型。它不只关注不相似任务的灾难性遗忘，还关注相似任务的知识迁移。



--------------

# CAT 改进的地方

## 迁移模型和参考模型的比较机制合理吗？

文章只是单纯比较效果大小。

我的感觉是，肯定是自己家训出来的（参考模型）效果好的可能性大一些，因为毕竟迁移模型用的是其他任务的知识。单纯地比较效果数值大小公平吗？这样天平会不会更向参考模型好——即不迁移知识上倾斜？

作者有无“有KTA这部分，但设计倾斜机制故意不用”的嫌疑？需要看他的实验。

- 一是看这个if...else...选择的比例是否平衡，统计一下；
- 如果比例无严重不平衡，二看比例与效果有无关系。（假如KTA 用的越多，效果越差，说明？）；
- 三是手动地调低天平，比如让不等式比较加个系数，这可能会带来改进？

引入参数：天平系数，调控天平平衡的条件？这个参数可以是超参数，在训练前固定下来。也可以设计成可学习的参数。

在论文笔记中可以看到，第一次相似度比较是 1 与 0 比，随着任务的增多，就是 t 与 0,...,t-1 比。旧有知识是增多的，这样固定地比较迁移模型和参考模型效果是否不公平？如果我们要手动给一个系数的话，我认为要让它随 t 增大越来越向迁移知识倾斜。



## if...else...判断完了再训练好吗？可能出现的悖论：真的相似吗？


if...else...的方式肯定不好。它在拿自己的判断为后面的一切做担保，万一判断错误，全盘皆输。

例如本文的if...else...判断中，断言了是否一个任务与前面的任务是否相似。但是后面有很多指标能反映相似程度，如学到 Mask 的重合程度。本文的做法是，先断言不相似——则强制让其不重合。断言相似，则顺其自然，按照作者的想象，可能会重合。但是，如果此时就是不重合怎么办？岂不是反映了不相似？

解决这个问题，要么修正主义，要么推倒重来。修正就是遇到这种悖论时选择推翻之前的断言；推倒重来就是干脆不用之前那种比较判断，直接顺其自然训练，拿这个 Mask 重合程度做标准：比如都先统一只训练 KB + TM，看重合程度，如果大于某个阈值再去训练 KTA。



## KTA task embedding 会不会覆盖？

例如，任务1与0相似，则训练得到 $$e_{KTA}^{(0)}$$；如果后面有任务2 也与0 相似，$$e_{KTA}^{(0)}$$怎么处理？覆盖掉吗？


## 测试过程中的问题

测试时，如果 $$t$$ 与之前有的任务相似，文章的做法是 $$x$$ 通过 KB 在 $$\tau_{sim}$$ 任务上的 mask 得到一系列特征 $$h_{mask}^{i_{sim}}$$，让它们过 KTA 和后面的分类头，得到分类结果。

我们记得在训练时，这种情况仍然能得到 $$e^{(1)}$$ 即 task mask 的。如果$$t$$ 与之前任务无任何相似，是： $$x$$ 通过 $$e^{(t)}$$ 生成的该任务的 mask，得到最后一层特征 $$h_{mask}^{(t)}$$，再通过 KB 后面的分类头，得到分类结果。

问：如果不是无任何相似，就不能用这个$$h_{mask}^{(t)}$$了吗？它就没有用了吗？



## 有关相似关系的讨论 X

注：我考虑错了。持续学习不应该考虑这些。

在本文中，任务间的相似关系非常奇怪。如果每次判断相似性后，画图来看，它是一个不断引入新节点的有向图，有向边只能从新节点出发指向旧节点。即受限的有向图。

一般来说，我们希望相似关系具有对称性，此场景下不要求传递性。引入对称性变成无向图，引入传递性变成等价类。但是本文自始至终只用到了受限的有向关系。

这样的问题是，不能保证 permutation invariant。





# 能不能和应用到 IIRC 场景中



