---
title: 学习笔记：循环神经网络
date: 2021-04-20
categories: [科研]
tags: [课程笔记]
img_path: /assets/img/
math: true
---

本笔记将系统整理学过的循环神经网络（RNN）的知识。写笔记时我正参加信科那边的《神经网络与深度学习》等课程。


# 可变长数据的学习问题

机器学习任务中有很多可变数据要出来

关键区别，输入输出都可以是**可变**长度（即序列数据）。主要设计不同时序之间如何连接（梯度流）。几种情况：一对多 多对一 多雨多 
structural learning


# RNN 的定义与机制

普通的固定输入输出的网络解决不了，需要引入新的机制——时序。 

其实是一张可以向右无限伸展的网络，其中的参数会复制，所有时间上的 W 是共享的。。我们用的时候只是取一部分输入输出。

Loss 是各个可变输出的总和。

RNN cell

Sequence to Sequence， Encoder Decoder 


对 Seq to Seq，RNN 的训练方式一般是自监督的，不带标签的，随着时序不断地训练，拖尾巴。即t就是一个训练的一步。

如果所有训练数据看作一个序列，尾巴会越拖越长，每次都要前向和反响一整个尾巴。计算和空间都是不可取的。可不可以将数据砍段？不太好，数据有头尾，头部总有一些损失，无法与前面接上，还要处理好砍的位置。
反向传播 through time 方法：截断（Truncated）固定长度。
怎么搞 batch。这类似于随机梯度下降的随机思想。近似地估计整个梯度。




对RNN 的任务允许多种输出，Softmax不必选最大值的，也可以把 Softmax 得分当作概率值采样。

测试时先给一些文本的前缀， 让他启动。




随着时间t 梯度消失——用LSTM，梯度爆炸——gradient clipping


LSTM 就是把 cell 重新设计了一些，为了。类似于ResNet


## Multilayer RNN 双向 RNN
 类似加深网络


 能不能让网络有分叉等图结构等流动？和GNN 什么关系？

## lSTM


## GRU


# 通用流行的 RNN 架构





本章讲历史上流行并取得巨大成就的 RNN 架构，一般是通用地用在图像分类的，其他针对具体 CV 任务的放在 CV 笔记中写。

第一个引起关注的是 1998 年的 LeNet，它是一个很简单的网络

## Attention 机制


## Transformer

## BERT







之后就是深度学习时代的大型 CNN 网络了，主要成功的有 AlexNet，VGG，GoogLeNet，ResNet。由于细节太多，不打算一个模型分一节来讲其所有细节，而是放在一起作各种对比，并就其中的几个重要设计思想进行讲解。


ILSVRC


## 对比：参数数量



## 对比：结构

去掉全览阶层



### GoogLeNet 的 Inception 层

大小不变，channel  深度增加/

1\times 1 减少深度 bottleneck

### 残差块

残差块可以是多层

残差块之间是相互堆叠的。

为梯度反向传播提供高速公路，更容易训练和收敛。

梯度流（gradient flow）

## 实践经验

如果想用这些架构做自己模型的 backbone，通常无脑选择 ResNet。因为