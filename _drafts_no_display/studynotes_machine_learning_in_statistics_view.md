---
title: 统计视角下的机器学习
date: 2021-04-20
categories: [科研]
tags: [课程笔记]
img_path: /assets/img/
math: true
---

注意，本文对向量维数非常敏感。粗体一定是向量，非粗一定是标量。

在一般人眼里，对深度学习、机器学习是按照**模型视角**（暂且叫这个名字好了）来理解的：机器学习就是建立一个有输入有输出的模型，然后利用数据依照训练算法更新模型。这种任务通常是监督学习。

但我们经常会在书籍、论文里见到很多用**统计视角**来描述机器学习的，里面通篇的概率统计式子，什么条件概率啊、期望啊、似然啊。

我不是统计出身，曾经看到这些东西就头大然后跳过，觉得概率公式只是统计专业习惯写法，按模型视角理解完全够用，对我来说不过显得高级点而已。后来接触了机器学习更深的领域，如各种无监督学习，才知道有很多别的任务本身就是用概率建模的，不得不学习统计视角。统计视角是一个更高的角度，可以囊括包括模型视角的模型在内的所有机器学习内容，且对机器学习所做的事情有更深的理解。

先说结论，无论是哪种视角，都是表示、学习、解决问题三步。



本文想总结一下如何在统计视角下看机器学习。希望以后在机器学习内容中看到概率统计式子时彻底不发怵。对于之前我理解不够的机器学习模型（尤其是涉及概率统计的），也会详细讲解。本文推荐参考书：李航《统计学习方法》，此书是能站在统计视角就站，非常地统计。

站在模型视角，决策函数。

站在统计视角，主要是对概率分布进行建模（假设），估计。这些模型叫概率模型。

概率模型也有超参数一说，比如规模。。。

一下无特别说明，都是参数估计，而不是非参数估计。

# 判别模型

监督学习的学习、表示、推断




以回归问题为例，一般所说的监督学习在模型视角下是这样子的：对训练数据 $$\{(\mathbf{x}_i,y_i)\}_{i=1}^N$$，寻找一个模型 $$y=f_\theta(\mathbf{x})$$ 使得它尽量拟合这些数据，即最小化损失函数 $$L(\theta) = \frac1N \sum_{i=1}^N l(f(\mathbf{x}_i), y_i)$$，通常选用平方损失 $$l(\hat{y}, y) = \frac12 |\hat{y}-y|^2$$。

在统计视角下（这些内容是任何一本回归分析教材的基础知识）：$$\mathbf{x},y$$ 是两个随机变量，假设 $$y$$ 在 $$\mathbf{x}$$ 给定时（即条件分布 $$p(y|\mathbf{x})$$）服从**正态分布** $$N(f_\theta(\mathbf{x}),\sigma^2)$$，该正态分布有两个参数 $$\theta, \sigma^2$$，对其进行参数估计（主要要估计的参数是 $$\theta$$），使用训练数据作为样本。所以，统计视角下机器学习做的就是**对因变量对自变量的条件分布作参数估计**。参数估计方法有很多种（见数理统计知识），常用**极大似然估计**（MLE）。得到条件分布 $$p(y|\mathbf{x})$$ 的具体形式后，给定 $$\mathbf{x}$$ 对 $$y$$ 值的预测应当依据此分布，最合理的方式是取最可能的值（因为参数估计是用极大似然估计得到的，测试阶段应与训练阶段吻合）：对此分布 $$N(f_\theta(\mathbf{x}),\sigma^2)$$ 来说概率密度最大点就是均值 $$f_\theta$$。

**两种视角是等价的**，因为可以证明最大化正态分布的对数似然函数与最小化平方损失这两个优化问题是等价的。只需把两个目标函数化一化就出来了，无论回归分析还是机器学习的书籍都非常喜欢把这个当作习题，随便搜搜就能找到答案。

分类问题是一个道理，对于 C 分类问题：在模型视角下，模型 $$\mathbf{y} = f_\theta(\mathbf{x})$$ 多了一个要求，最后一层必须复合上 Softmax 函数使得输出的 $$\mathbf{y}$$ 是 $$[0,1]$$ 上和为 1 的（代表 C 个类得分值），损失函数选用交叉熵损失 $$l(\hat{\mathbf{y}}, \mathbf{y}) = -\sum_{k=1}^C y_k \log(\hat{y}_k)$$。在统计视角下，$$y$$是一个离散型随机变量，取值为 $$0,\cdots, k-1$$，它就是从几个类中按一定概率取出一个，所以应当假设条件分布（是离散型分布列，以概率函数 $$P$$ 而非密度 $$p$$ 表示）$$P(y|\mathbf{x})$$ 服从**重复次数为 1 的多项分布 MN(1, \mathbf{\pi})**，其中参数为各个类取值概率 $$\mathbf{\pi}$$，即 $$f_\theta(\mathbf{x})$$ 的各元素。也可以证明两种视角是等价的，即最大化多项分布的对数似然函数与最小化交叉熵损失是等价的。

> 交叉熵损失不像平方损失，它不够直观，人是不太容易设计出来的，其实更像是在统计视角下推导出来的。平方损失可能。
{: .prompt-info }

总结来看，以上机器学习方法在统计视角下都是对**因变量对自变量的条件分布 $$p(y|\mathbf{x})$$**建模，这种模型称为**判别模型**（discriminative model）。对条件分布 $$p(y|\mathbf{x})$$ 不同的建模，得到不同的判别模型。它只适用于**监督学习**任务（因为有 x 有 y）。

不要害怕，条件分布是指固定了条件的分布，所以 $$p(y|\mathbf{x})$$ 形式上就是个 $$p(y)$$，只不过这个分布概率函数中出现了 x 罢了。对于监督学习问题，y 通常是一维的，函数形式不需要担心（用常用的分布），只需对其参数作合理的建模即可，一般来说可以归杰为一个函数 $$f_\theta$$，$$p(y|\mathbf{x})$$ 按照上文方式由它构造，例：

- 线性回归：$$f_\theta$$ 是线性函数 $$\mathbf{w}^T \mathbf{x} + b$$，用于回归任务；
- 逻辑回归 / 多项逻辑回归（Softmax 回归）：$$f_\theta$$ 是线性函数复合 Softmax，用于分类任务；
- 深度学习：$$f_\theta$$ 是深度神经网络，用于分类、回归都可以。

上面也看到了，很巧的是这些就是模型视角下的决策函数。








也有对条件分布 $$p(y|\mathbf{x})$$ 直接建模的例子，如最大熵模型。

## 最大熵模型（对数线性模型）

**最大熵模型**又称**对数线性模型**，用于解决多分类任务。它对条件分布建立的模型是：

$$p(y|\mathbf{x})=\frac1{Z_\mathbf{W}(x)} \exp(\sum_{j=1}^p \mathbf{w}_j^T f_j(\mathbf{x}, y)), Z_\mathbf{W}(\mathbf{x})=\sum_{y=0}^{C-1} \exp(\sum_{j=1}^p \mathbf{w}_j^T f_j(\mathbf{x}, y))$$

即认为在 $$\mathbf{x}$$ 给定条件下，各个类 $$y$$ 出现的概率由一组指标 $$f_j(\mathbf{x},y)$$（与 $$\mathbf{w}_j$$ 维度一致）决定，分别为 $$\exp(\sum_{j=1}^p \mathbf{w}_j^T f_j(\mathbf{x}, y=0)), \cdots, \exp(\sum_{j=1}^p \mathbf{w}_j^T f_j(\mathbf{x}, y=C-1))$$。注意这些值只是一些大于 0 的数，为保证是概率值，除以它们的和即 $$Z_\mathbf{W}(\mathbf{x})$$ 即可。

其实它就是对上述分类问题一般的模型里多项分布的参数 $$f_\theta(\mathbf{x})$$ 作了一个对数线性的假设。$$f_j$$ 不带参数，分布参数 $$\theta$$ 是 $$\mathbf{W}=(\mathbf{w}_1,\cdots, \mathbf{w}_p)$$，同样地对其作极大似然估计即可。

> 逻辑回归属于对数线性模型的一种：对 C 分类问题，$$p=C, f_1(\mathbf{x},y) = \cdots =  f_{C-1}(\mathbf{x},y) = \mathbf{x}, \cdots, f_C(\mathbf{x},y) = \mathbf{0}$$。
{: .prompt-info }

一般的最大熵模型很难也不会在模型视角下作解释，等价的损失函数应该是奇奇怪怪的。逻辑回归由于是其特例，它能作模型视角的解释（即交叉熵损失）也不奇怪。


## 基于能量的模型（EBM）


# 纯粹的判别模型

上述判别模型即可以设计决策函数与学习算法，在模型视角下解释；也可以建模条件概率，确定参数估计法（一般是极大似然法），在统计视角下解释。**纯粹的判别模型**是指只可在模型视角，而无法或难以在统计视角下解释的。

> 很多地方把纯粹的判别模型混淆到判别模型这个大类里：只要有输入有输出的决策函数与对应的学习算法，就叫判别模型。这里我分出来是为了方便理解。
{: .prompt-info }

它与判别模型的主要区别在于学习算法，它们的学习算法找不到合适的统计上的参数估计方法对应。例如以下机器学习模型都属于纯粹的判别模型：

- 感知机：决策函数 $$f_\theta(\mathbf{x}) = Sigmoid(\mathbf{w}^T\mathbf{x}+b)$$，学习算法是基于误分类点的调整算法，等价于损失函数 $$l(f_\theta(\mathbf{x}), y) =  \[- y \f_\theta(\mathbf{x})]_+$$；
- 支持向量机（SVM）：决策函数 $$f_\theta(\mathbf{x}) = Sigmoid(\mathbf{w}^T\phi(\mathbf{x})+b)$$，学习算法依据间隔最大化原则，等价于合页损失函数 $$l(\f_\theta(\mathbf{x}),y)=\[1 - y \f_\theta(\mathbf{x})]_+$$。

它们都解决二分类问题，甚至决策函数都与逻辑回归一模一样，使其成为纯粹的判别模型主要靠学习算法的不同：这些学习算法都是从别的角度想出来的，而不是由统计建模的参数估计反推出来的。我觉得这是很多初学者容易迷糊的地方。

其他纯粹的判别模型：k 近邻（kNN）、决策树、各种集成学习方法等等。





# 生成模型

对于监督学习（有 $$\mathbf{x}$$ 有 $$y$$），除了直接对条件分布 $$p(y|\mathbf{x})$$ 建模，还可以对自变量的分布 $$p(\mathbf{x})$$ 和联合分布 $$p(\mathbf{x},y)$$ 建模，根据条件概率公式：

$$p(y|\mathbf{x}) = \frac{p(\mathbf{x},y)}{p(\mathbf{x})}$$

间接地得到条件分布的模型。这种模型称为**生成模型**（generative model）。它还要多一步求 $$p(\mathbf{x},y)$$ 的边缘分布 $$p(\mathbf{x})$$ 的步骤。 
无法站在模型视角了。

唯一区别是 $$x$$ 的角色不再是作为条件分布中确定的参数，而是随机变量，需要有一定的分布假设了。原来对 $$p(y|\mathbf{x})$$ 假设是容易的，因为是 1 维。生成模型就多了很多麻烦。

生成模型更常见于解决无监督学习任务（有 $$\mathbf{x}$$ 无 $$y$$），无监督学习不涉及条件概率，也没有判别模型一说，此时就是对分布 $$p(\mathbf{x})$$ 建模，依据 $$p(\mathbf{x})$$ 完成各种无监督任务。

当然，可以直接假设 $$p(\mathbf{x})$$ 服从某一分布（离散情况如泊松分布，连续情况如正态分布等），用样本数据对其参数进行估计。不用怀疑，这样做是完全没问题的，就是无监督生成模型最单纯的思路。但是这个模型太简单了，难以应付无监督学习中高维的、复杂的数据。试想，比如数据是一堆图片，谁会简单地假设图像的像素点符合一个多维正态分布呢。为此，统计学家设计出了各种模型，它们并不只是换成了一个更复杂的分布（其实概率论里实用的分布也没几个，大都是简单的），很多引入了一些新机制，如隐变量，由此还衍生出一堆关于该机制的理论，使得这一部分内容比那些判别模型更繁琐艰深：判别模型尤其是纯粹的判别模型真的只是研究怎么设计一个函数，逻辑上很易理解；而无监督生成模型关注点不是设计概率分布函数，而是每种模型都有独特的机制，百花齐放。所以下面需要花大篇幅详细讲解各种模型了，不是因为我不熟想拘泥于细节，而是里面涉及思想性的东西，很关键。


学习、解决问题穿插在表示里。通用的放在机制里讲，具体模型具体的放在具体。

什么无监督任务？——解决问题
- **密度估计**：$$p(\mathbf{x})$$ 就是想要的，已经完成了；
- **生成**：从分布 $$p(\mathbf{x})$$ 采样是比较简单的生成方式。很多是借助隐变量，从 $$p(z)$$ 生成 z，再从p(z|x)$$。见隐变量机制。
- **推断**（inference）：已知测试样本的部分变量 $$\mathbf{e}$$（$$x_1,\cdots,x_m$$ 的一个子集），预测剩余某个或某些变量 $$ \mathbf{q}$$。有了 $$p(\mathbf{x})$$ 后，通常借助 Bayes 公式计算 $$\mathbf{q}$$ 对已知变量的条件概率，根据此来预测：
$$p(\mathbf{q}|\mathbf{e}) = \frac{p(\mathbf{q},\mathbf{e})}{p(\mathbf{e})} = \frac{\sum_\mathbf{z} p(\mathbf{x})}{\sum_{\mathbf{q},\mathbf{z}} p(\mathbf{x})}$$

连续情形改为积分号。问题转化为如何对如何求一个分布 $$p(\mathbf{x})$$ 的边缘分布。



## 图关系表示

当 $$\mathbf{x}=(x_1,\cdots,x_m)$$ 维度太高时，直接建模不太容易下手。一个思路是对随机变量 $$x_1,\cdots,x_m$$ 之间具有某种关系，在这些关系的基础上建模。将变量之间的关系建模为图关系的模型统称为**概率图模型**（Graphical Model）。

建模方式由两种：有向图、无向图。

### 有向图模型

将 $$p_\theta(\mathbf{x})$$ 这个联合分布按照概率的乘法公式

$$p(\mathbf{x}) = p(x_r |\mathbf{x}_c) p(\mathbf{x}_c)$$

**约定！！！一次只拿一个放到左边。这边重新整理**

分解为条件分布与边缘分布的积，对这二者建模。但是它们（$$\mathbf{x}_r, \mathbf{x}_c$$）的维度可能还是很高，不易下手。于是继续递归地将其分解下去：

$$p(\mathbf{x}_r | \mathbf{x}_c) = p(\mathbf{x}_{rr}| \mathbf{x}_{rc}, \mathbf{x}_c) p(\mathbf{x}_{rc})$$

$$p(\mathbf{x}_c)) = p(\mathbf{x}_{cr}| \mathbf{x}_{cc}) p(\mathbf{x}_{cc})$$



直到变成一堆**一维条件分布与一维边缘分布的积**。对一维的分布建模是很简单的，概率论课上学的分布就够用了。这样就把一个对高维分布建模的问题转化成低维，重点转移到了怎么分解 $$p(\mathbf{x})$$ 的问题上。不同的分解方式属于模型假设，是人来设计的。

如果画图示意，把变量当作结点，最后的因式中出现一个条件概率，就给条件概率竖线右边变量的结点（可能有多个）连一个箭头到左边变量的结点（只有一个），这样最后分解出的式子都可以画一张有向无环图。可以证明，如果按上面方式，无论怎么分解，最后一定是这种形式（不再证明，可以自己找个例子试试）：

$$p(\mathbf{x}) = p(x_{i_1}|x_{i_2},\cdots,x_{i_m})  p(x_{i_2}|x_{i_3},\cdots,x_{i_m})\cdots  p(x_{i_{m-1}}|x_{i_m})  p(x_{i_m})$$

其中 $$(i_1,\cdots,i_m)$$ 是 $$(1,\cdots,m)$$ 的一个顺序。这种形式与**完全有向无环图**一一对应，此顺序就是此有向无环图的拓扑序。完全的有向无环图是能连则连的，所以可以由拓扑序完全决定。这样不同模型在图结构上的差别仅在于一个顺序 $$(i_1,i_2,\cdots,i_m)$$。

在概率建模中，有时会对变量之间作一些条件独立性假设，形式如下（注意竖线左边暂时限定为只有一个变量，不考虑多个变量）：

$$p(x_r|\mathbf{x}_c) = p(x_r|\mathbf{x}_{c'})$$，其中 $$c'\subset c$$。

上面 $$p(\mathbf{x})$$ 的分解出的条件概率中，有些尾巴太长，可以对其加上这类假设，让其尾巴变短，能极大地减少模型的复杂度。也即允许砍掉完全有向无环图的一些边。从而与全体有向无环图一一对应。这种模型就叫**有向图模型**，又称**贝叶斯网络**、**信念网络**。

例子：

![](Bayesian_network_example.png)

B,E,A,D,S 都是 1 维离散的概率可以分解为 $$p(B,E,A,D,S)=p($$

其中的条件独立性假设为

图中是对他们都作了最简单的分布假设，条件概率作次数为1的多项分布，边缘概率作两点分布。这些分布的参数就是图中表格里的数。



总结一下：

有向图模型就是对变量之间作了一些**条件独立性假设**，使得 $$p(\mathbf{x})$$ 能分解为

$$p(\mathbf{x})=\prod_{j=1}^m p(x_j|\mathbf{x}_{\pi_j})$$

其中 $$\pi_j$$ 是一个 $$\{1,\cdots,j-1\}$$ 的子集，使其与全体有向无环图作了一一对应（通过一个简单的定理保证，上面其实就是证明过程）。

> 可以看到，这些条件独立性假设不是任意的，必须满足一定性质使能以上面的方式构成有向无环图，这个性质称为**马尔可夫性**，描述为：存在一个变量排序 $$x_{i_1},\cdots,x_{i_m}$$，使得所有条件独立性假设 $$p(x_{i_r}|\mathbf{x}_c) = p(x_{i_r}|\mathbf{x}_{c'}),c'\subset c$$ 中的 $$\mathbf{x}_c$$ 必须为 $$(x_{i_1},\cdots,x_{i_{r-1}})$$。
> 可以证明该排序若存在则唯一。上面那个所谓“简单的定理”本质上就是在说这个唯一的变量排序就是上述有向无环图的拓扑序。
{: .prompt-info }

。该性质主要涉及变量顺序：$$\pi_j$$ 只能比 $$j$$ 小就体现了这个性质。
{: .prompt-info }


以下总结了使用有向图模型表示、学习、解决实际问题的流程。

- 表示：就是为实际问题中的变量 $$x_1,\cdots,x_m$$ 设计一个有向无环图。由于其中每条有向边 $$e_{i\rightarrow j}$$ 代表 $$p(\mathbf{x})$$ 出现一个 $$p(x_j|x_i)$$ 的条件概率，所以它反映的是变量间的**因果关系**，在设计图时应该遵循此关系，将“因”变量连到“果”变量。有了有向图后，$$p(\mathbf{x})$$ 自然根据分解式表示出来了，然后对分解出来的分布作一些简单的建模，如正态分布，次数为 1 的多项分布等；
- 学习：有了 $$\mathbf{x}$$ 的一组样本，直接对 $$p(\mathbf{x})$$ 作极大似然估计即可；
- 解决实际问题：有向图模型最大的应用在推断任务，上面说了就是求边缘分布。有**精确推断算法**如变量消除法、信念传播算法；维度太高时计算代价很大，就有了**近似推断算法**，如有名的 MCMC、Gibbs 采样等。不再详述。

### 无向图模型

邱锡朋 书总结地很好，言简意赅，提纲挈领。

**无向图模型**，又称**马尔可夫随机场**（MRF)，是另一种概率分解方式，它的想法没有那么直观了。我像上面有向图模型那样总结：

无向图模型也是对变量之间作了一些概率分布上的假设（很巧的是它也是些**条件独立性假设**），使得 $$p(\mathbf{x})$$ 能分解另一种形式：

$$p(\mathbf{x})=\frac1Z \prod_{c\in\mathcal{C}} \phi_c(\mathbf{x}_c; \theta_c), Z(\theta) = \sum_{\mathbf{x}}  or  \int_\mathbf{x} \prod_{c\in\mathcal{C}} \phi_c(\mathbf{x}_c;\theta_c)$$  

其中 $$\mathcal{C}$$ 是 $$(1,\cdots,m)$$ 的一堆子集 $$c$$ 的集合，使其与全体**无向图**作了一一对应（通过 Hammersley-Clifford 定理保证），对应方式为：所有 $$\mathcal{C}$$ 是无向图所有**最大团**构成的集合
。（图论知识回顾：1. 无向图的完全子图即称为团，即所有结点之间都连边；2. 对团扩充进其他任何结点，都无法再形成团，则该团为最大团。最大团的意思不是结点数最多的团；3. 所有最大团集合 $$\mathcal{C}$$ 能唯一确定一个无向图，无向图的最大团集合也是唯一的，所以上面是一一对应，这连定理都不算，想想就明白了。）

> 无向图模型的条件独立性假设同样也不是任意的，必须满足一定性质使能以上面的方式构成无向图。这个性质称为**局部马尔可夫性**，描述为：所有条件独立性假设 $$p(x_r|\mathbf{x}_c) = p(x_r|\mathbf{x}_{c'}),c'\subset c$$ 中的 $$\mathbf{x}_c$$ 必须是除 $$x_r$$ 的所有其他变量。
> 如果把变量看作结点，将$$x_r$$ 与 $$\mathbf{x}_c$$ 对应的结点以无向边相连，则可构成一张无向图。Hammersley-Clifford 定理本质上就是在说此无向图恰恰是上述模型对应的那个无向图。
{: .prompt-info }


以下总结了使用无向图模型表示、学习、解决实际问题的流程。

- 表示：就是为实际问题中的变量 $$x_1,\cdots,x_m$$ 设计一个无向图。无向边反映的是变量间普通的**依赖关系**，在设计图时应该遵循此关系，将有依赖关系的变量以无向边相连。有了无向图后，求它所有的最大团 $$c$$（有很多算法，如Bron-Kerbosch 算法）。然后对各个 $$\phi_c(\mathbf{x})$$ 建模，注意它只要保证非负即可，它不是概率分布。最后乘起来使 $$\sum_{\mathbf{x})p(\mathbf{x})=1$$ 由 $$Z(\theta)$$ 保证，$$Z(\theta)$$ 不需要建模。
- 学习：有了 $$\mathbf{x}$$ 的一组样本，直接对 $$p(\mathbf{x})$$ 作极大似然估计即可；
- 解决实际问题：无向图模型最大的应用也是推断任务，同有向图模型。


## 隐变量机制

以上我们研究 $$p(\mathbf{x})$$，里面只涉及随机变量 $$\mathbf{x}=(x_1,\cdots,x_m)$$。为了将模型变复杂，假设还有一些别的随机变量 $$\mathbf{z}=(z_1,\cdots,z_s)$$ 参与到分布中起作用，此时考虑的就是另一个分布 $$p(\mathbf{x},\mathbf{z})$$ 了。我们转而对这个联合分布作假设（$$z$$ 与 $$x$$ 的关系就蕴含其中）

我们手头的样本只有 $$\mathbf{x}$$，而没有 $$\mathbf{z}$$，所以 $$\mathbf{z}$$ 被称为**隐变量**。带了隐变量的概率模型称为**隐变量模型**（Latent Variable Model）。

以下总结了隐变量模型的表示、学习、解决问题：
- 表示：发挥对实际问题的认知，考虑一下有哪些值得关注的但是无法观测的变量，将其引入为隐变量。要建模的分布由 $$p(\mathbf{x})$$ 改为显变量和隐变量的联合分布 $$p(\mathbf{x},\mathbf{z})$$，建模时应当关注实际问题中隐变量与$$\mathbf{x},\mathbf{z}$$显变量的联系；通常是将其分解为 $$p(\mathbf{x}|\mathbf{z})p(\mathbf{z})$$，对这两个东西建模；
- 学习：从逻辑上看，没有 $$\mathbf{z}$$ 的样本是无法完整地估计 $$p(\mathbf{x},\mathbf{z})$$ 的（如极大似然估计），那该怎么办呢？应当转变思维：不一定非要精确的 $$p(\mathbf{x},\mathbf{z})$$，估计个大概也行啊，反正要用的也不是完整的 $$p(\mathbf{x},\mathbf{z})$$，而是它的边缘分布 $$p(\mathbf{x})$$。

这个近似算法是有名的**期望最大化（EM）算法**。它的思想是引入一个 $$z$$ 的。不打算详细讲它（任何一本统计模型的教材都有），只要记住它是**带隐变量的概率模型的学习算法**：已知输入样本 $$\mathbf{x}^{(1)},\cdots,\mathbf{x}^{(N)}$$，估计它与隐变量的联合分布 $$p(\mathbf{x},\mathbf{z})$$


出现了后验。p(z|x)。需不需要借助bayes 理解？


- 解决问题：$$p(\mathbf{x},\mathbf{z})$$ 可以做很多事情：
在无监督任务中：
- 推断任务：可以关注隐变量，它通常有实际意义，作推断 $$p(z|x)$$，即已知 x 推测背后的隐变量 z。
- 可以只关注显变量，隐变量只是个辅助手段，则直接对他求边缘分布即可，这是个理想，非常非常难实现：
- 生成任务，也有的借助一个隐变量 $$z$$ 根据 $$p(\mathbf{x},\mathbf{z}) = p(\mathbf{x}|\mathbf{z}) p(\mathbf{z})$$，先对 z 采样，再对 x 采样。它**等价于**直接从分布 $$P(\mathbf{x}) = \sum_{z} P(\mathbf{x},z) 或 \int_z (\mathbf{x},z)$$中采样。很典型的例子是混合模型、VAE、GAN 等。

深度生成模型指哪些？



当然也可以有监督。有监督 x,y 都是显变量




## 图模型的例子

以下是两种重要的机制，通过这种机制可以组合出各种乱七八糟的模型。以下无论有监督还是无监督，都统一考虑对 $$p(x)$$ 的建模。有监督的 y 可以考虑成 x 的一个元素。


要么是特殊的图结构，要么是建模


### Sigmoid 信念网络

1992 年。它完成监督学习：自变量都是 {0,1}，是不带隐变量的有向图模型。它特殊在对条件概率 $$p(x_j|\mathbf{x}_{\pi_j})$$的建模上，而不是图结构上（边缘分布仍然为两点分布，注意它是一维的）：

$$p(x_j|\mathbf{x}_{\pi_j})$$ 不是蚕蛹

每个变量的每一组



## 朴素贝叶斯（用于监督学习）

**完全不用有向图模型解释：**

**朴素贝叶斯法**（Naïve Bayes）适用于**自变量也是有限取值的分类问题**（这种对自变量的要求导致它几乎无法用到常见的实际场合）：设 $$\mathbf{x}=(x_1,\cdots,x_m)$$，每个变量 $$x_k$$ 有 $$s_j$$ 个取值；$$y$$ 有 $$C$$ 个类。它的做法是根据 Bayes 公式：（现在 $$\mathbf{x}, y$$ 都是离散型随机变量，$$p$$ 都改写为 $$P$$） 

$$P(x,y) = P(\mathbf{x}|y) p(y)$$

$$P(x) = \sum_{y=0}^{C-1} P(\mathbf(x), y)$$

将对 $$P(\mathbf{x}), $$P(\mathbf{x}, y)$$ 建模的问题转化为对 $$P(y), P(\mathbf{x}|y)$$ 的建模（这里我就不叫它们先验分布什么的了，容易与后面 Bayes 观点所说的参数的先验分布混淆）。这种转化的可能考虑是：1. $$y$$ 只有少数几种情况，对 $$P(y)$$ 建模比对 $$P(\mathbf{x})$$ 更可信，参数估计时需要的样本数也更少；2. $$P(\mathbf{x},y)$$ 很难估计。

朴素贝叶斯法采用最简单、最 naïve 的模型——**重复次数为 1 的多项分布**，即从所有情况中按一定概率抽取一个出来。记

- $$P(y) ~ MN(1, \mathbf{p}_y)$$
- $$P(\mathbf{x} | y) ~ MN(1, \mathbf{p}_{x|y})$$

只需作极大似然估计估计出这两个分布，$$P(y|\mathbf{x})$$ 就能根据套 Bayes 公式算出来，就能拿来预测分类了。

这种模型最大的问题是参数太多。$$y$$ 有 $$C$$ 种情况，则 $$\mathbf{p}_y)$$ 有 $$C$$ 个参数（准确来说有 $$C-1$$ 个独立参数，有 1 个参数可以通过 1 减其他参数得到），还不错；$$\mathbf{x}$$ 有 $$\prod_{j=1}^m s_j$$ 种情况，则 $$\mathbf{p}_{x|y})$$ 有 $$\prod_{j=1}^m s_j$$ 个参数。

为了减少参数数量，对分布 $$P(\mathbf{x} | y)$$ 又作了**条件独立性假设**：无论 $$y$$ 取何值，$$x_1,\cdots, x_m$$ 是互相独立的，即 $$x_1,\cdots, x_m$$ 对 $$y$$ 条件独立。这样 $$P(\mathbf{x} | y)$$ 就不必建模为。根据条件独立性的定义公式：

$$P(\mathbf{x} |y) = \prod_{j=1}^m P(x_j,y)$$ 

只需对 $$P(x_j|y) (j=1,\cdots,m)$$ 建模。同样地，它们也被建模为重复次数为 1 的多项分布，各分布分别有 $$S_j$$ 个参数，合计 $$\sum_{j=1}^m s_j$$ 个，比不作独立假设的 $$\prod_{j=1}^m s_j$$ 少得多。


**拥有向图解释**，必须要求y 是类别。

不带隐变量、有向图。

拓扑序 y,x_1,x_2,x_3
P(x_3|y,x_1,x_2) = P(x_3|y)
P(x_2|y,x_1) = p(x_2|y)

P(y,x_1,x_2,x_3) = P(x_3|y,x_1,x_2)P(x_2|y,x_1)P(x_1|y) P(y) = P(x_3|y)P(x_2|y)P(x_1|y)P(y)



形式上用了bayes 公式。条件独立性还可以写为

实际上。。


一种特殊的图结构。 做假设







> 监督学细 是 (x,y) 建模 p(x,y) 的也叫生成模型，不要混淆。直接对 $$p(y|x)$$ 叫判别模型。对 $$p(y|x)$$ 建模可以用 logistic 回归、神经网络。。对 p(x,y) 或者 p(x)的建模是更难的，只能用概率分布，概率分布不如神经网络表示能力强。所以现在。



### 对数线性模型：

不带隐变量，最大熵，监督模型。无向图/

不是特殊的图结构，而是建模



和前面讲的判别模型只是形式上差不多，但思想内核是不一样的。按照这里生成模型解决监督学习问题的逻辑，应该是

$$p(\mathbf{x},y)=\frac1{Z_\mathbf{W}(x)} \exp(\sum_{j=1}^p \mathbf{w}_j^T f_j(\mathbf{x}, y)), Z_\mathbf{W}(\mathbf{x})=\sum_{y=0}^{C-1} \exp(\sum_{j=1}^p \mathbf{w}_j^T f_j(\mathbf{x}, y))$$

然后在求边缘 $$p(\mathbf{x})$$ 相除才得到 $$p(y|\mathbf{x})$$ 的模型。它与判别模型不是一回事。





### 隐马尔可夫模型

带隐变量，有向图


### 条件随机场

带隐变量

### 玻尔兹曼机


### 受限玻尔兹曼机


## 深度信念网络



### 深度学习的可解释性：统计角度

什么是可解释性？从模型角度，。。。


从统计角度，就是知道任意中间变量 z (真正意义的中间变量)。统计是个更高的角度，由条件分布可以导出一个判别函数，最简单的取概率最大只。反过来不行。统计是一个更高的角度！

普通的监督学习的深度网络其实就是经过很多层中间变量的图模型。


学习完了，中间任取一层中间变量（真正意义上的中间变量）当作隐变量，我们只知道判别函数 $$z=f(x)$$,$$p(x|z)$$，具体是啥，$$p(z|x)$$ 具体是啥。或者相邻两层之间 $$p(z_1|z_2)$$ 具体是啥。


## 非图模型的例子



### 混合模型

混合模型就是一个复杂的分布，直接对 $$p(\mathbf{x})$$ 建模。定义如下（这里为了方便我把参数放分号后面了）：

$$p(\mathbf{x}) = \sum_{k=1}^K \pi_k p_k(\mathbf{x})$$

其中 K 是超参数，是人定的。注意，$$\pi_k$$ 是也是分布的参数，而不是人事先固定好的超参数。这东西是通过数据估计出来的，混合模型厉害就厉害在这立。

常见的是 高斯混合模型GMM：

它的参数是 $$\pi_k,\mu_k,\sigma_k$$。


混合模型的复杂性体现在如下定理：高斯混合模型在高斯分布的数量足够大的情况下, GMM 可以用来近似 
R
D
 中的任意密度分布。 它类似于神经网络的通用近似定理。







直接对它作参数估计是非常困难的，。。。但是我们观察它的分布形式，是求和符号，灵感在于如果将求和好它正好与隐变量模型对上了（见隐变量模型解决问题，第2条）

$$P(\mathbf{x}) = \sum_{z=1}^K P(\mathbf{x},z)= \sum_{z=1}^K P(\mathbf{x}|z)P(z)$$

$$P(\mathbf{x}|z) = 由z 决定的高斯，分段函数$$

对应的就是将 p(z) 建模为是离散型随机变量，分布列是 \pi_1,\cdots,\pi_k$$，p(x|z) 建模为单纯的高斯分布。这样就：把原来混合在一大个x的分布参数 pi1，k，，变成了

- pi_1,\cdots, p_k ：，变成了隐变量z的边缘分布参数；
- \sigma_k ：变成条件

无论怎么说，它们都是联合分布 $$p(\mathbf{x},z)$$ 的参数（联合分布是它们撑起来的），EM 算法可以将这些参数都估计出来，解决了参数估计的难题。






P(\mathbf{x}|\pi)=

原 x 变量看作一个整体，假设它服从某一个


有一个 隐变量 z 负责选择哪个分布，不是硬选择（次数为1 的多项分布），而是软性地。




带隐变量、无监督。不适合看成概率图模型，因为它既没有有向图模型分解成一维分布，如果硬要看也是可以的，$$p(x,z)=p(x|z)$$（一维的），看成 z指向 x，但高维的话\mathbf{x} 拆不开。无向图解释不通，它就没有条件概率。

## 引入 z 当作 z->x  的监督任务

它是个 toy，也是个模型，就单独开一节了。


自己学的后果：
- Generator 自己学（当作传统的监督学习）。相当于向量到图片，像图像分类反过来。
- 问题是输入的向量如何产生。随机产生肯定不好，容易相似的离得很远；所以需要人来设计。

下面的VAE和GAN 算是从这个方面的改进。





### 自编码器（Autoencoder）与 变分自编码器（VAE）

自编码器不适合划分到生成模型或者判别模型的任一类，有点四不像。因为它没有对 p(x) 或者 p(x,z) 建模。讲他主要是为了引出 VAE。

站在模型视角出发看更加合适，先说模型视角。我们学了一个重建函数，然后最小化重建损失函数。模型假设是它经过了哪些中间变量，如何经过的。例如对神经网络，中间有几层，每层几个神经元。

在统计视角下，设 x 是随机变量，它其实是对 $$p(x|x)$$ 建模。没错，它必须服从在 x 处概率为 1 的分布列（连续情况是 dirac 函数）。这甚至连假设都算不上，只是概率论的定理。

可以证明，对它作极大似然估计（样本只需x）对应了重建损失。


模型有很多中间变量（必须是真正意义上的中间，即它决定于 x，且重建x 完全决定于它，例如前向网络的一层），任取一层 $$\mathbf{z}$$，则重建函数 x' = R(x) 可拆成拆成两个函数 Encoder，Decoder
 z = Encoder(x)  x'=Decoder(z)
 
 通过上面的优化过程学习完毕，得到了两个具体的函数 Encoder，Decoder，这两个函数是互逆的，从而 z 可以作为x 的一个**表示**。换句话说，把输入空间作了一个变换，变换空间与原空间一一对应。能可逆是很重要的。
 （平时看到的网络的某一层只能叫 嵌入 Embedding，因为不可逆，把输入空间嵌入。这种东西甚至不用学，直接构造Embedding 函数即可）。学习可逆的表示是一个无监督任务，称为表示学习，Autoencoder 就是做这个的。

 > z 维数必须比 x 低很多。否则会偷懒，copy 恒等映射！！中间任何一层都要这样。而且也没有意义，为什么要学一个更高维的东西表示它呢？找麻烦。


 从统计角度看，中间变量可以看成为隐变量 $$\mathbf{z}$$，自编码器是一个隐变量模型。
 
从分布的角度，在考虑 $$p(x|x) = p(x|z)p(z|x)$$。
统计是一个更高的角度，可以认为 $$p(z|x)$$, $$p(x|z)$$ 根据某种策略（例如最大概率）生成了决策函数 Encoder，Decoder，但是学习完后我们手里有的不知道它们的具体是什么样子的！就像回归那样，知道了决策函数，就是知道了真太分布 $$N(f(x),)$$。换句话说，学到的那个表示是不可解释的。


要想让它可解释，应该对 $$p(x|z),p(z|x)$$ 作假设，核心矛盾在于， $$p(x|z)$$ 和 $$p(z|x)$$ 是联动的、耦合的，因为它们都是 $$p(x,z)$$ 的产物（受到的约束是 p(x|z)p(z|x)=p(x|x)）。我们在做假设的时候，不能独立地 $$p(x|z)$$ 和 $$p(z|x)$$，会与概率公式矛盾。而我们不能显示地表示它们的联动关系。

自编码器的做法则是不对二者任意之一作显示的分布假设，让二者根据概率论公理的限制 + 通过决策函数的启发，自动地来学。

Autoencoder 拿来作生成任务是很勉强的，不是说效果不好，而是逻辑不通，为了逻辑通顺只能做一些不合理的近似。我说的当然不是根据 $$p(x)$$，因为Autoencoder 压根没有估计出来 p(x)过它，而是估计 $$p(x|x)$$。当然也不能根据 $$p(x|x)$$  来生成（自己不能生成自己啊hhh），而是通过隐变量 z 就是隐变量机制那里讲的
p(x,z) = p(x|z) p(z)

Autoencoder 算是估计出了 p(x|z)，体现在 Decoder 函数上，但是 p(z) 是不知道的！解决方案：
- Autoencoder 还估计出来了 p(z|x)，可以近似替代一下 p(z)，但它是通过 Encoder 函数，x 从哪儿来？只有样本可以用。如果拿 x 来生成 z，最后生成的结果 x' 只能和 x 极度相似。
- 那我完全可以 z 随便取，过一下Decoder，但这样没道理（相当于胡乱假设了 p(z)）效果肯定不如生成出来的z好。

变分自编码器 2014提出，。



上面第一条的解决方案，试图用 $$p(z|x)$$ 近似替代 $$p(z)$$。它们两个的唯一不同，就是 $$p(z|x)$$ 是很多个分布（视x不同取值而不同），如果不管 x 取什么值，都 $$p(z|x)$$ 都一样，此时说明 $$x$$ 与 z 独立，于是 $$p(z|x)$$ 就等于 $$p(z)$$ 了。

VAE 的想法就是在学习的过程中，不仅限制其学到可逆的变换（即重建损失函数 或虽大似然估计），还限制 $$p(z|x)$$ 必须都靠近某个固定的分布：即最小化 p(z|x) 与此固定分布的 KL三度（是衡量分布接近程度的工具）。损失函数是二者的综合。要优化这种损失函数可不容易，幸亏 VAE 的作者通过再参数化（Reparameterization）的技巧将其转化成了梯度下降法可以用的形式，使他引入的损失函数VAE项可以统一跟着似然用梯度下降法，不再详述。

另外要说明，VAE 作者建议固定的分布选择标准正态分布，在论文里有解释。通过引入此机制，VAE 可以像。。隐变量那样完成生成任务：从标准正态分布中生成 z，在过 Decoder 生成 x。由于$$p(z|x)$$ 就等于 $$p(z)$$，用到了$$p(x,z) = p(x|z)p(z)$$，因此，变分自编码器应当划归到生成模型。

从一个固定的分布产生 z，然后学习 z ，这个模型上与上面那个 naive 的模型是一样的。区别在于怎么训练，上面的需要 z 的样本，而这个不需要，而是充分挖掘了 x 自身的信息。


### 生成对抗网络（GAN）

另一种不需要 z 样本的。

GAN 也是基于 $$p(x,z) = p(x|z)p(z)$$，

Generator 后加了 Discriminator，Discriminator 是启发式的。

思想 两种：对抗关系、教师学生

从训练算法的角度（而不是逻辑的）合看成一个网络
算法 必须固定下层的 discriminator，否则只需调后面几层参数，让output全都是 1


经验：AutoEncoder 需要更大的网络才能得到和 GAN 相同的效果。直观原因？没有考虑样本间的关系？


### 单独使用 Discrimator 的模型

用一个 Discriminator 。argmax最大值。类似CNN可视化某一层。。不穷举。
Discriminator 更倾向于，，像老师、文学批评家。
最主要的问题是只有正样本，没有负样本，它会倾向于把所有东西盘城正的。。非常考验负样本的收集技术。它是手动的。



好的负样本某种程度上是生成出来的。这里的一个算法是可以一开始随机副样本，相当于 GAN 的geneator 不单独搞了，直接用 discriminate的argmax。穷举是不可能的，。通常又一些开率论的假设。这就是graphic model（见lhy ppt），概率图模型，沈心意讲的那节。STructure learning 传统技术。


会不会这个generator 和discri 是相同的知识，导致反复否定自己，反复横跳？？lhy ppt？？？

讲  Generator dis 各自的优缺点，然后配合起来才能互补。（lhy 第一节课最后）


不如把他们分独立开，这也是GAN的一个想法来源。




# Bayes 观点




总结上面所谓的统计视角，都是对概率分布进行建模，学习算法即对分布参数的估计，这其实属于统计学中**概率学派**的视角。统计学中还有一种视角来源于 **Bayes（贝叶斯）学派**，Bayes 学派和概率学派的主要区别就是把不把**分布的参数当作随机变量**。这两个学派在历史上经常吵来吵去，但基本是互相对应的，真真确确地是一个事情的两种解释，其实完全可以不用了解。经典的参考书籍是 [PRML]()，作者 Christopher Bishop 是一个坚定的贝叶斯人，通篇都以 Bayes 学派的视角解释机器学习，以下大部分内容也是参考此书。



> 要注意，上面出现的带 Bayes 名字的东西都与 Bayes 观点无关。Bayes 公式只是概率论中的基本公式，朴素 Bayes 是指用了 Bayes 公式推导，Bayesian 网络也只是一个名词。
{: .prompt-tip }



可以解释正则化



学习算法就是=参数估计。。，模型=概率模型。。训练数据 = 样本

记号上，分布参数一般写在p下标，也有写分号后，这些都是常规写法，。但也有人写在条件里的，比较逆天，尤其是贝叶斯



隐变量与参数其实看起来没什么不同，如果放在bayes观点来看？主要是 EM 算法对它的处理让它与众不同。


## Bayes 观点下的监督学习

不管是判别模型还是生成模型，都要建模 $$p(y|x)$$。由于这里把 \theta 看作随机变量，应写为 $$p(y|x,\theta)$$。一个特点是把数据集 $$D = (X,Y)$$ 也引入到随机变量中（而不是看作x,y的样本），所以是 $$p(y|x,\theta,D)$$。


给了数据集 D，先验分布 $$p(\theta)$$（通常都是假设连续的），求后验分布 $$p(\theta|D)$$。训练过程在贝叶斯术语中称为推断（inference），就是根据贝叶斯公式：

一个细节很重要：先验分布 $$p(\theta)$$ 和 $$p(\theta|X)$$ 是一样的。\theta 与 x 无关，只有给了y 才有关。

$$ p(\theta|D) =p(\theta|X,Y) =  \frac{p(Y|X,\theta) p(\theta|X)}{\int p(Y|X,\theta) p(\theta|X)d\theta} = \frac{p(Y|X,\theta) p(\theta)}{\int p(Y|X,\theta) p(\theta)d\theta} $$ 求出后验分布。（注意不要把整个D翻过来，而是只翻 Y）数据集 D 的信息蕴含在似然函数 $$p(Y|X,\theta)$$ 里:i.i.d.

$$p(Y|X,\theta) = \prod_{i=1}^N p(y_i| x_i,\theta) $$

分母即归一化即可。

测试阶段即预测（prediction）。有了后验分布：

方法一：简单地从后延分布 $$p(\theta|D)$$ 中找最可能的 $$\theta$$ 值 $$\theta^\star$$，然后使用模型 $$p(y|x,\theta^\star)$$ 来预测。这叫最大后验概率估计（MAP），不是纯正的贝叶斯。

方法二：这是纯正的 Bayes 方法

而是平均：

$$p(y|x,D) = \int p(y,\theta|x,D) d\theta = \int p(y|\theta,x,D) p(\theta |x,D) d\theta = \int p(y|x,\theta,D) p(\theta |D) d\theta$$


对于复杂的，是不是方法一二时间上相同？