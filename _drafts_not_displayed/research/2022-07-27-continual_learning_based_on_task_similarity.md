---
title: 基于任务相似性的持续学习
date: 2022-07-27
categories: [科研]
tags: [学习笔记, 持续学习]
img_path: /assets/img/
math: true
---


## 模型容量问题

持续学习的一大特点是学习任务的类型和数量没有预定义。在学习每个任务的期间，永远不知道未来有多少个任务、它们是什么样子的。之前所说的：每个任务学习一个独立的模型，其模型大小随任务量线性地增加。这样，模型尝试学习、记下每个任务所有的知识，对应的算法也是与普通机器学习没有差别，是持续学习不允许的。

我们不希望模型大小无序地膨胀，而是**固定模型容量**（capacity），让算法在固定容量的模型下完成持续学习（偶尔也会允许少量的膨胀）。这里所说的模型容量更多的是一个抽象概念，指模型的**表示能力**。当然，对于深度学习，模型的表示能力也与参数量成正相关。

很显然，固定容量的模型，随着任务越来越多，模型也不能容纳所有的知识，知识必须有所舍弃，各任务上的效果也会打折扣，灾难性遗忘也就越严重。这个问题是不可能解决的，但是可以缓解这个问题。一个好的持续学习算法能让模型尽量记住任务重要的知识，在需要舍弃知识时舍弃不重要的，减缓遗忘的速度。

在持续学习中，每个任务会占据模型的一部分容量，任务之间也会共用部分容量（根据任务相似性）。但是如果不加限制，第一个任务学习后就会很自然地占满所有模型容量，这样不仅容易导致第一个任务的过拟合（因为通常适合持续学习多个任务的模型要比适合第一个任务的模型要大很多），也让后面的（与第一个任务不太相似的）任务无处占据模型容量，导致后面的任务效果都变差。所以，**如何处理第一个任务的学习方式**是很重要的，也应验了那句俗语：万事开头难。


