---
title: 学习笔记：卷积神经网络
date: 2021-04-20
categories: [科研]
tags: [课程笔记]
img_path: /assets/img/
math: true
---

本笔记将系统整理学过的**卷积神经网络（CNN）**的知识。写笔记时我正参加信科那边的《图像理解》《神经网络与深度学习》等课程，另有参考课程：[Stanford CS231n: Convolutional Neural Networks for Visual Recognition]()。


# CNN 网络结构

卷积神经网络是一类**网络结构**。一般的卷积神经网络通常由一系列卷积层、池化层组成，有的还有反卷积层。

## 卷积层

### 卷积层的定义

**卷积层**（Convolutional Layer）是一种适用于图像数据的层。它的输入、输出都是三维张量（设 $$W \times H \times C$$），即 $$C$$ 个 $$H\times W$$ 矩阵，称为**特征图**（feature map），$$C$$ 称为深度或通道数。例如 RGB 图像就是 3 张特征图，它可以直接在输入端连接一个卷积层。

卷积层的操作可以借助卷积核的卷积操作来理解。**卷积核**是与输入深度相同的三维张量（$$F_1 \times F_2 \times C$$），里面填充的是该层的权重参数。卷积核按照固定的方式扫一遍输入特征图（起到 filter 的作用），完成其卷积操作，得到一张输出特征图。一个卷积层有多个**卷积核**（要求尺寸相同，否则输出特征图尺寸也不同，无法堆叠），从而输出多张特征图。

![](convolution.webp)

卷积操作如上图所示：卷积核（黑框框）它卡在输入特征图某位置时，卷积核与该位置作内积（即对应位置元素相乘，最后相加），再加上 bias 参数，经激活函数激活，得到输出特征图上的一个数（黑框框汇聚的点）。卷积核扫过不同的位置得到不同的数，它按照固定的步长来扫，得到的数也可以排列为成矩形（图右的网格），即一张输出特征图。卷积核扫描的方式与范围由两个超参数决定：

- **步长**（stride）：每次扫描跨过的像素数，分为水平、垂直两个方向：$$(S_1, S_2)$$；
- **填充**（padding）：在图像周围可以填充出边框，该边框的宽度，也分水平、垂直两个方向：$$(p_1,p_2)$$。

填充的作用是不遗漏地利用图片边角信息，它的大小通常让卷积核刚好覆盖到角落即可。通常的填充方法：
- 以 0 填充（zero padding）；
- 复制相近位置的值。

> 卷积的数学定义原指与倒序作内积。但不必纠结卷积是否符合这个定义。
{: .prompt-info }

如果将输入输出的特征图拉直成 1 维，可以发现卷积层也像全连接层那样：输入神经元通过权重连接到输出神经元，每个输出神经元对应 bias 与激活函数。矩阵化表示下，$$\mathbf{a}^k = \mathbf{W} \mathbf{o}^{k-1} + \mathbf{b}$$，权重矩阵 $$\mathbf{W}$$ 就是由卷积核扩展出来的，它有如下特点：

- **局部连接**：不是所有神经元之间都有权重连接，$$\mathbf{W}$$ 很稀疏；这样就有了**感受野**（receptive field）的概念，一个神经元并不是受上层所有神经元的影响，而是一部分（越往上层范围越广），这个范围称为该神经元的感受野。
- **权重共享**：$$\mathbf{W}$$ 有很多共享的变量，来自于同一卷积核里的权重。它们相当于复合函数中一个变量复合了多次，值与梯度在任何时刻都相同，等价于同一个参数。bias $$\mathbf{b}$$ 也是共享的，一个卷积核只有一个 bias。

以上两个特点使得卷积层的参数量比全连接情形大大减少，详见下面数目计算。

另外，卷积核的这种操作使得卷积网络有如下优点：

- 保留空间信息：与拉直的全连接层相比，它的连接方式能体现出图像中位置的概念，更适合图像；
- 适应图像尺度变化：由于可以使用不同尺寸的卷积核（注意：不是在同一个卷积层中使用！），大的局部和小的细节都能关注到。


### 数目计算

卷积神经网络这块需要会一些小小的算数。我们将以上出现的参数汇总一下：设输入特征图大小分别为 $$W \times H \times C$$，卷积层超参数为：
- $$K$$ 个卷积核；
- 卷积核尺寸为 $$F_1 \times F_2$$；
- 步长为 $$(S_1, S_2)$$；
- 填充为 $$(p_1, p_2)$$

**问题 1：输出特征图大小 $$W' \times H' \times C'$$？**
以水平方向为例，首先，两边加上填充后实际有效的图宽度为 $$W_1+2p_1$$；只观察卷积核的右边界，留给它扫描的宽度为 $$W_1+2p_1 - F_1$$，它能扫 $$(W_1+2p_1-F_1)/S_1$$ 次；注意 “扫” 的动作是间隔，所以还应加 1，即 

$$W' = (W+2p_1-F_1)/S_1 + 1$$

同理 

$$H' = (H+2p_2-F_2)/S_2 + 1$$

有几个卷积核就有几张特征图，所以 

$$C'=K$$

> 最好选取合适的超参数，使得公式中的除法可以除尽。除不尽时，向下取整，因为宁愿丢弃也不想引入无用的特征。
{: .prompt-warning }

以水平方向为例，通常来说，W+2p_1-F_1 不会比 W 还大，因为填充通常是正好让卷积核覆盖到角落，即 $$p_1 = floor(F_1/2)$$，更大的 padding 也没有意义；所以；步长 $$S_1$$ 最少为 1，最终 $$W'\leq W$$，同理 $$H' \leq H$$。当 $$W'(H')<W(H)$$ 卷积层会让特征图变小，起到某种**下采样**（subsampling）的作用。

> 当不想有下采样的效果，不想损失信息时，即 $$W'(H')=W(H)$$ 的情况。只要设置步长 $$S=1$$，填充$$p = floor(F/2)$$。试想如果没有 padding 机制，$$W'(H')=W(H)$$ 就无法实现，除非使用 $$1\times 1$$ 的卷积核。
{: .prompt-info }

**问题 2：卷积层参数数量？**
权重全部存放在卷积核中，因此一个卷积核有 $$F_1\times F_2 \times C$$ 个权重，共有 $$K$$个卷积，所以权重数量为 $$F_1\times F_2 \times C \times K$$。

一个卷积核只有 1 个 bias，因此 bias 数量为 $$K$$。

如果该层改为全连接层，权重数量为输入和输出神经元数量相乘，即 $$W\times H\times C\times W'\times H'\times C'$$，bias 数量为输出神经元数量 $$W'\times H'\times C'$$，都远大于卷积层的数量。

一个实际计算例子，感受一下数量级：输入 $$32\times 32\times 3$$，10 个 $$5\times 5$$ 1 步长 2 填充的卷积核，输出 $$32\times 32 \times 10$$。卷积层参数个数为 $$5\times 5\times 3 \times 10 + 10 = 760$$，改为全连接层参数个数为 $$32\times 32\times 3\times 32 \times 32 \times 10 + 32\times 32 \times 10 = 31467520$$。

### 可视化

卷积层的输入输出都是特征图，和彩色图像类似，可以通过一定手段将其可视化。

。。。。。怎么画出来每一层的特征的，可视化。一种是激活函数最大值，一种是直接输入得到的 feature map


### 反卷积层

**反卷积层**（Transposed Convolution），有时称去卷积（Deconvolution）。


分数步长的卷积（fractional-strided convolution）。


### 实践经验

卷积核通常在横竖方向不作区分，即 $$F_1=F_2, S_1=S_2, p_1=p_2$$。另外填充通常是正好让卷积核覆盖到角落，由卷积核大小决定，即 $$p = floor(F/2)$$。常用超参数：

- $$3\times 3$$，步长为 1，填充为 1；
- $$5\times 5$$，步长为 1 或 2，填充为 2；
- $$1\times 1$$，步长为 1，填充为 0。$$1\times 1$$ 的卷积核只起到了融合每个像素深度方向信息的作用，但它也是有用的。

反卷积层一般用在需要上采样的场景，例如语义分割任务，见讲 CV 任务的笔记。




## 池化层

**池化层**（Pooling Layer）是对特征图进行下采样的层，输入和输出都是特征图（三维张量），它们深度相同，输出特征图的尺寸相比输入是压缩的，详见下面数目计算。

### 池化层的定义

池化层是纯操作层，**不带任何参数**。池化操作也涉及 filter，大小为 $$F_1\times F_2$$，也有步长和填充的概念。注意，池化层通常不在深度方向下采样（会损失很多信息），即它在特征图所有深度方向上的操作是相同的，所以 **filter 无所谓深度**。可以将输入特征图按深度看作一张张二维图，经池化操作后得到同样数量的二维图，再按深度堆叠起来得到输出特征图。为了实现下采样，操作有：

- **最大池化**（Max Pooling）：取 filter 框住所有元素的最大值作为输出；
- **平均池化**（Average Pooling）：取 filter 框住所有元素的平均值作为输出。

![](pooling.gif)

示意如上图。注意此图只展示的是一个深度的池化操作。

> 赤化野核卷积也能起到downsampeld 的操作，为什么不只用。也有全卷积

### 数目计算

池化层没有参数，只涉及输出大小的计算。设输入特征图大小分别为 $$W \times H \times C$$，池化层超参数为：
- filter 尺寸为 $$F_1 \times F_2$$；
- 步长为 $$(S_1, S_2)$$；
- 填充为 $$(p_1, p_2)$$

输出大小的计算与卷积层类似，只是特征图深度不变：

$$W' = (W+2p_1-F_1)/S_1 + 1$$

$$H' = (H+2p_2-F_2)/S_2 + 1$$

$$C'=C$$

池化层能大大减少特征数量，起到了缩减模型的作用。


### 上池化层

上池化层与与反卷积层类似，起到上采样的作用。

### 实践经验

池化层的 filter 一般不作填充，步长更倾向于等于 filter 尺寸（即不重叠地采样）。通常也在在横竖方向不作区分，是正方形的，即 $$F_1=F_2$$。常用超参数：

- $$2\times 2$$，步长为 2，填充为 0；
- $$3\times 3$$，步长为 2，填充为 0。

通常不用上池化层来作上采样。


# 通用流行的 CNN 架构

本章讲历史上流行并取得巨大成就的 CNN 架构，一般是通用地用在图像分类的，其他针对具体 CV 任务的放在 CV 笔记中写。

第一个引起关注的是 1998 年的 LeNet，它是一个很简单的网络


之后就是深度学习时代的大型 CNN 网络了，主要成功的有 AlexNet，VGG，GoogLeNet，ResNet。由于细节太多，不打算一个模型分一节来讲其所有细节，而是放在一起作各种对比，并就其中的几个重要设计思想进行讲解。


ILSVRC


## 对比：参数数量



## 对比：



## GoogLeNet 的 Inception 层

大小不变，channel  深度增加/

1\times 1 减少深度 bottleneck

## 残差块

残差块可以是多层

残差块之间是相互堆叠的。


## 实践经验

如果想用这些架构做自己模型的 backbone，通常无脑选择 ResNet。因为