---
title: 代码学习笔记：从论文 HAT 学会写持续学习代码
date: 2022-10-09
categories: [科研]
tags: [代码笔记, 持续学习]
img_path: /assets/img/
math: true
---


本文是我看论文代码整理的总结。下面这篇论文是持续学习领域的一个经典工作，我曾经以此论文为样板，完整仔细地看过整个项目的代码，从而理解并开始自己实现持续学习的实验的，现将心得整理于此。这篇总结将详细介绍代码的各个细节，目的是一站式搞懂一个持续学习项目乃至深度学习项目的写法，也提供了一种自外而内剥洋葱式阅读他人代码的思路，供刚入门该领域的小白同学参考。

这个 HAT 项目的代码有很多地方写的比较乱，注意取其精华，掌握思想，并了解代码不合理的地方在哪（我会在笔记中指出）。

> 看懂这篇笔记的先修条件是掌握 Python 和 PyTorch，以及 Linux 系统的基本使用，并了解深度学习和持续学习的基础知识，请参考我的相关笔记：
> [Python 学习笔记]()；
> [PyTorch 学习笔记]()；
> [Linux 学习笔记]()；
> [持续学习基础知识]()。


# 论文信息 


### [Overcoming Catastrophic Forgetting with Hard Attention to the Task](https://proceedings.mlr.press/v80/serra18a.html)
- 会议：ICML 2018
- 作者：西班牙巴塞罗那的大学
- 内容：持续学习模型 HAT，是将 mask 机制加到持续学习的第一篇论文，提出了一个很简单的、每个神经元引入一个任务 mask 的方法，并给出了训练方法，和一个解决模型容量问题的稀疏正则项，让新旧任务 mask 重合。它属于参数隔离方法，之后很多带 mask 机制的持续学习论文以此篇为基础。


论文代码：<https://github.com/joansj/hat>



------------------------------

# 工程逻辑

从根目录开始，`src` 文件夹存放的是真正的代码，我们稍后讨论。根目录下的其他文件都是与代码没有直接关系的：
- `LICENSE`: 一个文本文件（可以看到没有后缀名），打开可以看到，里面的文字是在声明版权，告诉他人可以怎么用此项目、禁止怎么用（否则追究责任）。一般项目都会在根目录放一个这种名为 LICENSE 的文件，里面声明性的文字不需要自己写，去网上查查各种常见的选项（如 CC 4.0、MIT License 等），选一个合适的复制过来就好。在 Github 创建项目时有个选项可以选 license，之后会自动在根目录里创建 LICENSE 文件文本，很方便。在项目代码中大家一般是这样做，在其他例如文章、博客中也有其他方式声明版权，起到相同的作用，例如，可以看看我这个博客每篇文章结尾有一段话：“本文由作者按照 CC BY 4.0 进行授权，转载请注明”，点击就能跳转到 CC 4.0 指示的版权声明文本。
- `readme.md`: 顾名思义就是“请先读我”，它是作者写的对项目的描述性文本，给别人看的。可以在这里写任何想跟读者说的，例如使用说明等信息。在 Github 项目的首页也是默认展示这段文本（在 Github 创建项目时可以选择是否 add a README file）。众所周知在电脑上做笔记用 Markdown 格式很方便，现在的代码项目也是用它写（语法请自行学习），因为可以技术上方便地接到网页上显示，而不是 word 之类的文件（例如我的博客每一篇文章都是 Markdown 格式写的）。本项目由于是一篇论文的代码，所以作者主要在这里写了论文的信息，并简要介绍了程序的安装和运行方法。
- `requirements.txt`: 一个文本文件，列举了代码所需要的环境，放在项目根目录中，告诉别人运行此项目需要装什么第三方库。注意这个不一定是手打的或复制的，而是可以通过 Pip 或 Conda 生成的（当然有的时候自动生成的太过详细反而不合适，于是只需要手打一些重要的即可。本项目就做的不太好，把其他无关的环境里的包也包含进来了）。以下是相关命令：
  - `pip freeze >requirements.txt` 或 `conda list -e >requirements.txt`: 生成环境列表到 `requirements.txt`；
  - `pip install -r requirements.txt` 或 `conda install --file requirements.txt` 或 `conda create --name XXX --file requirements.txt`: 安装 `requirements.txt` 的环境到当前环境或新建环境。
- `.gitignore`: 一个文本文件，表示在上传到 Github 时应该忽略哪些文件（语法请自行学习），注意它用 `.` 开头，在 Linux 或 Mac 系统中代表隐藏文件，下同。这些文件通常是临时的、结果性质的、runtime 的非代码、非必要的文件，不需要上传占用空间让别人看到。在 Github 创建项目时可以选择是否 
add .gitignore。本项目忽略的文件有（其中一些文件在运行之后才会出现）：
  - `logs` 文件夹: 
  - `dat` 文件夹: 存放深度学习的数据集。它是非代码性质的数据文件，占用大量空间，不能上传。
  - `res` 文件夹: 存放实验结果，包括准确率结果、画图用的 pickle 文件，由下文所述 `--output` 参数指定；
  - `old`、`temp` 文件夹:
  - `src` 中的 `.idea` 文件夹:
  - `src` 中的 `__pycache__` 文件夹:
  - `pyc` 类型文件:
  - `.DS_Store` 类型文件: 是苹果系统
  - `.png`:
  - `src` 中的 `work.sh` 文件:
  - `src` 中的 `immalpha.sh` 文件:
  除此之外，还有一些我认为可能需要隐藏的：
  - `src` 中的 `.vscode` 文件:


下面看代码的 `src` 文件夹。有三个文件起到**程序入口**的作用：
- `run.py`: 是最基本的主函数。对于深度学习，在这里可以看到一次深度学习实验的整个流程：读取数据、训练、测试等等；
- `run_multi.py` 是用 `run.py` 改写的，负责运行多次深度学习实验；
- `run_compression.sh`: 是 Linux 系统的 shell 脚本命令（即命令行中的单个命令组合成的打包命令）。可以看到，这里它包含了多行运行 `run.py` 的命令行命令，当运行 `run_compression.sh` 时，相当于运行了里面写的这些命令。在这里的意思是，
- `work.py` 文件：
- `plot_results.py`: 本项目是先把结果存下来，再在需要时单独对其可视化。可视化与核心业务分离，这个 py 文件就是单独的可视化程序。

其他的是具体的模块代码：
- `approaches` 文件夹：定义各种持续学习算法，每个算法是一个 py 文件；
- `dataloaders` 文件夹：定义了数据预处理方法，每个数据集是一个 py 文件。由于持续学习数据集一般是现有数据集构造的，这里也定义了如何构造数据集；
- `networks` 文件夹：定义了神经网络结构，每个结构是一个 py 文件；
- `utils.py` 文件：




# 主程序

`run.py` 是整个项目的核心，它完成一次深度学习实验的整个流程。

## 解析命令行参数

一次深度学习实验需要指定很多东西：数据集、网络结构、学习算法、各种超参数，还有一些细节的配置如随机数的种子、输出格式等。这些信息一般是不出现在代码里的，而是作为运行程序时用户指定的参数，即命令行参数。关于如何使用 Python 命令行参数，我在[这篇博文](https://pengxiang-wang.github.io/posts/studynotes_Python_argparse/)有详细讨论。

`run.py` 定义命令行参数的部分在 9-20 行，解析命令行参数在 29-97 行。可以看到，它定义了如下 7 个命令行参数：
- `--seed`: 随机数种子。深度学习为了复现方便，公平，
- `--experiment`: 解析时通过 if 语句手动对应选择选项。根据命令行选项，把 `dataloaders` 文件夹中的模块统一解析到名为 `dataloader` 的变量中，在下面统一调用；
- `--approach`: 解析时通过 if 语句手动对应选择选项。根据命令行选项，把 `approaches` 文件夹中的模块统一解析到名为 `approach` 的变量中，在下文统一调用；
- `--nepochs`: 训练轮数，是比较重要的参数，需要用户手动指定；
- `--lr`: 学习率，是比较重要的参数，需要用户手动指定；
- `--parameter`: 为其他超参数的预留位置（因为每个 approach 的超参数都可以有所不同），具体是解析成几个、什么超参数，要看具体 approach 的定义；
- `--output`: 指定输出结果文件名路径，如果为空，21-22 行定义了默认的文件路径，可以看到是用`--experiment`、`--approach`等元信息命名的，用于区分不同实验的结果。


## 深度学习流程

接下来的代码对应深度学习流程：
- **读取数据集**（99-102行）：可以看到，所有 `dataloaders` 中的模块都只有一个 `get` 函数，在这里统一调用，用于得到数据集（包括训练集、验证集、测试集，作者的处理办法是先打包成一个 data 变量，再在训练或测试时抽离出来，见125、154等行），以及每个任务有几个类、输入维度等信息（用于定义网络）；
- **网络结构初始化**（104-107行）：可以看到，所有 `networks` 中的模块都只有一个 `Net` 类，在这里统一实例化为要训练的网络结构。实例化需要确定每个任务有几个类、输入维度等信息，来自上面数据集 `get` 函数的返回值；
- **定义学习算法**（109-112行）：可以看到，所有 `approaches` 中的模块都只有一个 `Appr` 类，在这里统一实例化为学习算法。粗略阅读其代码可发现，这种 `Appr` 类：
  - 不仅定义了持续学习的机制（因此实例化时需要传入持续学习有关的超参数，作者的做法是把 `args` 整个传进去，例如 `/approaches/hat.py` 中27-31行解析了 `args.parameter` 为 lamb 和 smax 两个超参数，用户在传 `--parameters` 时就知道 `--parameter` 代表这两个超参数）；
  - 还把优化器和损失函数一并包进来定义，因此实例化时需要指定优化器的超参数、训练轮数等，这些都在命令行参数 `args` 里；
  - 请注意，`Appr` 类还把网络也包进来作为实例属性了，从这里开始程序不再出现网络 `net` 变量；
- **训练**（148-149行）：统一调用 `Appr` 类的 `train` 方法，它接受上面抽离出来的训练集和验证集，以及第几个任务这个信息。注意不需要传网络，它在 `Appr` 里面，这个训练函数本质上也是在修改更新它；
- **测试**（152-159行）：统一调用 `Appr` 类的 `eval` 方法，它接受上面抽离出来的测试集，以及第几个任务这个信息。注意这里外层有个 u 循环，是要测试所有任务的。仍然不需要传网络；
- **保存结果**（161行-）：测试时把准确率存放到上三角矩阵 `acc` 中，t行u列表示训练完第t个任务时模型在任务u的准确率，这个是持续学习最主要的指标（见[持续学习基础知识笔记]()）；此外，在 178 行之后把一些结果信息用 pickle 保存下来（pickle 是一个 Python 内置库，可以完整保存、还原任意 Python 变量），在需要画图的时候被 `plot_results.py` 还原调用。


## 打印调试信息

作者穿插了各种 print 调试信息在代码中。额外把输入的参数信息打印了一遍，供用户参考（23-27行）：
- 








# Dataloaders



# Networks



# Approaches



# 代码的不足之处（个人理解）

调用 API 不统一。
给用户的说明不够详细，例如 --parameter 疑惑。


## 在服务器上运行的过程

申请账户，
按照说明。
选择节点，salloc，sbatch
salloc 的坏处是需要挂着。服务器维持会话，挂后台。python 命令 hangup，tmux





每个方法要和原论文方法差不多，大概调一调，但也没必要给他调的很好。超参数不一定一样。
别人的结果调过的，调到。

tqdm