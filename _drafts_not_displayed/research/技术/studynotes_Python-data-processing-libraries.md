---
title: Python 学习笔记：搞懂各种数据处理相关库
date: 2019-05-05
categories: [科研]
tags: [学习笔记, 技术]
img_path: /assets/img/
---

我学习 Python 的第一目的就是数据处理。和大多数人一样，我会接触各种各样的数据处理的库，主要的有：Numpy、Pandas 以及各种深度学习框架。它们很容易混淆，比如都能用矩阵表示数据，能进行线性代数运算，等等。那么，除了 API 形式不一样，它们的内在区别和联系是什么？什么时候该用 Numpy，什么时候 Pandas？这些是困扰过我的问题，也是本文想探讨的。

此外，本文也是一篇并不系统的 Numpy 和 Pandas 的学习笔记。深度学习框架 PyTorch 的学习笔记我将以《Dive into Deep Learning》读书笔记的形式呈现。

- Numpy 官方文档：<https://numpy.org/doc/stable/index.html>
- Pandas 官方文档：<https://pandas.pydata.org/docs/>
- PyTorch 官方文档：<https://pytorch.org/docs/stable/index.html>

# 应用场景

首先应该理解这些数据处理有关库的定位，根据其定位选择使用：

- **Numpy**：是 Numerical Python 的缩写，即 Python 扩展了数值计算功能。所以它的地位就是只要 Python 涉及到处理矩阵、向量、张量等的数值计算，就用它；
  - math 模块仅提供了简单的数学函数，它只能处理数值，不能处理向量、矩阵。math 模块有的功能 Numpy 都包括了。
- **Pandas**：可以理解为 Python 中的 Excel。Pandas 名称由 “panel data” 衍生而来，它处理的是 Excel 那样的表格数据，各种操作也和 Excel 类似；
- **深度学习框架**：重新封装了 Numpy，让其更好地为深度学习任务服务。功能上看是 Numpy 的扩展，增加了深度学习相关功能（如梯度）；在内部实现上也使其可以在 GPU 上计算。在牵扯到要训练大型深度学习模型的数据处理任务时，用这些框架是最方便的。

本文重点是介绍以上三个。其实 Pandas 和深度学习框架都是基于 Numpy 的（安装时可以看到会把 Numpy 一起安装）。其他基于 Numpy 的库（以下库操作的基本数据结构都是 Numpy 类型）：

- **Matplotlib**：数据可视化工具，画各种数据图；
- **Sklearn**：机器学习库，内置了各种机器学习模型，除了大型深度网络基本都有。适用于任务不太重的轻量深度学习。

以机器学习的工作为例，能用到哪些框架？机器学习的流程主要的三块：

- 数据预处理：最好用 Pandas，它的数据结构支持非数值类型、缺失值，且这种类 Excel 功能方便；清洗之后再转换为 Numpy 或深度学习框架的类型；
- 训练模型：视模型类型而定，大型深度网络用深度学习框架，简单网络或其他模型可用 Numpy + Sklearn；
- 呈现结果：此时 Matplotlib 可能派上用场。

> 注意到以上存在不同库的数据结构转换问题，将放在下面的“兼容性”一节讲述。

# 基本数据结构

先说结论：
| 库      | 基本数据结构名称  | 意义                             |
| ------- | ----------------- | -------------------------------- |
| Numpy   | array             | 数学上的张量                     |
| Pandas  | Series, DataFrame | Excel表格                        |
| PyTorch | tensor            | 数学上的张量（外挂深度学习功能） |

## 张量

**张量**是一个广义的数学概念，它包括数、向量、矩阵以及更高阶的东西。Numpy 及深度学习框架的基本数据结构 array、tensor 等都是张量。

张量可以用一个用中括号嵌套的字符串表示，在构造或打印的时候，都是以这种格式表示的。例：

- 0阶（数）：`1`
- 1阶（向量）：`[1,2,3]`
- 2阶（矩阵）：`[[1,0,0],[0,1,0],[0,0,1]]`
- 3阶：`[[[1,2],[3,4]],[[-1,-2],[-3,-4]]]`

张量的阶数即中括号的嵌套层数。张量的**形状**可以用一个 **size 数组**表示，上面几个例子形状分别为 `[], [3], [3,3], [2,2,2]`。

> 可以看到，张量的**阶数**是严格规定的，在数学上 $$n$$ 维向量经常可以看成 $$n\times 1$$ 的矩阵，但是在包括 PyTorch、Numpy 这些框架中，就是 1 阶张量和 2 阶张量的区别，使用不当会报错。同理，数 `1` 和1阶张量 `[1]`、2阶张量 `[[1]]` 是不一样的。
{: .prompt-warning }

### 广播机制

**广播**（broadcast）机制是指两个**形状不同**的张量执行**按元素操作**所遵循的规则。[官方文档](https://pytorch.org/docs/stable/notes/broadcasting.html)中规定了广播机制的限定条件：

![broadcast](/assets/img/broadcastable.png)

说白了就是比较两个张量的形状：如果阶数不同，则在最高阶处（size 数组最左边）补齐维数为 1 维；如果阶数相同，则检查每一维维数是否要么为 1 要么相等。

广播机制的操作是将两个张量 1 维的那些维度**复制** $$n$$ 份（$$n$$ 为对方对应维度的维数），再进行按元素操作。例：

` a = [[0],[1],[2]], b = [[0,1]], 则 a + b = [[0,1],[1,2],[2,3]] `

`a,b` 的形状分别是 `[3,1], [1,2]`，经过广播复制后分别变为形状为 `[3,2],[2,2]` 的 `[[0,0],[1,1],[2,2]], [[0,1],[0,1],[0,1]]`，再进行相加操作。

广播机制不仅是形式上的：经常会极大简化代码，主要还是效率上的：它在背后使用了比 for 循环高效得多的实现方式。

> 广播带给我们方便，也会偶尔带来麻烦：如果在无意识的情况下触发到它，可能就是一个很棘手的 bug。所以要写代码时要时刻盯好 tensor 的形状。
{: .prompt-tip }

## 兼容性相互转换

# Numpy 线性代数

既然 Numpy 的基本数据结构是张量，包含了数、向量、矩阵等，则线性代数应是其功能。
