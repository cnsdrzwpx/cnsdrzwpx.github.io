---
title: 可解释性与持续学习
date: 2022-10-08
categories: [科研]
tags: [论文笔记, 持续学习]
img_path: /assets/img/
math: true
---


# 可解释性的工作分类

维度一：intepretable model


维度三：
- Local 的，也不太行，因为无法反映整个信息。LIME 不行
- 从 global 的里面找吧

维度二：生成了什么解释
- 原则：生成人能看得懂的解释的，面向客户的，机器、代码没法利用的，如：生成 rules 的，LIME 不行
- 隐藏层特征，可以
- Attribution 可以
- Example-based 可以








## 可解释性辅助持续学习

用可解释性里的特征重要性方法，压缩持续学习模型
用解释性生成的实例选出样本点
解释信息当做下一个任务的训练信息

Active Interpretability 

Saliency map mix-up，
influence functions 增强样本





## 持续学习的可解释性

用已有的解释性方法找一找持续学习模型里的规律，看看有什么启发

找一找相关的论文，看看有没有分析具体某个模型的可解释性的就能发出来的


把可解释性套在持续学习上，需要突出持续学习的特色，如何突出？
- ICICLE：没怎么突出
- 通用的：
  - 解释为什么一个样本在训练完下一个任务之后，预测结果发生了改变，
    - 它的意义在于，对于本来分对了的样本后来分错，就是遗忘，有了这样的解释可以帮助诊断（这样写在论文里足够吗？）
    - 提出一些方法？
    - 在网络中加结构，特殊的可解释的持续学习网络
    - Mask + Network Dissection，先从人工的概念出发分析