---
title: 深度学习训练实践经验
date: 2021-04-20
categories: [科研]
tags: [课程笔记]
img_path: /assets/img/
math: true
---

深度学习并不是搭了模型喂了数据就保证出货的，训练过程的各种细节需要仔细处理，通过加一些 trick，才能训得好的模型，有很多经验上的东西。本文是[《学习笔记：深度学习训练理论知识》]()的实践篇，按照该笔记的顺序组织。



# 一、激活函数

激活函数一般的选择经验 ：ReLU > ReLU 变种 > Tanh > Sigmoid。

- 无特别情况一般选 ReLU 即可；
  - ReLU 虽然有 “挂掉” 的风险，但在实际中也就挂 10-20%，通常无大碍；
  - 使用 ReLU 时要注意调好配置，例如选用小的 bias 初始化（为让这些直线尽量穿过 data cloud）、学习率不要过大（防止不稳定进入 “挂掉” 状态）；
- ReLU 变种比较花哨，多是试验性质的，实践中用的也不多，可以试试；
- Tanh 偶尔也可以试试，不要太指望；
- Sigmoid 不要用，太古老了。


# 二、数据预处理


数据标准化几乎是每个深度学习项目必须要做的。注意图像数据通常只作零中心化，不作归一化，因为它的每一维（像素）范围都差不多。

分布偏移纠正非常冷门，且有作弊的嫌疑，了解深度学习中可能存在这样的问题即可，除了特定的科研一般不用。



# 三、网络结构


集成学习通常用在实际项目或比赛（如 Kaggle）中，通常能给结果带来固定的提高（如几个百分点） 。科研上因为研究具体的模型，不会去做模型集成，但有人也会用一些涉及集成的小 trick 提高效果。


# 四、参数初始化

- 不可使用全零初始化；
- 随机初始化是一种简单的方式，若想简单处理可以选用，不过要注意选好标准差；
- 一般使用 Xavier 初始化就能达到不错的效果。

实践中 “预训练 + 微调” 训练策略非常常用，因为一些大公司已经在超大型数据集上训练过大型网络，取得了惊人的效果，我们一般人没有这么强的计算资源，无法从头训练，还想分享到这些成果，就要 “预训练 + 微调” 了。

无论是 CV 还是 NLP 任务，都有很多大型网络的预训练权重可供下载，例如：

- ResNet 在 ImageNet 数据集（约 1400 万张图像）；
- BERT 的几个版本：BERT-base 在 BookCorpus 数据集（约 8 亿字）；BERT-large 在英文维基百科（约 25 亿字）；等等。
 
由此，通常设计深度模型的思路：用现有流行大模型作 backbone，作为通用的特征提取器，使用预训练权重，其后接剩下的部分负责具体的任务，由自己设计。

在微调时更新不能剧烈（即微调的字面意思），因为通常模型在预训练时已足够收敛，预训练数据往往比微调多。例如，一般设置预训练时 1/10 的学习率。

# 五、优化器

Adam 算法是最常用的、也是很多深度学习框架默认的优化器（Adam 论文引用有 10w 次！）。推荐的超参数：$$\rho=0.9, d=0.999, \eta=10^{-3}, 5\times 10^{-4}$$。

如果要单独使用的话，AdaGrad 与 RMSProp 相比倾向于不用前者。最原始的 SGD 也不推荐使用。

学习率调整可当作独立的 trick 施加在超参数 $$\eta$$ 上，这些 trick 常用于 SGD，在 Adam 等已有学习率衰减机制的优化器上不常用。实践中它的用法一般是先不加调整，观察效果，根据效果设计适当的调整机制。

注意，学习率不可设得太低，它对训练时间的影响比其他任何因素都厉害。

对于二阶算法 L-BFGS，实践发现它更适合于 full-batch 的深度学习。少数特定的深度学习项目需要 full-batch，平时见到的通常是 mini-batch 的，所以该算法也不常用。



# 六、损失函数

无特别需求一般选用 L2 正则化，即 weight decay。L1 正则化会给模型带来稀疏性，较少使用。

Smooth L1 损失出现于 Faster R-CNN 中。

# 七、超参数优化

验证集一般划训练集的 20-30%。

手动粗选的实践经验即调参经验。以下汇总一些 tips：

- 参数更新幅度的比值一般在 0.001 附近，过大或过小都可能有问题；
- （待更新）

对于搜索算法，无需过多地担心超参数之间不独立的情况，每次独立地调少量的超参数是问题不大的。一般一次独立地调 1, 2 或 3 个参数，不宜太多，否则将极大加重搜索算法的负担。


# 过拟合问题


可以看到，解决欠拟合、过拟合一般从模型复杂度下手。在实际项目中，通常的做法是设计多个不同复杂度的模型，分别在验证集上作出学习曲线进行比较，从中选择最接近“正常”学习曲线对应的模型，称为**模型选择**。

在实际项目中，通常需要跑很多天，epoch 是比较慢的，一般是时刻盯着 Tensorboard 画出的 loss 图，根据以上三种情况的特点判断欠拟合与过拟合。怎么把握 loss 多大才算充分下降、epoch 要忍几轮这个度？就是靠经验和多次尝试。

不可一开始就急着往模型上加防止过拟合的 trick。要在需要的时候、出现过拟合问题时一点点往上加。另外，有些过拟合技巧不可同时应用，例如 Dropout 和 Batch Normalization。

目前大部分网络使用的 Batch Normalization 防止过拟合，而且使用这个就够了。如果它不够用，再考虑正则化等手段。

深度学习框架中一般都有发现临界时刻、实现自动早停的 API，无需人手工指定最大 epoch 数、用肉眼观察。




