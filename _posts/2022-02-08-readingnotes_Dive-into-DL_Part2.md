---
title: 读书笔记：《动手学深度学习》Part 2：深度学习训练的问题及其解决方案
date: 2022-02-08
categories: [科研]
tags: [读书笔记, 《动手学深度学习》, 技术]
img_path: /assets/img/
math: true
---


## 书籍信息 

### [Dive into Deep Learning (PyTorch 版)](https://d2l.ai)
- 作者：亚马逊团队
- 本 Part 内容：第 4 章中间部分，介绍深度学习训练时可能出现的问题及其解决方案：过拟合/欠拟合、数值稳定性。
- 
------------------------------

# 一、过拟合、欠拟合

**欠拟合（Underfitting）**是模型没有充分探求训练集的规律，导致训练集和测试集效果都差。**过拟合（Overfitting）**是过分探求了训练集上的规律，导致虽然训练集效果好，但测试集效果差，模型泛化能力差。这两种情况都不是训练想得到的。

## 过拟合、欠拟合的判断

首先给出在训练过程中判断模型是否过拟合或欠拟合。

对于一个给定的模型，判断过拟合与否主要是通过**学习曲线（learning curve）**，见下图。此图的横坐标是训练的轮数。正如上一部分笔记所述，此图是可以实时画出来的，使用作者的 `Animator` 类或 Tensorboard 等工具。（注意上图是正常状态下画出的。还有两种不正常状态：总是欠拟合、总是过拟合，见下文）
![2](ML_learning_curve.jpg)

> 请注意，在实际深度学习流程中，测试数据是只能用于最终的测试，不可以辅助模型训练，所以此图的“测试集 loss”应当使用从训练集中分出来的数据，称为**验证集**。
{: .prompt-warning }

![3](d2l_always_underfitting.png)
![3](d2l_always_overfitting.png)
![3](d2l_not_overfitting_or_underfitting.png)


以上是书中多项式拟合的例子，分别是用一阶、三阶、五阶多项式拟合一组由三阶多项式生成的数据，分别对应：
- 总是欠拟合：两个 loss 都非常大，降不下去；
- 正常：两个 loss 都能充分下降，呈现出前面学习曲线的模样，有一个临界点：之前的是欠拟合，之后就慢慢变成过拟合了；
- 总是过拟合：训练 loss 能充分下降，但测试 loss 总是较大；而且之后训练很多 epoch 也不见得拉低二者的差距。

在实际项目中，通常需要跑很多天，epoch 是比较慢的，一般是时刻盯着 Tensorboard 画出的 loss 图，根据以上三种情况的特点判断欠拟合与过拟合。怎么把握 loss 多大才算充分下降、epoch 要忍几轮这个度？就是靠经验和多次尝试了。


## 解决过拟合、欠拟合的方法

在训练时判断出欠拟合与过拟合后，就该解决了。如果学习曲线出现以上“总是欠拟合”、“总是过拟合”，就将学习曲线变“正常”。

欠拟合和过拟合与模型复杂度、训练数据量有关。它们的关系如下图：

![1](ML_overfitting_and_complexity.jpg)

模型复杂度越高，越容易“总是过拟合”。反之，模型复杂度太低，则容易“总是欠拟合”。这里的模型复杂度是相对于训练数据量而言的，即越复杂的模型需要越大的数据量，若不匹配则会出现这两种情况。

解决“总是欠拟合”，只需提高模型复杂度，设计更复杂的模型即可，例如增加神经元等方法。这一点很容易做到，欠拟合的情况是容易处理的，因此也无需专门如何防止欠拟合的理论。
> 通过减少训练数据量不能解决欠拟合，一个“偷懒”的模型效果不可能好！而且这也是很蠢的行为，有数据为何不用呢？只需稍一动手把模型搞复杂点即可。
{: .prompt-warning }

解决“总是过拟合”，一个思路是收集更多的数据，提高数据量，但实际很难做到，训练数据量一般是固定的。另一思路就是降低模型复杂度，当然可以砍掉一部分神经元，但通常我们不想把辛辛苦苦设计的模型直接砍掉，这样太生硬，就想出一些花招：包括以下小节介绍的权重衰减、Dropout 等方法。

> 用“偏差-方差”理论来解释，这张图有两个维度：偏差（bias）和方差（variance），理想状态是低偏差和低方差。数据量越大，方差越小；模型越复杂，偏差越小。“总是欠拟合”对应高偏差，“总是过拟合”对应高方差和低偏差。
> ![2](bias_and_variance.png)
{: .prompt-tip }



> 可以看到，解决欠拟合、过拟合一般从模型复杂度下手。在实际项目中，通常的做法是设计多个不同复杂度的模型，分别在验证集上作出学习曲线进行比较，从中选择最接近“正常”学习曲线对应的模型。这个过程叫**模型选择**。
{: .prompt-info }

学习曲线“正常”化之后，还是需要找到临界时刻。临界时刻即测试 loss 开始升高时，继续训练会导致过拟合，令训练停在临界时刻的策略称为**早停**（early stopping）。深度学习框架中一般都有发现临界时刻、实现自动早停的 API，无需人手工指定最大 epoch 数、用肉眼观察。

### 权重衰减（L2 正则化）

**L2 正则化**又称权重衰减、岭回归，**L1 正则化**又称 LASSO、特征选择。以线性回归为例，**L2 正则化** 就是在损失函数中加正则项：

$$ L(\mathbf{w}, b) = \frac{1}{n} \sum_{i=1}^n SquaredLoss (\mathbf{w}^T \mathbf{x}^{(i)} + b, y^{(i)}) + \frac{1}{\lambda} \left\| \mathbf{w}\right\|^2$$

书 4.7 节讲解了加正则化的网络训练时的前向传播计算图长什么样、反向传播的公式推导，我直接跳过。


#### 从头开始实现

只需在前向传播 loss 中加入正则项：
```python
def l2_penalty(w):
    return torch.sum(w.pow(2)) / 2

#...
for epoch in range(num_epochs):
    for X, y in train_iter:
        l = loss(net(X), y) + lambd * l2_penalty(w)
        l.sum().backward()
        sgd([w, b], lr, batch_size)
        #...
```
一个小细节：`lambda` 是 Python 的关键字，不能占用，此超参数改写为 `lambd`。

#### 简洁实现

在 PyTorch 的高级 API，权重衰减功能在优化器中提供：
```python
trainer = torch.optim.SGD(
    [{'params':net[0].weight, 'weight_decay':3}, {'params':net[0].bias}],
    lr=lr)
```
相当于线性回归简洁实现中修改了优化器的 `params` 参数，现在仔细研究一下这个参数。[PyTorch 中文文档](https://pytorch-cn.readthedocs.io/zh/latest/package_references/torch-optim/)上写：

> Optimizer 也支持为每个参数单独设置选项。若想这么做，不要直接传入 Variable 的 iterable，而是传入 dict 的 iterable。

此时属于后一种情况。传入各参数的字典必须包含一个参数键值：`'params':参数`（其中“参数”为 `nn.Parameters` 类），其余的键值负责该参数的其他选项，如本例中的键 `'weight_decay':wd`，其值 `wd` 代表正则化系数 $$\lambda$$。当然，Optimizer 自己的构造参数（写在字典外面）能全局设置，例如本例 `lr=lr` 指定了整个学习率。如果与字典中的设置冲突，以字典为准。本例没有全局设置 `weight_decay`，因为不想给 bias 正则化。

### 暂退法（Dropout）

#### 从头开始实现

#### 简洁实现


# 二、数值稳定性

本节讨论数值稳定性问题，首先考察深度学习训练的主要算法——反向传播算法。

我们知道，根据链式法则，反向传播是按层迭代的：损失函数对一层神经元（激活前）$$a_j^k$$ 的梯度按层向后传播，传播就是不断乘以该层权重以及该层激活函数的导数：
 $$\delta_{j}^{k}=\sum_{l=1}^{r^{k+1}} \delta_{l}^{k+1} w_{j l}^{k+1} g^{\prime}\left(a_{j}^{k}\right)=g^{\prime}\left(a_{j}^{k}\right) \sum_{l=1}^{r^{k+1}} w_{j l}^{k+1} \delta_{l}^{k+1}$$

得到对第 k 层神经元的梯度后，再乘以第 k-1 层（激活后）$$ o_i^{k-1}$$ 就是对权重 $$w_{ij}^{k}$ 的梯度了：

$$\frac{\partial E}{\partial w_{i j}^{k}}=\delta_{j}^{k} o_{i}^{k-1}=g^{\prime}\left(a_{j}^{k}\right) o_{i}^{k-1} \sum_{l=1}^{r^{k+1}} w_{j l}^{k+1} \delta_{l}^{k+1}$$

详细公式推导参考[此文](https://brilliant.org/wiki/backpropagation/)。

可以看到，反向传播求得的梯度是前面一系列权重和激活函数导数的乘积。为了数值稳定性，应当保证权重和激活函数导数的稳定。尤其是对较后（靠近输入端）的权重求梯度时，会加倍地放大权重和激活函数不稳定带来的后果。这些后果及产生的原因有：

- **梯度消失**：随着反向传播，梯度与很小的数值累乘最后无限接近 0，使梯度下降参数更新非常缓慢。
  - 激活函数导数的问题：Sigmoid 激活函数的导数很小；
  - 权重的问题：权重初始化一开始就是 0 或非常接近 0。
- **梯度爆炸**：随着反向传播，梯度与很大的数值累乘最后也很大，使梯度下降参数更新非常剧烈不稳定。
  - 权重的问题：权重初始化太大。

当然，导致数值不稳定的根本原因是层数过多。

最后，还有一个**对称性陷阱**，就是对一个对称的网络的权重对称地初始化（例如全部初始化为同一个值），如果反向传播过程也是对称的（例如普通的 SGD 算法），最后训练出来地东西也是对称的，这样就和训练了一个简单得多的网络没区别了，影响网络的表达能力。只要打破一个地方的对称——初始化、优化算法、网络结构，最后训出来的东西也就不对称了。


## 解决数值稳定性问题的方法

上面已经暗示了一些解决方法：

- 网络层数不宜过多；
- 选用 ReLU 等非 Sigmoid 的激活函数，缓解梯度消失；
- 采用 Dropout 方法训练可以打破对称性陷阱；
- ...
  
除了这些方法，解决数值问题主要还是从**参数初始化**下手。上面已经提到参数初始化应避免的坑：

- 不可全是 0 或过于接近 0；
- 不可初始化太大；
- 注意尽量减少对称。

简单的**随机初始化**就能很好地避免的上面的问题。常用做法是使用正态分布，通常均值（取0）、方差是固定的。之前的例子都是我们手动初始化了的，对简洁实现的高级 API 来说，即使不手动初始化，框架也将默认使用随机初始化，说明此方法非常常用且普遍。

初始化是深度学习研究中的专门的领域，有大量的初始化方法被提出，例如书中又讲解了一种常见的 Xavier 初始化，这里不再详述。PyTorch 也提供了几十种初始化方法，[下一 Part]() 会系统讲解初始化的代码。


# 三、环境和分布偏移

机器学习假设训练数据和测试数据同分布，即都是从一个联合分布中取出来的：$$p_s(\mathbf{x}, y)$$。但实际数据可能并不满足这一条件，称为**分布偏移**，这种情况下测试效果肯定不好。


分布偏移有以下几种类型，下面是统计学上的理解：

- **协变量偏移**（covariate shift）：$$p_s(\mathbf{x}, y) = p(\mathbf{x}) p(y | \mathbf{x})$$，协变量 $$\mathbf{x}$$ 的分布 $$p(\mathbf{x})$$ 不同，而标签与协变量的关系 $$p(y | \mathbf{x})$$ 相同；
- **标签偏移**（label shift）：$$p_s(\mathbf{x}, y) = p(y) p(\mathbf{x} | y)$$，标签 $$y$$ 的分布 $$p(y)$$ 不同，而标签与协变量的关系 $$p(\mathbf{x} | y)$$ 相同；
- **概念偏移**（concept shift）：而标签与协变量的关系 $$p(y | \mathbf{x})$$ 不同。

协变量偏移和标签偏移其实是同一个问题的两种形式，比较常见，例如下面两幅图的情况；概念偏移是最严重的偏移，意味着训练数据和测试数据蕴含的知识不是一码事，例如二分类问题，训练集 1 代表猫、0 代表狗，到了测试集 1 代表黑物体、0 代表白物体，此时标签 1,0 的含义都不一样了，在分类猫狗上训练的模型完全不适用分类黑白物体。

![3](covariate_shift_1.png)
![3](covariate_shift_2.png)

有时候分布偏移并无大碍，尤其是非概念偏移，模型还是能正常工作。当模型想尽办法怎么调也效果不好时，就要考虑数据是否偏移过大，并解决分布偏移了。

识别是否有分布偏移必须比较训练数据和测试数据，通常是通过统计、观察来判断，至于分布偏移属于哪种类型，根据数据是判断不出来的，只能通过对当前问题的理解、经验来判断。



## 解决分布偏移的方法

如果判断是概念偏移，那没救了，出了从零开始收集新数据别无妙方；如果不是概念偏移，即协变量偏移和标签偏移，书中提供了两种偏移纠正算法：

协变量偏移纠正算法的思路。

标签偏移纠正算法就是把，计算量 维度小。上面说了，协变量偏移和标签偏移是一回事，根据计算需要来选择看成哪一种偏移，并应用相应的纠正算法。
