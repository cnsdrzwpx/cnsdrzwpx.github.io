---
title: 读书笔记：《终身机器学习》第 2 章：机器学习范式总结
date: 2022-05-11
categories: [科研]
tags: [读书笔记, 《终身机器学习》, 机器学习, 持续学习]
img_path: /assets/img/
math: true
---

## 书籍信息 



### [Lifelong Machine Learning (Second Edition)](https://www.cs.uic.edu/~liub/lifelong-machine-learning.html)


- 出版年月：2018 年 8 月
- 作者：
    - [Zhiyuan Chen](https://www.cs.uic.edu/~zchen)： Google 研究员，可能是后者的学生
    - [Bing Liu](https://www.cs.uic.edu/~liub)：伊利诺伊大学芝加哥分校，教授
    
------------------------------

<br>

本章讨论和终身学习相关的其他领域，阐述其定义，给出与终身学习的区别，再给出几个例子（例子有深度学习的，也有不是深度学习的）。我觉得把例子也仔细看一下为好，这是一个感性认识。

本章其实总结了**机器学习的学习范式**。经典的机器学习只关心一个任务，随着研究的发展开始关心**多个任务的机器学习**。对多个任务，数据的获取和学习逻辑不同，引出了不同的机器学习范式。目前常见的 5 类都列于此：**迁移学习、多任务学习、在线学习、持续学习、元学习**。以下这张图摘自一篇[持续学习综述]()，很直观：

![](machine_learning_paradigms.png)

# 一、迁移学习

**迁移学习**（Transfer Learning）和**领域自适应**（Domain Adaptation）是一个东西，搞 NLP 的经常称呼后者。其任务是有一个信息很多、有充足了解的源领域（source domain）和一个信息不足、需要探索的目标领域（target domain），通过源领域掌握的知识帮助目标领域的学习。通常的设定是，源领域的数据有标签，目标领域的数据无标签，以这些信息学习一个模型，能在源领域和目标领域表现都不错。

与终身学习的区别：
- 只涉及两个领域，只需把源领域的知识应用到目标领域即可。而终身学习涉及的领域会很多，常需要决定使用过去的哪些知识。
- 迁移是一次性的，不要求积累知识。
- 迁移学习的两个领域通常需要有较强的相似性（否则迁了还不如不迁）。终身学习没有这种要求。
- 没有 Backward Transfer。 Backward Transfer 是指终身学习中使用新任务中的学到的知识反过来提高旧任务的表现。
- 不会出现终身学习中某些复杂的要求，比如上一章我标在图中的⑤⑥。


# 二、多任务学习

**多任务学习**（Multi-Task Learning）的目标是**同时**学习多个任务（每个任务有自己的训练和测试数据），使模型在所有任务上表现都好。

关于模型是一个还是多个的问题，逻辑是这样的：由于要解决多个任务，所以至少输出头是有多个的。前面的层可共享也可以不共享，共享则毫无疑问是一个模型，不共享这几个模型一定也是有联系的（否则当作多个单任务学习好了）。所以就看成**一个**能解决多个任务的大模型好了，反正学习也是同时的。

与终身学习的区别是巨大的：多任务学习本质上是独立式学习，因为其同时性（可以将多任务看成一个大任务）。这一点是关键，点明即可。


# 三、在线学习

**在线学习**（Online Learning）是一个任务的训练数据不是直接全部获得的，而是一点点地按顺序到达的。目标仍是训练结束后的模型表现好，只是关注的点变成了时间、效率问题。

在线学习也是早就提出的问题，书中主要列举了一些非深度学习的例子，不详述。

在线学习虽然看起来很像终身学习的缩小版（数据都是 sequential 地到达的），但是有本质不同，区别就在于在线学习是解决单个任务的，本质上还是独立式学习。


# 四、强化学习

**强化学习**（Reinforcement Learning）不必多解释，这里关心的是它和终身学习等的关系。看上去强化学习是在不断 trial and error，有终身学习那味了，但实际上强化学习和这几个概念根本不是一码事，不是并列关系。强化学习应该与监督学习、无监督学习这些概念并列。

由于这种非并列的关系，就有很多强化学习与迁移学习、多任务学习乃至终身学习等结合的文章，见书 p32。

如果硬要比较，强化学习与终身学习区别在，尽管强化学习不停地试错、与环境交互、提升自己的性能，但本质上还是（在同一环境下）解决一个任务，有点类似在线学习与终身学习的关系。


# 五、元学习

**元学习**（Meta-Learning）目标也是利用一堆旧任务去帮助学习新任务，但其角度更高：它把这件事看作高级的普通机器学习，任务看作数据，一个个旧任务是训练数据点，新任务则是测试数据点。这种思想称为“学会学习“（learning to learn）。


元学习也是很早就有的概念，既有很早的工作，也在最近有火起来。见书 p33。


到目前为止，元学习应该是和终身学习最像的了，虽然站的高度不同，但目标都是差不多的。除了这一点，元学习和终身学习还有一大区别是：元学习通常假设任务是服从统一分布的（与普通机器学习一致），而终身学习任务可以有很大差异。

<br>

# 书中的例子

## 迁移学习例：Structural Correspondence Learning（非深度学习方法）

这是 NLP 中的工作，年代较久远，发在 EMNLP 2007 上，属于非深度学习方法（论文[在此](http://john.blitzer.com/papers/emnlp06.pdf)）。它解决的任务是：现有来自不同领域的两个任务（作者例：任务1，都是华尔街日报里的句子，标注词性；任务2，都是医学期刊 MEDLINE 里的句子，标注词性），任务1已完成——所有词已经被标注（即所有数据都有标签），任务2未完成——只有句子没有标注，想要利用此结果来指导完成任务2的标注。

SCL 的做法是抽取两个任务中共现频率较高的词作为 pivot features，构造关于这些词的二分类问题，学到的网络参数 $$W$$ 就浓缩了两个领域共有的知识，也就是要迁移的知识。使用此知识的方法是：用 $$W$$ 构造一个映射 $$\theta$$ 把输入 $$\mathbf{x}$$ 映射到另一个空间，用这个新输入 $$\theta \mathbf{x}$$ （也包含 $$\mathbf{x}$$）训练模型。这个模型理论上可以在两个领域上都有不错的效果。具体算法见下图：

![SCL](SCL_algorithm.png){:w='500'}

类似的非深度学习的迁移还有对朴素贝叶斯算法下手的，即 Naïve Bayes Transfer Classifier（NBTC），不详细讲了。

## 领域自适应例：深度学习

[Domain Adaptation for Large-Scale Sentiment Classification: A Deep Learning Approach](https://icml.cc/2011/papers/342_icmlpaper.pdf) 是用深度学习做领域自适应的开山之作，是 Bengio 等人 2011 年发在 ICML 的一篇文章。

此文解决的是情感分类问题，数据集是大量亚马逊上的商品评论，有的有标签，有的无标签（标签就是评论文本是“积极”还是“消极”）。想做的是：原来有很多完整带标签的数据，又有很多不带标签的，但是后者不用白不用嘛，就是想个办法把它们也用起来。但数据领域很多且都是混杂在一起的，怎么迁移学习？别搞麻烦了，只要把所有有标签的和无标签的看作源领域和目标领域即可。

文章的思想很简单，不是不给标签吗，直接用无监督呀！用的模型是 2008 年提出的一种自编码器（SDA），自编码器可以无监督地训练。后面再接一个 SVM 分类器，用有标签数据（源领域）训练，再在无标签数据（目标领域）上测试。我们能看到的是那个时候这种思想还是很先进的。

### 多任务学习：GO-MTL（非深度学习方法）

虽然不是深度学习，但也是参数化方法，每个任务都有一个模型 $$f^t(\mathbf{x}; \theta_t)$$ 对应。GO-MTL 的思想是这些参数 $$\theta_t$$ 是由一组基底 $$\mathbf{L} = (\mathbf{L}_1, \cdots, \mathbf{L}_k)$$ 组合出来的（$$k<$$ 任务数 $$N$$)：$$\theta_t = \mathbf{L}_t s_t$$。优化的损失函数也是所有任务（外加对 $$\mathbf{L}$$ 和 $$\mathbf{S}=(s_1,\cdots, s_N)$$ 大小的限制）：

$$\sum_{t=1}^{N} \sum_{i=1}^{n_{t}} \mathcal{L}\left(f\left(\mathbf{x}_{i}^{t} ; \mathbf{L s}^{t}\right), y_{i}^{t}\right)+\mu\|\mathbf{S}\|_{1}+\lambda\|\mathbf{L}\|_{F}^{2}$$

这个优化问题虽然很怪，但是可以解决的，有相应的算法，论文里用了交替优化算法，不用操心。

它是多任务而不是多个单任务，体现在把各任务模型参数用共同的一组基底联系了起来（$$k<N$$ 至关重要)。

### 多任务学习例：深度学习

书中的两个例子（multi-task DNN，TDNNs）都是一个大网络，前几层是共享的，后几层独立的、与任务关联的。以第一个例子为例，模型如下图。

![m](multi-task_example.png){:w='600'}

<br>

最后引用一句导师的话，表达一下对以上概念应采取的态度：

> 自己做研究时，不要被概念限制住，也不要硬套到哪一个场景中。
