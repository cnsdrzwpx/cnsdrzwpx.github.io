---
title: 持续学习基础知识
date: 2022-07-27
categories: [科研]
tags: [学习笔记, 持续学习]
img_path: /assets/img/
math: true
---

目前我的研究方向是持续学习。本文汇总了持续学习的基础知识体系，可以看作一篇综述吧，希望这篇笔记能带你进入我的研究领域。本文涉及的方法都是我觉得有代表性的，只介绍思想，不会非常详细地讲细节。


# 一、相关概念

相关概念已经在[《终身机器学习》读书笔记](https://pengxiang-wang.github.io/tags/终身机器学习/)的第一、二章中详细介绍过，这里只是简单概括一下。

**持续学习**（Continual Learning, CL）是多个任务的机器学习的一种学习范式。持续学习又称**终身学习**（Lifelong Learning），终身学习于很早提出，进入深度学习时代后研究者逐渐改叫持续学习；还有人把持续学习叫做**增量学习**（Incremental Learning）。

持续学习从大面来说是：多个任务数据**依次**交付给持续学习算法学习（每次**只有当前任务数据**），使得最终学到的模型能够胜任**所有**任务。“依次”和“所有”是持续学习的核心，缺一者就与只有单任务的机器学习无差别了：

- 缺“依次”：每次算法能用之前所有任务数据学习，则最后一个任务时就能学习所有任务，就不“持续”了。在实际场景中，旧数据主要是由于存储限制或隐私保护等原因而不可用的。
- 缺“所有”：若不要求在所有任务上表现都好（例：只要求当前任务），则就是当前任务的单任务的机器学习。

另外，持续学习也不允许每个任务都学习一个独立的模型，这样相当于多个单任务的机器学习。（参考下面讲解的模型容量分配问题）


持续学习与其他学习范式的主要区别：

- 在线学习：数据都是同一个任务来的，一定是独立同分布的；持续学习划出多个任务，数据不一定（不是一定不，但通常不是）是独立同分布的。且它的研究重点是“在线”与“离线”的区别；
- 迁移学习：重点关注的是“迁移”——如何利用旧任务的学习成果帮助新任务的学习；
- 多任务学习：数据是一次给完的，强调同时学习多个任务；
- 元学习：“学会学习”的角度更高，不只关注如何解决当前所有任务，还试图提取学习经验，泛化到新的任务。


# 二、持续学习关心的问题

以下先从较抽象的角度介绍持续学习关心的问题，之后再给出形式化定义和具体的例子。

## 灾难性遗忘

上面已经说过，在持续学习中，使用新任务数据训练模型使其在该新任务上效果好，是很容易做到的，只需应用成熟的单任务机器学习的算法即可；相反，很多时候学习新任务后，模型在旧任务上的效果会变差，这就是**灾难性遗忘**（Catastrophic Forgetting, CF），也是持续学习关心的核心问题。解决这一问题的方法是引入**防遗忘机制**，使模型保持旧任务上的效果。

从哲学上来说，假设模型的学习能力是固定的，模型在新任务上效果好，则在旧任务上效果会变差；反之，模型保持了旧任务上的效果，则在新任务上效果就不会好。前者是模型学习新知识的能力，称为**可塑性**（plasticity）；后者是旧知识的记忆能力，称为**稳定性**（stability）。可塑性与稳定性是内在相互矛盾的，术语叫**可塑性-稳定性困境**（Stability-Plasticity Dilemma），这是机器学习的一个天然的哲学约束，类似于 “没有免费午餐定理”。持续学习的目标是在所有任务上表现都好，即同时追求可塑性和稳定性；但这个困境说明了实现这一目标没有捷径，持续学习场景不是伪命题，并不是无脑加防遗忘机制、加强防遗忘的力度（例如调大防遗忘正则项超参数）就可以了，必须切实地提高模型的真本领。

## 后向迁移与前向迁移

除了灾难性遗忘作为核心问题，持续学习还关心算法是否具备：

- **后向迁移**（backward transfer）能力：学习后面的任务时，能否帮助到前面的任务；
- **前向迁移**（forward transfer）能力：学习前面的任务时，能否帮助到后面的任务。

试问，后向迁移与灾难性遗忘说的是一回事吗？因为学习后面的任务时，通常不会对前面的任务有正向的帮助，反而是负面的帮助——遗忘。算法的防遗忘机制加得狠，是否等价于提高后向迁移能力？

> 请注意用词：在后向迁移和前向迁移术语中，“前”是指旧任务方向，“后”是指新任务方向。而我平时习惯说“后”是指新任务。
{: .prompt-tip }


## 模型容量分配问题

持续学习的一大特点是学习任务的类型和数量没有预定义。在学习每个任务的期间，永远不知道未来有多少个任务、它们是什么样子的。之前所说的：每个任务学习一个独立的模型，其模型大小随任务量线性地增加。这样，模型尝试学习、记下每个任务所有的知识，对应的算法也是与普通机器学习没有差别，是持续学习不允许的。

我们不希望模型大小无序地膨胀，而是**固定模型容量**（capacity），让算法在固定容量的模型下完成持续学习（偶尔也会允许少量的膨胀）。这里所说的模型容量更多的是一个抽象概念，指模型的**表示能力**。当然，对于深度学习，模型的表示能力也与参数量成正相关。

很显然，固定容量的模型，随着任务越来越多，模型也不能容纳所有的知识，知识必须有所舍弃，各任务上的效果也会打折扣，灾难性遗忘也就越严重。这个问题是不可能解决的，但是可以缓解这个问题。一个好的持续学习算法能让模型尽量记住任务重要的知识，在需要舍弃知识时舍弃不重要的，减缓遗忘的速度。

在持续学习中，每个任务会占据模型的一部分容量，任务之间也会共用部分容量（根据任务相似性）。但是如果不加限制，第一个任务学习后就会很自然地占满所有模型容量，这样不仅容易导致第一个任务的过拟合（因为通常适合持续学习多个任务的模型要比适合第一个任务的模型要大很多），也让后面的（与第一个任务不太相似的）任务无处占据模型容量，导致后面的任务效果都变差。所以，**如何处理第一个任务的学习方式**是很重要的，也应验了那句俗语：万事开头难。



# 三、任务：分类问题

持续学习也分监督学习、无监督学习等，也有判别模型、生成模型。目前大家研究最多的是监督学习，且更多地关心分类问题。本文只讨论**分类问题**。

在分类问题中，研究者公认的持续学习场景有以下几个（主要是前两个）：

- **类别增量学习**（Class Incremental Learning, CIL）：每个任务包含若干不重复的类别；
- **任务增量学习**（Task Incremental Learning, TIL）：每个任务对数据的类别等信息不作要求，但数据中包含“属于哪个任务”这个信息；
- **领域增量学习**（Domain Incremental Learning, DIL）：每个任务包含的类别相同，但背后的领域不同。

与类别增量学习相对的是**每个任务类别相同的场景**，DIL 一定是，TIL 有可能是，也可能不是。

## 形式化定义

下面给出几个场景的形式化定义。设有任务 $$t=1,2,\cdots$$，每个任务的数据集为 $$\mathcal{D}^{(t)}$$，其中 $$\mathcal{D}^{(t)} =\{(\mathbf{x}_i,y_i)\}_{i=1}^{N_t} \in (\mathcal{X}^{(t)},\mathcal{Y}^{(t)})$$。算法在每个时刻 $$t$$ 利用 $$\mathcal{D}^{(t)}$$ 将 $$f^{(t-1)}$$ 更新 $$f^{(t)}$$，希望 $$f^{(t)}$$ 能完成目前涉及到的**所有**分类任务，即输入 $$\mathbf{x} \in \mathcal{X}^{(1)}\cup\cdots\cup\mathcal{X}^{(t)}$$，输出所有涉及到的类别 $$\hat{y} \in \mathcal{Y}^{(1)}\cup \cdots \cup \mathcal{Y}^{(t)}$$。

- 类别增量学习：$$\mathcal{Y}^{(t)}$$ 之间互不相交。可以记 $$\mathcal{Y}_1 = \{C_1,\cdots,C_{k_1}\}, \mathcal{Y}_2 = \{C_{k_1 + 1}, \cdots, C_{k_2}\}, \cdots$$；
- 任务增量学习：知道了输入的任务 ID $$t_{\mathbf{x}}$$（主要是对测试数据说的，训练数据天生是知道任务 ID 的），即目标简化为输入 $$\mathbf{x}\in \mathcal{X}^{(t_\mathbf{x})}$$，输出 $$\hat{y} \in \mathcal{Y}^{(t_\mathbf{x})}$$；
- 领域增量学习：$$\mathcal{Y}^{(1)}=\cdots=\mathcal{Y}^{(t)}$$，但强调 $$\mathcal{X}^{(1)},\cdots,\mathcal{X}^{(t)}$$ 的不同。


注意点：

- 之前说过，持续学习过程中永远不知道之后有多少个任务。但在实际实验中，持续学习数据集是固定的，总任务数是固定的 $$T$$ 个（也为了计算指标方便），以上 $$t=1,\cdots,T$$，但是持续学习算法在任何时刻都禁止使用 $$T$$ 这个信息。
- 一定要强调上面加粗的“所有”二字。对于 CIL，很多人的误区是以为任务 $$t$$ 只在 $$\mathcal{Y}^{(t)}$$ 中分类，而事实是在 $$\mathcal{Y}^{(1)}\cup\cdots\cup\mathcal{Y}^{(t)}$$ 中分类（就是下图多头模型有无灰色箭头的区别）。所以 CIL 场景是比 TIL 场景要困难的。
- CIL 第一个任务至少要包含 2 个类，之后的任务没有限制。




## Baseline：多头模型

对于非每个任务类别相同的场景，类是越来越多的。而且系统不知道未来有哪些类，无法在一开始就把所有类包括进来，构造出输出头固定的分类器；只能每当出现新类，临时加入该类的输出头。

所谓的**多头模型**是指模型的主要部分（特征提取器 $$\varphi$$）由各任务共用，但输出端不固定，随时会引入新的输出头。因此模型参数会包含共享参数和任务独有（task-specific）的参数两部分，后者的比例应该是非常小的，所以即使它的数量线性增长问题也不大，是允许的。

下图是以多头模型为基础的持续学习最简单的算法，算是所有持续学习算法的 **baseline**。它用最简单的学习方式，并不是每个新任务都从头开始训练，而是用上一个任务的训练结果作为下一个任务的初始化。具体来说，上图模型参数分为网络共享权重 $$\mathbf{w}_0$$ 和每个类别独有的权重 $$\mathbf{w}_1,\mathbf{w}_2,\cdots$$。每遇到新类别都会引入新的 $$\mathbf{w}_i$$，都作（随机）初始化。$$\mathbf{w}_0$$ 在算法的最开始（随机）初始化，且在每个时刻 $$\mathbf{w}^{(t)}_0$$ 都会用 $$\mathbf{w}^{(t-1)}_0$$ 初始化。

![](continual_learning_baseline.pdf)


这个算法在持续学习论文里习惯叫做**微调（fine-tuning）**，因为直接拿上一个任务初始化的方式有微调上一个任务的意思。直观上看这种方式直接覆盖了上个任务的成果，很容易产生灾难性遗忘。尽管有每个任务特定的参数能防止遗忘，但它们占的比例太小，起的作用是远远不够的。因此这个模型可以认为**没有任何防遗忘机制**，是一个白板模型，大家研究的持续学习模型都是在其基础上引入自己的防遗忘机制的。




## 数据集

持续学习分类问题的常用**数据集**是通过机器学习的标准数据集划分、构造出来的。标准数据集例如常用的 MNIST、CIFAR-10、CIFAR-100、ImageNet 等。划分方式主要有两种：

- **分割**（split）：按照类别划分数据集为任务，用于 CIL；
- **置换**（permute）：对原数据集所有数据做一次相同的变换，得到一个任务，可以用于 TIL、DIL 等每个任务类别相同的场景。

以 MNIST 为例，可以构造 Split MNIST、Permuted MNIST 两种数据集。Split MNIST 按类别划分成（以 5 个任务为例）0v1, 2v3, 4v5, 6v7, 8v9；Permuted MNIST 每构造一个任务时就按相同方式打乱各图片像素的顺序。


# 四、持续学习的指标

持续学习的主要目标是让模型在所有任务上表现都好，因此持续学习的**指标**首先是**各任务平均指标**，其次关注其他关心问题上的表现，如**后向迁移能力**、**前向迁移能力**。这些指标都是持续学习过程训练的各模型在各任务上的单个指标计算出来的[^footnote]：

记 $$R_{\tau,t}$$ 表示时刻 $$\tau$$ 训出的模型在第 $$t$$ 个任务上的指标（例，对分类问题是准确率），注意每个任务都有自己的测试集 $$\mathcal{D}^{(t)}_{test}$$，$$R_{\tau,t}$$ 是用 $$\mathcal{D}^{(t)}_{test}$$ 做测试的。有以下指标：

- 各任务平均指标：$$ ACC = \frac1T \sum_{t=1}^T R_{T,t}$$，即**最后**得到的模型在所有任务上的平均表现；
- 平均后向迁移：$$ BWT = \frac1{T-1} \sum_{t=1}^{T-1} (R_{T,t}- R_{t,t}) $$，即任务刚开始学（$$t$$ 时刻）与学到最后（$$T$$ 时刻）效果之差，对所有非最后一个任务取平均。这个指标只衡量了最后一个任务的后向迁移情况。
- 平均前向迁移：$$ FWT = \frac1{T-1} \sum_{t=1}^{T-1} (R_{t-1,t} - \bar{b}_t)$$。$$\bar{b}_t$$ 是随机初始化并用 $$\mathcal{D}_t$$ 训练的模型效果（多次实验取平均），有点 $$R_{0,t}$$ 的意思，但不太一样。指标表示到在任务刚开始学但还没有学（$$t-1$$ 时刻）期间累积的知识（比较的对象是不使用 $$\mathcal{D}_1,\cdots, \mathcal{D}_{t-1}$$ 前向迁移的知识、而只使用 $$\mathcal{D}_t$$ 自己知识的结果 $$\bar{b}_t$$），对所有非第一个任务取平均。

在实验中，有人会观察这些指标随 $$T$$ 的变化曲线。也就是说，测试过程通常是每训练完一个新任务就对已涉及的所有任务测试一遍。

> 第一个指标的定义方式是公认的，后两者可能还有待探索。举个例子，FWT 中的 $$R_{t-1,t}$$ 在 CIL 场景下是无法计算的，因为在 $$t-1$$ 时刻压根就没有 $$t$$ 时刻新出现的类别，FWT 需要另外定义。
{: .prompt-tips }

> 以上指标都是以任务为单位作算数平均计算出来的，通常要求任务划分得比较均衡（事实上多数数据集是这样的，例如 CIL 每个任务的类数量相等），否则最好根据任务规模/难易加权平均。还有的指标转而以类别为单位算平均。
{: .prompt-tips }

在训练过程中需要监视**学习曲线**，持续学习有多个任务，就有多个独立的学习曲线（下图对角线上的图）。但持续学习的目标不只是让当前任务学好，更关注是否会灾难性遗忘，所以还需要监视模型在旧任务上的表现，于是得到更多的学习曲线（下图对角线上方的图）。下图是一个例子（来自 [EWC]() 论文），我暂且称为 “三角图”：

![](CL_learning_curve.png)

> 注意这些图画的是模型于各训练阶段在**整个测试集**上的表现。为了画这张图，需要每个一段时间就做一次完整的测试，其实很耗时间，但这时间不是算在训练时间内的，无所谓。
{: .prompt-tip }

一个好的持续学习算法应当在任务切换后（并不是瞬间）在旧任务上效果不变差太快，例如图中对任务 A，在训练结束切换至 B 时，准确率曲线 EWC 几乎不下降，而不加防遗忘机制的 SGD 就会迅速下降，说明 EWC 防遗忘性能比较优秀。

# 五、防遗忘机制概论

目前学界普遍承认防遗忘机制可以分成三大类：重演数据法、正则化法、网络结构法。


## 重演数据法

防止遗忘最直接的方式是获得旧任务的训练数据，称为**重演**（replay）数据。重演数据不能太多，当然不允许使用全部的旧任务数据（否则就不是持续学习了），通常要求限制固定的重演数据记忆容量 $$\mathcal{M}$$，需要精炼旧任务数据的信息。


重演数据法的三个要素：

- 在旧任务结束、新任务到来之前，获取重演数据的算法；
- 重演数据空间如何管理；
- 如何在新任务上使用重演数据。

由此可以划分出各类**重演数据法**。例如获取重演数据可以直接从旧任务抽取代表元（exemplar），也可以用生成模型生成（后者称为伪（pseudo）重演数据法）；重演数据在新任务上使用，可以直接当作普通数据构造 random batch，也可以有其他方法。


### iCaRL

**iCaRL** 是最早提出的重演数据法，也是非常 naïve 的想法，解决 CIL 场景。三要素：

- 获取重演数据：近邻法。对于任务 $$t-1$$，（在特征空间上）选取离真实数据中心 $$\mu=\sum_{i=1}^{\mathcal{D}_{train}^{(t-1)}} \varphi(x)$$ 最近的若干个点（不需要存储标签）构成任务 $$t-1$$ 的重演数据 $$\mathcal{M}^{(t)}_{t-1}$$。注意特征空间时刻都在随训练更新，不是固定的。
- 空间管理：总记忆容量固定，大小平均分配给任务。设记忆容量为 $$K$$ 条数据：$$t=1$$ 时全部分配给任务 1，获取 $$\mathcal{M}^{(1)}_1$$ 时上述“若干个”为 K 个；$$t=2$$ 时分配给任务 1,2 各 $$K/2$$ 条，获取 $$\mathcal{M}^{(2)}_2$$ 时为 K/2 个， $$\mathcal{M}^{(2)}_1$$ 要从 $$\mathcal{M}^{(1)}_1$$ 中舍弃 $$K/2$$ 条（按照获取时离中心的近邻顺序，舍弃较远的）；……以此类推。
- 使用重演数据：重演数据也当作普通数据构造 random batch 训练。区别在于，重演数据的标签不是存储下来的真实标签，而是输入到训练完任务 $$t-1$$ 的模型输出的标签。 


### GEM



## 正则化法

**正则化法**是对损失函数下手，对任务 $$t$$ 的分类损失函数加**防遗忘正则项**，引导训练过程考虑防遗忘。引导的方向不同，就导致了不同的正则化法。

$$\min_{\theta} L^{(t)}(\theta) = L_{FINETUNE}^{(t)}(\theta) + L_{REVIEW}^{(t)}(\theta)$$

其中 $$\theta$$ 指代持续学习的所有参数（可能随任务越来越多，也可能固定）；$$L_{FINETUNE}^{(t)}(\theta) = \sum_{(x,y)\in \mathcal{D}_{train}^{(t)}} L(f(x;\theta),y)$$ 即任务 $$t$$ 正常的分类损失；$$L^{(t)}_{REVIEW}(\theta)$$ 是需要设计的防遗忘正则项。

纯的正则化法禁止正则项中使用到重演数据，正则项只能从模型本身出发构造，对模型参数施加限制。



### LwF

**LwF**（Learning without Forgetting）是一个非常简单的防遗忘机制：在任务 t 训练开始前，先让任务 t 的数据 $$\mathcal{D}_{train}^{(t)}$$ 过一遍旧模型，得到旧模型分类的结果；在正式训练时，引导分类结果与旧模型分类结果靠近，模仿旧模型，达成防遗忘的作用。即加正则项：

$$L_{REVIEW}^{(t)}(\theta) = \sum_{x\in \mathcal{D}_{train}^{(t)}} L(f(x;\theta),f(x;\theta^{(t-1)}))$$

注意，在这个过程中没有用过重演的旧数据。用到是任务 $$t$$ 训练之初自然继承下来的旧模型。

这种简单的方法缺陷是致命的：旧模型的信息全部浓缩到了分类结果这个小小的标签中，信息量太少——因为达成一个分类结果的方式有很多，这样最终可能使得模型与旧模型只有 “形似” 而没有 “神似”。另外一个角度，模型会对同一个数据参考两个标签（旧模型的分类标签、真实标签），若二者不同，这种冲突不太合理；若相同，就没有引入正则项的必要了。


### EWC

**EWC**（Elastic Weight Consolidation）是正则化法中第一个取得重大影响的算法。

$$L_{REVIEW}^{(t)}(\theta) = $$


## 梯度操控法

正则化法通过修改损失函数影响反向传播，间接地改变了参数更新过程。我们也直接规定、操控训练的更新过程。

最常用的做法是直接操纵梯度，修改梯度的计算、梯度下降公式等，这里我称为 “**梯度操控法**”。

### RGO



## 网络结构法

网络结构法从网络结构下手，将网络划分成各部分并按某种机制分配给各任务，因此又称**参数隔离法**（Parameter Isolation）。它显式地体现了模型容量分配问题，将模型容量这一概念显化到模型各部分参数了。这也是一种直接规定、操控更新过程的方法。


### Mask 机制

使用 mask 来规定参数更新的范围。

### Evolution ?? Network

不断扩充网络的列的那篇论文

## 优缺点讨论

重演数据法最大的缺点是重演数据量不够。

直接操控更新过程的方法（包括梯度操控法、参数隔离法）有更好的可解释性，但也会带来僵硬的问题。（二者有冲突）



# 六、前沿方向

这里列举一些持续学习的其他前沿方向，不作详细介绍。

- 小样本持续学习：
- 持续异常检测；
- 持续学习 + ViT；
- ...


# 参考资料

以下列举一些持续学习相关的参考资料与学习资源。

1. 课程 Continual Learning（比萨大学）
2. 持续学习社区 Continual AI



<br>


[^footnote]: [Gradient Episodic Memory for Continual Learning]()