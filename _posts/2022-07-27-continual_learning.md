---
title: 持续学习基础知识
date: 2022-07-27
categories: [科研]
tags: [学习笔记, 机器学习, 持续学习]
img_path: /assets/img/
math: true
---

目前我的研究方向是持续学习。本文汇总了持续学习的基础知识体系，可以看作一篇综述吧，希望这篇笔记能带你进入我的研究领域。本文涉及的方法都是我觉得有代表性的，只介绍思想，不会非常详细地讲细节。


----------
**目录**

* TOC
{:toc}





---------


# 一、相关概念

相关概念已经在[《终身机器学习》读书笔记](https://pengxiang-wang.github.io/tags/终身机器学习/)的第一、二章中详细介绍过，这里只是简单概括一下。

**持续学习**（Continual Learning, CL）是多个任务的机器学习的一种学习范式。持续学习又称**终身学习**（Lifelong Learning）、**序列学习**（Sequential Learning），终身学习于很早提出，进入深度学习时代后研究者逐渐改叫持续学习；还有人把持续学习叫做**增量学习**（Incremental Learning）。

持续学习从大面来说是：多个任务数据**依次**交付给持续学习算法学习（每次**只有当前任务数据**），使得最终学到的模型能够胜任**所有**任务。“依次”和“所有”是持续学习的核心，缺一者就与只有单任务的机器学习无差别了：

- 缺“依次”：每次算法能用之前所有任务数据学习，则最后一个任务时就能学习所有任务，就不“持续”了。在实际场景中，旧数据主要是由于存储限制或隐私保护等原因而不可用的。
- 缺“所有”：若不要求在所有任务上表现都好（例：只要求当前任务），则就是当前任务的单任务的机器学习。

另外，持续学习也不允许每个任务都学习一个独立的模型，这样相当于多个单任务的机器学习。这种称为**独立式学习**（isloated learning）。（参考下面讲解的模型容量分配问题）


持续学习与其他学习范式的主要区别：

- 在线学习：数据都是同一个任务来的，一定是独立同分布的；持续学习划出多个任务，数据不一定（不是一定不，但通常不是）是独立同分布的。且它的研究重点是“在线”与“离线”的区别；
- 迁移学习：重点关注的是“迁移”——如何利用旧任务的学习成果帮助新任务的学习；
- 多任务学习：数据是一次给完的，强调同时学习多个任务；
- 元学习：“学会学习”的角度更高，不只关注如何解决当前所有任务，还试图提取学习经验，泛化到新的任务。


# 二、持续学习关心的问题

以下先从较抽象的角度介绍持续学习关心的问题，之后再给出形式化定义和具体的例子。

## 灾难性遗忘

上面已经说过，在持续学习中，使用新任务数据训练模型使其在该新任务上效果好，是很容易做到的，只需应用成熟的单任务机器学习的算法即可；相反，很多时候学习新任务后，模型在旧任务上的效果会变差，这就是**灾难性遗忘**（Catastrophic Forgetting, CF），也是持续学习关心的核心问题。解决这一问题的方法是引入**防遗忘机制**，使模型保持旧任务上的效果。

从哲学上来说，假设模型的学习能力是固定的，模型在新任务上效果好，则在旧任务上效果会变差；反之，模型保持了旧任务上的效果，则在新任务上效果就不会好。前者是模型学习新知识的能力，称为**可塑性**（plasticity）；后者是旧知识的记忆能力，称为**稳定性**（stability）。可塑性与稳定性是内在相互矛盾的，术语叫**可塑性-稳定性困境**（Stability-Plasticity Dilemma），这是机器学习的一个天然的哲学约束，类似于 “没有免费午餐定理”。持续学习的目标是在所有任务上表现都好，即同时追求可塑性和稳定性；但这个困境说明了实现这一目标没有捷径，持续学习场景不是伪命题，并不是无脑加防遗忘机制、加强防遗忘的力度（例如调大防遗忘正则项超参数）就可以了，必须切实地提高模型的真本领。

## 后向迁移与前向迁移

除了灾难性遗忘作为核心问题，持续学习还关心算法是否具备：

- **后向迁移**（backward transfer）能力：学习后面的任务时，能否帮助到前面的任务；
- **前向迁移**（forward transfer）能力：学习前面的任务时，能否帮助到后面的任务。

试问，后向迁移与灾难性遗忘说的是一回事吗？因为学习后面的任务时，通常不会对前面的任务有正向的帮助，反而是负面的帮助——遗忘。算法的防遗忘机制加得狠，是否等价于提高后向迁移能力？

> 请注意用词：在后向迁移和前向迁移术语中，“前”是指旧任务方向，“后”是指新任务方向。而我平时习惯说“后”是指新任务。
{: .prompt-tip }


## 模型容量分配问题

持续学习的一大特点是学习任务的类型和数量没有预定义。在学习每个任务的期间，永远不知道未来有多少个任务、它们是什么样子的。之前所说的：每个任务学习一个独立的模型，其模型大小随任务量线性地增加。这样，模型尝试学习、记下每个任务所有的知识，对应的算法也是与普通机器学习没有差别，是持续学习不允许的。

我们不希望模型大小无序地膨胀，而是**固定模型容量**（capacity），让算法在固定容量的模型下完成持续学习（偶尔也会允许少量的膨胀）。这里所说的模型容量更多的是一个抽象概念，指模型的**表示能力**。当然，对于深度学习，模型的表示能力也与参数量成正相关。

很显然，固定容量的模型，随着任务越来越多，模型也不能容纳所有的知识，知识必须有所舍弃，各任务上的效果也会打折扣，灾难性遗忘也就越严重。这个问题是不可能解决的，但是可以缓解这个问题。一个好的持续学习算法能让模型尽量记住任务重要的知识，在需要舍弃知识时舍弃不重要的，减缓遗忘的速度。

在持续学习中，每个任务会占据模型的一部分容量，任务之间也会共用部分容量（根据任务相似性）。但是如果不加限制，每个任务学习后就会很自然地占满所有模型容量，这样不仅容易导致任务的过拟合（因为通常适合持续学习多个任务的模型要比适合某个任务的模型要大很多），也让后面的（与该任务不太相似的）任务无处占据模型容量，导致后面的任务效果都变差。所以，需要在算法中加入一些**稀疏化**（sparsity）机制来解决模型容量不够的问题。



# 三、任务：分类问题

持续学习也分监督学习、无监督学习等，也有判别模型、生成模型。目前大家研究最多的是监督学习，且更多地关心分类问题。本文只讨论**分类问题**。

在分类问题中，持续学习场景按照如下两个分类维度划分：

- **任务类别是否相同**：有所有任务完全相同的、完全不相同的（每个任务来的都是新类），也有既有相同也有不相同的；
- **数据是否包含任务 ID**：包括训练数据与测试数据。（注：任务 ID 是指数据来自第几个任务的“几”）
    - 训练数据与测试数据都有任务 ID 信息：在测试时知道数据来自第几个任务；
    - 训练数据有任务 ID 信息，测试数据没有：在测试时不知道数据来自第几个任务，这样需要算法能够自行显式或隐式地判断任务 ID，更加困难；
    - 训练数据与测试数据都没有任务 ID 信息：没有任务 ID 信息就是没有明确的任务边界（task boundary），数据随任务是流式地进入算法的。

根据此分类，某些常见场景有习惯性叫法：

- **类别增量学习**（Class Incremental Learning, CIL）：每个任务包含若干不重复的类别，训练数据有而测试数据没有任务 ID 信息；
- **任务增量学习**（Task Incremental Learning, TIL）：训练数据与测试数据都有任务 ID 信息，对任务类别是否相同无要求；
- **领域增量学习**（Domain Incremental Learning, DIL）：每个任务包含的类别相同，但背后的领域不同。


## 形式化定义

下面给出几个场景的形式化定义。设有任务 $$t=1,2,\cdots$$，每个任务的数据集为 $$\mathcal{D}^{(t)}$$，其中 $$\mathcal{D}^{(t)} =\{(\mathbf{x}_i,y_i)\}_{i=1}^{N_t} \in (\mathcal{X}^{(t)},\mathcal{Y}^{(t)})$$。算法在每个时刻 $$t$$ 利用 $$\mathcal{D}^{(t)}$$ 将 $$f^{(t-1)}$$ 更新 $$f^{(t)}$$，希望 $$f^{(t)}$$ 能完成目前涉及到的**所有**分类任务，即输入 $$\mathbf{x} \in \mathcal{X}^{(1)}\cup\cdots\cup\mathcal{X}^{(t)}$$，输出所有涉及到的类别 $$\hat{y} \in \mathcal{Y}^{(1)}\cup \cdots \cup \mathcal{Y}^{(t)}$$。

- 类别增量学习：$$\mathcal{Y}^{(t)}$$ 之间互不相交。可以记 $$\mathcal{Y}_1 = \{C_1,\cdots,C_{k_1}\}, \mathcal{Y}_2 = \{C_{k_1 + 1}, \cdots, C_{k_2}\}, \cdots$$；
- 任务增量学习：知道了输入的任务 ID $$t_{\mathbf{x}}$$，即目标简化为输入 $$\mathbf{x}\in \mathcal{X}^{(t_\mathbf{x})}$$，输出 $$\hat{y} \in \mathcal{Y}^{(t_\mathbf{x})}$$；
- 领域增量学习：$$\mathcal{Y}^{(1)}=\cdots=\mathcal{Y}^{(t)}$$，但强调 $$\mathcal{X}^{(1)},\cdots,\mathcal{X}^{(t)}$$ 的不同。


注意点：

- 之前说过，持续学习过程中永远不知道之后有多少个任务。但在实际实验中，持续学习数据集是固定的，总任务数是固定的 $$T$$ 个（也为了计算指标方便），以上 $$t=1,\cdots,T$$，但是持续学习算法在任何时刻都禁止使用 $$T$$ 这个信息。
- 一定要强调上面加粗的“所有”二字。对于 CIL，很多人的误区是以为任务 $$t$$ 只在 $$\mathcal{Y}^{(t)}$$ 中分类，而事实是在 $$\mathcal{Y}^{(1)}\cup\cdots\cup\mathcal{Y}^{(t)}$$ 中分类（就是下图多头模型有无灰色箭头的区别）。所以 CIL 场景是比 TIL 场景要困难的。
- CIL 第一个任务至少要包含 2 个类，之后的任务没有限制。




## Baseline：多头模型

对于非每个任务类别相同的场景，类是越来越多的。而且系统不知道未来有哪些类，无法在一开始就把所有类包括进来，构造出输出头固定的分类器；只能每当出现新类，临时加入该类的输出头。

所谓的**多头模型**是指模型的主要部分（特征提取器 $$\varphi$$）由各任务共用，但输出端不固定，随着新类别的引入，随时会引入新的输出头。因此模型参数会包含共享参数和类别独有的参数两部分，后者的比例应该是非常小的，所以即使它的数量线性增长问题也不大，是允许的。

下图是以多头模型为基础的持续学习最简单的算法，算是所有持续学习算法的 **baseline**。它用最简单的学习方式，并不是每个新任务都从头开始训练，而是用上一个任务的训练结果作为下一个任务的初始化。具体来说，上图模型参数分为网络共享权重 $$\mathbf{w}_0$$ 和每个类别独有的权重 $$\mathbf{w}_1,\mathbf{w}_2,\cdots$$。每遇到新类别都会引入新的 $$\mathbf{w}_i$$，都作（随机）初始化。$$\mathbf{w}_0$$ 在算法的最开始（随机）初始化，且在每个时刻 $$\mathbf{w}^{(t)}_0$$ 都会用 $$\mathbf{w}^{(t-1)}_0$$ 初始化。

![](continual_learning_baseline.pdf)


这个算法在持续学习论文里习惯叫做**微调（fine-tuning）**，因为直接拿上一个任务初始化的方式有微调上一个任务的意思。直观上看这种方式直接覆盖了上个任务的成果，重新把所有的模型容量让新任务占满，很容易灾难性遗忘。（尽管有类别独有的参数能防止遗忘，但它们占的比例太小，起的作用是远远不够的。）因此这个算法可以认为**没有任何防遗忘机制**，是一个白板算法，大家研究的持续学习算法都是在其基础上引入自己的防遗忘机制的。

下图描述了持续学习算法与微调白板模型参数更新路径的对比，取自 [OWM 论文](https://www.nature.com/articles/s42256-019-0080-x)
，图中展示的是参数空间，上面的点是参数，两个圈分别代表任务 1、任务 2 的分类损失函数（等高线）。训练任务 1 后参数位于右上方点，在训练任务 2 时，采用微调白板算法参数会直接更新到左上方点，而采用有防遗忘机制的持续学习算法会更新到下方点。

![](continual_learning_update_routes.png)

## 数据集

持续学习分类问题的常用**数据集**是通过机器学习的标准数据集划分、构造出来的。标准数据集例如常用的 MNIST、CIFAR-10、CIFAR-100、ImageNet 等。划分方式主要有两种：

- **分割**（split）：按照类别划分数据集为任务，用于 CIL；
- **置换**（permute）：对原数据集所有数据做一次相同的变换，得到一个任务，可以用于 TIL、DIL 等每个任务类别相同的场景。

以 MNIST 为例，可以构造 Split MNIST、Permuted MNIST 两种数据集。Split MNIST 按类别划分成（以 5 个任务为例）0v1, 2v3, 4v5, 6v7, 8v9；Permuted MNIST 每构造一个任务时就按相同方式打乱各图片像素的顺序。


# 四、持续学习的指标

持续学习的主要目标是让模型在所有任务上表现都好，因此持续学习的**指标**首先是**各任务平均指标**，其次关注其他关心问题上的表现，如**后向迁移能力**、**前向迁移能力**。这些指标都是持续学习过程训练的各模型在各任务上的单个指标计算出来的[^footnote]：

记 $$R_{\tau,t}$$ 表示时刻 $$\tau$$ 训出的模型在第 $$t$$ 个任务上的指标（例，对分类问题是准确率），注意每个任务都有自己的测试集 $$\mathcal{D}^{(t)}_{test}$$，$$R_{\tau,t}$$ 是用 $$\mathcal{D}^{(t)}_{test}$$ 做测试的。有以下指标：

- 各任务平均指标：$$ ACC = \frac1T \sum_{t=1}^T R_{T,t}$$，即**最后**得到的模型在所有任务上的平均表现；
- 平均后向迁移：$$ BWT = \frac1{T-1} \sum_{t=1}^{T-1} (R_{T,t}- R_{t,t}) $$，即任务刚开始学（$$t$$ 时刻）与学到最后（$$T$$ 时刻）效果之差，对所有非最后一个任务取平均。这个指标只衡量了最后一个任务的后向迁移情况。
- 平均前向迁移：$$ FWT = \frac1{T-1} \sum_{t=1}^{T-1} (R_{t-1,t} - \bar{b}_t)$$。$$\bar{b}_t$$ 是随机初始化并用 $$\mathcal{D}_t$$ 训练的模型效果（多次实验取平均），有点 $$R_{0,t}$$ 的意思，但不太一样。指标表示到在任务刚开始学但还没有学（$$t-1$$ 时刻）期间累积的知识（比较的对象是不使用 $$\mathcal{D}_1,\cdots, \mathcal{D}_{t-1}$$ 前向迁移的知识、而只使用 $$\mathcal{D}_t$$ 自己知识的结果 $$\bar{b}_t$$），对所有非第一个任务取平均。

在实验中，有人会观察这些指标随 $$T$$ 的变化曲线。也就是说，测试过程通常是每训练完一个新任务就对已涉及的所有任务测试一遍。

> 第一个指标的定义方式是公认的，后两者可能还有待探索。举个例子，FWT 中的 $$R_{t-1,t}$$ 在 CIL 场景下是无法计算的，因为在 $$t-1$$ 时刻压根就没有 $$t$$ 时刻新出现的类别，FWT 需要另外定义。
{: .prompt-tips }

> 以上指标都是以任务为单位作算数平均计算出来的，通常要求任务划分得比较均衡（事实上多数数据集是这样的，例如 CIL 每个任务的类数量相等），否则最好根据任务规模/难易加权平均，例如有些文章使用调和平均（harmonic mean）。还有的指标转而以类别为单位算平均。
{: .prompt-tips }

在训练过程中需要监视**学习曲线**，持续学习有多个任务，就有多个独立的学习曲线（下图对角线上的图）。但持续学习的目标不只是让当前任务学好，更关注是否会灾难性遗忘，所以还需要监视模型在旧任务上的表现，于是得到更多的学习曲线（下图对角线上方的图）。下图是一个例子（来自 [EWC]() 论文），我暂且称为 “三角图”：

![](CL_learning_curve.png)

> 注意这些图画的是模型于各训练阶段在**整个测试集**上的表现。为了画这张图，需要每个一段时间就做一次完整的测试，其实很耗时间，但这时间不是算在训练时间内的，无所谓。
{: .prompt-tip }

一个好的持续学习算法应当在任务切换后（并不是瞬间）在旧任务上效果不变差太快，例如图中对任务 A，在训练结束切换至 B 时，准确率曲线 EWC 几乎不下降，而不加防遗忘机制的 SGD 就会迅速下降，说明 EWC 防遗忘性能比较优秀。

# 五、防遗忘机制概论

目前学界普遍承认防遗忘机制可以分成三大类：重演数据法、正则化法、网络结构法。


每个持续学习算法都应有一个对象在随任务数增加不断迭代地积累知识，这个对象统称为**记忆**（memory），记为 $$\mathcal{M}$$。在重演数据法中记忆中存放的是数据，而在其他方法中通常也需要记忆来记忆梯度、mask 等非数据信息。在防遗忘机制中，通常有一个支配**防遗忘程度的超参数**可供人工调节，例如正则化法中的正则项系数，实验时一般会比较不同的防遗忘程度。每个方法我都会讲清楚记忆是如何迭代积累的（如果有记忆），以及调节防遗忘程度的超参数是什么。





## 重演数据法

防止遗忘最直接的方式是在记忆中旧任务的训练数据，称为**重演**（replay 或 experience replay）数据。此时这块记忆通常称为 episodic memory。重演数据不能太多，当然不允许使用全部的旧任务数据（否则就不是持续学习了），通常要求限制固定的重演数据记忆容量，需要精炼旧任务数据的信息。


重演数据法的三个要素：

- 在旧任务结束、新任务到来之前，获取重演数据的算法；
- 重演数据空间如何管理，即记忆如何迭代积累；
- 如何在新任务上使用重演数据（Reservoir？）。




由此可以划分出各类**重演数据法**。例如获取重演数据可以直接从旧任务抽取代表元（exemplar），也可以用生成模型生成（后者称为伪（pseudo）重演数据法）；重演数据在新任务上使用，可以直接当作普通数据构造 random batch，也可以有其他方法。


### iCaRL

论文链接：[iCaRL: Incremental Classifier and Representation Learning](https://openaccess.thecvf.com/content_cvpr_2017/papers/Rebuffi_iCaRL_Incremental_Classifier_CVPR_2017_paper.pdf)

**iCaRL** 是最早提出的重演数据法，也是非常 naïve 的想法，解决 CIL 场景。三要素：

- 获取重演数据：近邻法。对于任务 $$t-1$$，（在特征空间上）选取离真实数据中心 $$\mu=\sum_{i=1}^{\mathcal{D}_{train}^{(t-1)}} \varphi(\mathbf{x})$$ 最近的若干个点（不需要存储标签）构成任务 $$t-1$$ 的重演数据 $$\mathcal{M}^{(t)}_{t-1}$$。注意特征空间时刻都在随训练更新，不是固定的。
- 空间管理：总记忆容量固定，大小平均分配给任务。设记忆容量为 $$K$$ 条数据：$$t=1$$ 时全部分配给任务 1，获取 $$\mathcal{M}^{(1)}_1$$ 时上述“若干个”为 K 个；$$t=2$$ 时分配给任务 1,2 各 $$K/2$$ 条，获取 $$\mathcal{M}^{(2)}_2$$ 时为 K/2 个， $$\mathcal{M}^{(2)}_1$$ 要从 $$\mathcal{M}^{(1)}_1$$ 中舍弃 $$K/2$$ 条（按照获取时离中心的近邻顺序，舍弃较远的）；……以此类推。
- 使用重演数据：重演数据也当作普通数据构造 random batch 训练。区别在于，重演数据的标签不是存储下来的真实标签，而是输入到训练完任务 $$t-1$$ 的模型输出的标签。 

调节防遗忘程度的超参数：记忆容量 K。

### GEM

论文链接：[Gradient Episodic Memory for Continual Learning](https://proceedings.neurips.cc/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf)



## 正则化法

**正则化法**是对损失函数下手，对任务 $$t$$ 的分类损失函数加**防遗忘正则项**，引导训练过程考虑防遗忘。正则项不能直接是旧任务的损失函数（因为无法获取旧任务的数据），而是某种代理损失（proxy loss）。引导的方向不同，就导致了不同的正则化法。

$$\min_{\theta} L^{(t)}(\theta) = L_{FINETUNE}^{(t)}(\theta) + \lambda L_{REVIEW}^{(t)}(\theta)$$

其中 $$\theta$$ 指代持续学习的所有参数（可能随任务越来越多，也可能固定）；$$L_{FINETUNE}^{(t)}(\theta) = \sum_{(\mathbf{x},y)\in \mathcal{D}_{train}^{(t)}} L(f(\mathbf{x};\theta),y)$$ 即任务 $$t$$ 正常的分类损失；$$L^{(t)}_{REVIEW}(\theta)$$ 是需要设计的防遗忘正则项；调节防遗忘程度的超参数：$$\lambda$$。

纯的正则化法禁止正则项中使用到重演数据，正则项只能从模型本身出发构造，对模型参数施加限制。



### LwF

论文链接：[Learning without forgetting](https://arxiv.org/abs/1606.09282)

**LwF**（Learning without Forgetting）是一个非常简单的防遗忘机制：在任务 t 训练开始前，先让任务 t 的数据 $$\mathcal{D}_{train}^{(t)}$$ 过一遍旧模型，得到旧模型分类的结果；在正式训练时，引导分类结果与旧模型分类结果靠近，模仿旧模型，达成防遗忘的作用。即加正则项：

$$L_{REVIEW}^{(t)}(\theta) = \sum_{\mathbf{x}\in \mathcal{D}_{train}^{(t)}} L(f(\mathbf{x};\theta),f(\mathbf{x};\theta^{(t-1)}))$$

注意，在这个过程中没有用过重演的旧数据，用的是任务 $$t$$ 训练之初自然继承下来的旧模型。整个算法不需要任何记忆来记住某些信息。

这种简单的方法缺陷是致命的：旧模型的信息全部浓缩到了分类结果这个小小的标签中，信息量太少——因为达成一个分类结果的方式有很多，这样最终可能使得模型与旧模型只有 “形似” 而没有 “神似”。另外一个角度，模型会对同一个数据参考两个标签（旧模型的分类标签、真实标签），若二者不同，这种冲突不太合理；若相同，就没有引入正则项的必要了。


### EWC

论文链接：[Overcoming catastrophic forgetting in neural networks](https://www.pnas.org/doi/10.1073/pnas.1611835114)

**EWC**（Elastic Weight Consolidation）是正则化法中第一个取得重大影响的算法。

$$L_{REVIEW}^{(t)}(\theta) = $$

记忆如何积累：


## 梯度操控法

正则化法通过修改损失函数影响反向传播，间接地改变了参数更新过程。我们也**直接规定、操控训练的更新过程**。

最常用的做法是直接操纵梯度，修改梯度的计算、梯度下降公式等，这里我称为 “**梯度操控法**”。

### 正交梯度下降（OGD）

论文链接：[Orthogonal Gradient Descent for Continual Learning](https://arxiv.org/abs/1910.07104) (Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics, 2020)

这是一种直接修正梯度的方法。在训练新任务时，让参数向着**垂直于旧任务更新方向**更新。即对任务 $$t$$，旧任务在更新时的梯度方向 会张成子空间 $$V_{t-1}$$，让新任务更新时的梯度方向限制在该空间的正交补空间 $$V_{t-1}^\perp$$ 中进行（不用担心子空间占满了参数空间导致没有正交补，因为参数空间往往是很大的）。

当然这个梯度方向不是任意 $$V_{t-1}^\perp$$ 上的方向，首先要契合训练新任务，所以最终是将新任务原始梯度（分类损失正常反向传播计算的梯度）$$g$$ **投影**到 $$V_{t-1}^\perp$$，使用这个**修正**后的梯度 $$\tilde{g}$$ 作新任务的梯度下降参数更新：

$$\theta \leftarrow \theta - \eta \tilde{g}$$

这个子空间是属于旧任务的，所有必须在记忆中记住。下面讨论如何表示旧任务梯度子空间，即需要记住什么内容。表示一个子空间，通常是用空间中的向量。子空间由**旧任务梯度**张成的（以任务 $$t-1$$ 为例）：$$\{\nabla_\theta L(f(\mathbf{x};\theta),y)\}_{(\mathbf{x},y)\in \mathcal{D}_{train}^{(t-1)}}$$（注意 $$\theta$$ 一直在变化，每个求导点 $$\theta$$ 都不一样；目标函数由于 $$\mathbf{x}$$ 不同也不同）。

我们不能直接记住这些 $$\nabla_\theta L(f(\mathbf{x};\theta),y)$$，而应转换为正交基 $$S$$（用 Gram-Schmidt 公式，参考线性代数），原因有二：

- 正交基是空间的代表，数量较少，可减少记忆量；
- 算正交投影的公式（参考线性代数）只能用正交基计算：

$$ \tilde{g} = g - \sum_{v\in \mathcal{M}} proj_v(g)$$


记忆如何积累：记忆 $$\mathcal{M}$$ 里记录了旧任务子空间的正交基，随任务数增加是不断扩充的，因为 Gram-Schmidt 公式是迭代的。

还需要考虑以下问题：

- 为减少计算量和存储量，可以选用$$\{\nabla_\theta L(f(\mathbf{x};\theta),y)\}_{(\mathbf{x},y)\in \mathcal{D}_{train^{(t-1)}}$$的一部分而不是全部，例如 $$\mathbf{x}$$ 选 $$\mathcal{D}_{train^{(t-1)$$ 的一部分；对 $$C$$ 分类问题，$$ L(f(\mathbf{x};\theta),y)=[\nabla f_1(\mathbf{x};\theta), \cdots,\nabla f_C(\mathbf{x}_C,\theta)] [a_1-y_1, \cdots, a_C- y_C]^T$$，由于 $$y$$ 总是一个 one-hot 向量（即 $$y_1,\cdots,y_C$$ 只有一个 1，其他全是 0），可以只要 $$y_c = 1$$ （ground truth label）的，即 $$L(f(\mathbf{x};\theta),y) = \nabla f_c(\mathbf{x}; \theta)(a_c - 1)$$，原论文中称为 OGD-GTL；
- 可以用固定的旧任务训练好的 $$\theta^{(t-1)}_\star$$ 来代替旧任务梯度中一直变化 $$\theta$$，这样就可以在旧任务训练结束后再统一计算记忆的梯度，而不用边训练边算。

调节防遗忘程度的超参数算是选用的旧梯度数量。



### 递归最小二乘法（RLS）

论文链接：[Continual Learning of Context-dependent Processing in Neural Networks](https://www.nature.com/articles/s42256-019-0080-x) (Nature Machine Intelligence 2019)

这也是一种修正梯度的方法，它是将变换矩阵 $$P^{(t-1)}$$ 作用在梯度上：

$$ \theta \leftarrow \theta - \eta P^{(t-1)} g$$

这个变换矩阵记录了旧任务的某种知识，而且能够随任务 $$t$$ 迭代地积累知识。

假设任务采用线性回归模型 $$f(\mathbf{x}) = \mathbf{w}^T\mathbf{x}$$，由回归分析知识可知，在平方损失下，使用最小二乘法可得到参数的最优解公式：

$$\mathbf{w}^{\star} = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$$

这个结果可以看作汇聚了该任务的所有知识。本方法就利用此式构建变换矩阵。首先不能直接拿来当作 $$P^{(t-1)}$$，它是参数，乘在参数梯度 $$g$$（量纲与参数一致）上是没有意义的。应当取 $$(\mathbf{X}^T\mathbf{X})^{-1}$$，它是一个方形矩阵，维度等于输入维数，也等于参数维数，所以能作用在参数梯度 $$g$$ 上。（它在统计学上应该有某种意义，但我不太清楚，欢迎大佬指点）

众所周知，直接计算这个矩阵是非常麻烦的，主要是因为 $$\mathbf{X}$$ 包含样本数太多。有近似算法——**递归最小二乘（RLS）算法**可以让 $$\mathbf{X}$$ 中的数据一个一个地来，迭代地计算。设 $$\mathbf{X} = [\mathbf{x}_1,\cdots, \mathbf{x}_n]$$，前 $$i$$ 个记为 $$\mathbf{X}_i$$，$$(\mathbf{X}_i^T\mathbf{X}_i)^{-1}$$ 的近似 $$P_i$$ 迭代计算公式：

$$ k_i = \frac{P_{i-1}\mathbf{x}_i}{\alpha + \mathbf{x}_i^T P_{i-1} \mathbf{x}_i}$$

$$P_i = P_{i-1} - k_i  \mathbf{x}_i^T P(i-1)$$

$$P_0$$ 初始化为单位矩阵 $$I$$。（该算法的原理是线性代数中子矩阵、矩阵分块的计算，感兴趣可以自己推导试试，不再详述。）


该方法的一大优点是所需记忆固定的，即一个固定大小的矩阵 $$P$$，不会随任务量增长。

记忆如何积累：面对持续学习，这种迭代的计算方式起到的作用不仅是简化了计算，也让任务之间可以继承，即在计算完毕任务 $$t-1$$ 的 $$(\mathbf{X}^T\mathbf{X})^{-1}$$ 后，可以以此为下一个任务迭代的初值，继续积累下去。也就是说，$$P^{(t-1)}$$ 不是只记录了任务 $$t-1$$，而是记录了所有旧任务 $$1,\cdots, t-1$$。



还需要考虑以下问题：

- 模型不一定是线性模型：对于深度为 $$L$$ 的网络，可以看成 $$L$$ 个线性模型，每层 $$l$$ 都有独立的 $$P$$ 作用在该层的梯度上（**layerwise**），这些 $$P$$ 的更新也是独立的。注意迭代公式里只用到了数据的输入（不用标签），所以 $$\mathbf{X}$$ 用每层的输入即可；
- 数据不是一个一个来的，而是一个 batch 来的：可以简单取 batch 的平均，当作一个数据，当然也有其他更好的方法；
- 可以在 $$P^{(0)}$$ 初始化时引入调节防遗忘程度的超参数 $$\lambda$$：$$P_0 = \lambda I$$。


### GPM

论文链接：[Gradient Projection Memory for Continual Learning](https://openreview.net/forum?id=3AOj0RCNC2)(ICLR 2021)


与 OGD 类似，区别在构造旧任务梯度的方式不一样。它不是就地取材从旧任务实际使用的梯度出发构造子空间，而是从数据下手，提取旧任务数据的信息来构造。也设任务采用线性回归模型，旧任务数据为 $$\mathbf{X} \in \mathbb{R}^{N\times p}$$。提取数据矩阵信息的一大工具是**奇异值分解（SVD）**：

$$\mathbf{X}_{N\times p} = \mathbf{U}_{N\times N}\mathbf{\Sigma}_{N\times p}\mathbf{V}^T_{p\times p}$$

我们要的是与梯度量纲一致的量，观察可以发现是右奇异向量 $$\mathbf{V}^T = [\mathbf{v}_1,\cdots, \mathbf{v}_p]$$，它的维度与参数梯度一致，可以在参数空间中张成子空间。与 OGD 一样，在训练新任务时，让参数在该子空间的正交补中更新即可。


记忆如何积累：由于奇异向量本身是正交的，直接当作正交基存到记忆里即可。


还需要考虑以下问题：

- 对于深度为 $$L$$ 的网络，也需要 layerwise，每层 $$\mathbf{X}$$ 用该层的输入即可；
- 数据不是整个来的，而是分 batch 来的：那计算奇异值分解也分 batch 来即可；
- 为减少计算量和存储量，可以取奇异向量的一部分。$$\mathbf{V}^T$$ 天生就按重要程度（奇异值）排序了，按照一定的准则取前 $$k$$ 个即可。个数 $$k$$ 算是调节防遗忘程度超参数。



## 网络结构法

网络结构法从网络结构下手，将网络划分成各部分并按某种机制分配给各任务，构成某种子网络，因此又称**参数隔离法**（Parameter Isolation）。它显式地体现了模型容量分配问题，将模型容量这一概念显化到模型各部分参数了。这也是一种直接规定、操控更新过程的方法。

注意，
- 对网络划分的意思并不是为每个任务使用单独的模型，各模型之间完全独立的独立式学习。各部分之间一定有着某种联系；
- 划分不是数学概念上的划分（split），可以重叠也可以不重叠；
- 一般是对网络的特征提取器部分进行划分，输出头部分总是共用的。

这类持续学习算法只适用于 TIL 场景。在测试时，需要根据测试数据的任务 ID $$t$$ 的信息，选取对应的网络划分部分作预测。

对于这类方法，如果各部分能做到不相互重叠，那真的是可以显式地达到各任务互不干扰，它防遗忘方法中的最强者。某些论文里甚至声称能做到零遗忘（zero-forgetting）。


### 模型扩张法：Progressive NN

论文链接：[Progressive Neural Networks](https://www.deepmind.com/publications/progressive-neural-networks), arXiv 2016

该方法不固定模型大小，即模型大小可以随任务线性增加。它是每来一个新任务，就将网络（指特征提取器部分的网络，不算输出头）**扩充一列**（一列包括每层若干个神经元），如图：

![](Progressive_NN.png)


该方法主要的问题是模型大小线性增长，它以此为代价来解决模型容量问题，其实是很像独立式学习了。但它与独立式学习的区别是多了图中斜向右上箭头代表的权重，使新的一列网络与旧网络建立了联系——旧知识（旧网络的输出）可以通过该权重迁移过来。训练时，固定旧网络部分的参数（虚线）不动，训练新加入的参数（实线）。图中 $$a$$ 为论文额外加入的非线性函数，可以更加强调斜向右上箭头权重的特殊性。测试时，使用测试数据任务 ID $$t$$ 对应的部分网络作预测。


### Mask 机制

如果要固定模型大小，参数隔离方法就是对固定数量的网络参数按任务进行划分，可以归结为引入加在模型上的遮罩（**mask**），它用来规定对模型某部分的选择，遮盖其他部分。Mask 用于**不同的任务**上，每个任务都有自己的 mask。持续学习中有大量的工作是基于 mask 机制的，例如 [PackNet]()、[HAT]() 等等，我不打算对着几篇代表性论文讲解，而是根据 mask 的实现方式、构造方式与在训练过程中的指导作用，对这些工作分分类。我将在[这篇笔记]()中详细讨论一些工作应用 mask 机制的细节。


#### Mask 的实现方式

Mask 是遮在模型上的指示性变量，有两种形式：

- 加在参数上的（weight mask）：直接在每个参数上规定，对第 l 层与第 l+1 层之间的参数，$$\mathbf{M}^l$$ 为矩阵；
- 加在神经元上的（feature mask）：在每个神经元上规定，对第 l 层神经元，$$\mathbf{m}^l$$ 为向量，间接地影响与神经元相连接的参数。这种 mask 数量会大大减少。

各任务的 mask 需要存储在记忆 $$\mathbf{M}$$ 中。对于 weight mask，每个任务占用空间是参数量级的；对于 feature mask，每个任务占用空间与神经元数目一致（相比 weight mask 成平方级地减少）。虽然记忆是随任务数线性增长的，但不用担心，它们比重演数据小多了。另外再次强调，mask 覆盖的参数不包括输出头的参数，因为不能直接影响输出头的输出结果（例如可能会让输出值变为 0）。

Mask 变量的取值，可以是：

- 二元的 0,1：称为 hard mask，只有 2 种状态：被遮住与不被遮住； 
- 实数值：称为 soft mask。

以下不特别说明，都是指二元 mask。

#### Mask 如何规定训练过程与测试过程

Mask 如何影响新任务的训练过程（包括前向传播和反向传播）与最终的测试过程（前向传播），把这两件事定义清楚，mask 的作用就定义清楚了。以下是通常的选择：

- Mask 可以选择是否作用新任务**训练过程的前向传播**，若选择作用：
  - 对于 weight mask：将 $$\mathbf{W}_l$$ 乘以 mask，即 $$\mathbf{W}_l \odot \mathbf{M}_l$$；
  - 对于 feature mask：将神经元输出（激活前后都一样）$$\mathbf{o}_l$$ 乘以 mask，即 $$\mathbf{o}_l \odot \mathbf{m}_l$$；
- Mask 可以选择是否作用新任务**训练过程的反向传播**，若选择作用，指的是修改反向传播公式，在梯度流中阻塞：
  - 对于 weight mask：将计算的 $$\frac{\partial L}{\partial \mathbf{W}_l}$$ 乘以 mask，即被遮住的参数不更新；
  - 对于 feature mask：将神经元输出（指激活后）$$\mathbf{o}_{l-1}$$ 乘以 mask，根据反向传播公式 
$$\frac{\partial L}{\partial w_{i j}^{l}}=\delta_{j}^{l} o_{i}^{l-1}=g^{\prime}\left(a_{j}^{l}\right) o_{i}^{l-1} \sum_{k=1}^{r^{l+1}} w_{j k}^{l+1} \delta_{k}^{l+1}$$，被遮住的神经元发射的参数都不更新；
- 对于**测试过程的前向传播**，mask 通常都是作用的（一般没有例外），将 $$\mathbf{W}_l$$ 乘以 mask，即 $$\mathbf{W}_l \odot \mathbf{M}_l$$，即对于任务 ID 为 $$t$$ 的测试输入，使用任务 $$t$$ 的 mask 选择的子网络作预测。

> 如果 mask 对训练过程的前向传播和反向传播有独立的作用，那么一个二元 mask 是不够表示的，需要两个二元 mask，表示四个状态：前向传播、。但大多数论文只会涉及一个，所以只用一个二元 mask。

#### 如何构造 mask

训练任务 $$t$$ 时，不仅要将旧任务的 mask 作用到（如果选择作用的话）网络上，还要构造出新任务 $$t$$ 的 mask。构造 mask 可使用**固定算法**（例如按照某种指标选择一定比例的重要程度较高的参数），也可将 mask 看成**可学习的参数**融进训练过程学习，即将 mask 代入到模型中当作参数随模型参数正常训练，也就是对如下计算图（以一层为例）作反向传播：

- 对于 weight mask: $$f(\mathbf{o}_l;\mathbf{W}_l,\mathbf{M}_l) = (\mathbf{W}_l \odot \mathbf{M}_l)\cdot \mathbf{o}_l$$
- 对于 feature mask: $$f(\mathbf{o}_l;\mathbf{W}_l,\mathbf{m}_l) = \mathbf{W}_l \cdot (\mathbf{o}_l \odot \mathbf{m}_l)$$


注意：
- 如果 mask 融进训练过程来学习，每个任务的训练过程不仅要考虑训练新 mask，还要将旧任务 mask 的作用考虑进来，最终训练的反向传播过程形式更麻烦（并不只是上式）；
- 根据不同的构造方式，各任务的 mask 可以设计为允许重叠，也可以不重叠。如上所述，mask 不重叠（参数硬隔离）可以实现零遗忘，但也要面临模型容量问题；
- 二元 mask 是无法直接融进训练过程的（因为不连续不可导），需要采取一些训练技巧处理不可导。

对于融进训练过程与模型参数一起训练的 mask，重要的事情是使其**稀疏化**——因为如果不加任何限制，每个任务总会倾向于学到一个全 1 的 mask（因为为了在当前任务上效果好，学习算法会试图占据一切可能利用的模型容量资源的）。这是模型容量分配问题在 mask 机制上实际的表现，每篇使用可学习 mask 的论文都应设计恰当的稀疏化机制。

> Mask 机制实际上就是一种**注意力机制**，[注意力机制](https://en.wikipedia.org/wiki/Attention_(machine_learning))对输入（或者中间层）的每个元素考虑赋予不同的注意力得分，让模型更加关注输入中得分高的部分，得分低的则看作无用信息被抑制。重要的是，这个得分可以通过梯度下降自动地学习出来，而不用手动规定。
> 记住，注意力机制是一种思想的统称，可以与各种模型混合，不单单用在 RNN 中的 Encoder-Decoder 结构。这里的 mask 机制可能是比较简单的一种用法。
{: .prompt-info }


## 优缺点讨论

- 重演数据法最大的缺点是重演数据量不够；
- 直接操控更新过程的方法（包括梯度操控法、参数隔离法）有更好的可解释性，但也会带来僵硬的问题。（二者有冲突）



# 六、前沿方向

这里列举一些持续学习的其他前沿方向，不作详细介绍。

- 小样本持续学习：
- 持续异常检测；
- 带粗细粒度的持续学习；
- 持续学习 + Transformer（ViT）：DyTox。
- ...


# 参考资料

以下列举一些持续学习相关的参考资料与学习资源。

1. 课程 Continual Learning（比萨大学）
2. 持续学习社区 Continual AI



<br>


[^footnote]: [Gradient Episodic Memory for Continual Learning]()