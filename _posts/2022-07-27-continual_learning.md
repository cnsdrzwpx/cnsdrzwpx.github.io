---
title: 持续学习基础知识
date: 2022-07-27
categories: [科研]
tags: [学习笔记, 持续学习]
img_path: /assets/img/
math: true
---

目前我的研究方向是持续学习。本文汇总了持续学习的基础知识体系，可以看作一篇综述吧，希望这篇笔记能带你进入我的研究领域。本文涉及的方法都是我觉得有代表性的，只介绍思想，不会非常详细地讲细节。


----------
**目录**

* TOC
{:toc}





---------


# 一、相关概念

相关概念已经在[《终身机器学习》读书笔记](https://pengxiang-wang.github.io/tags/终身机器学习/)的第一、二章中详细介绍过，这里只是简单概括一下。

**持续学习**（Continual Learning, CL）是多个任务的机器学习的一种学习范式。持续学习又称**终身学习**（Lifelong Learning），终身学习于很早提出，进入深度学习时代后研究者逐渐改叫持续学习；还有人把持续学习叫做**增量学习**（Incremental Learning）。

持续学习从大面来说是：多个任务数据**依次**交付给持续学习算法学习（每次**只有当前任务数据**），使得最终学到的模型能够胜任**所有**任务。“依次”和“所有”是持续学习的核心，缺一者就与只有单任务的机器学习无差别了：

- 缺“依次”：每次算法能用之前所有任务数据学习，则最后一个任务时就能学习所有任务，就不“持续”了。在实际场景中，旧数据主要是由于存储限制或隐私保护等原因而不可用的。
- 缺“所有”：若不要求在所有任务上表现都好（例：只要求当前任务），则就是当前任务的单任务的机器学习。

另外，持续学习也不允许每个任务都学习一个独立的模型，这样相当于多个单任务的机器学习。这种称为**独立式学习**（isloated learning）。（参考下面讲解的模型容量分配问题）


持续学习与其他学习范式的主要区别：

- 在线学习：数据都是同一个任务来的，一定是独立同分布的；持续学习划出多个任务，数据不一定（不是一定不，但通常不是）是独立同分布的。且它的研究重点是“在线”与“离线”的区别；
- 迁移学习：重点关注的是“迁移”——如何利用旧任务的学习成果帮助新任务的学习；
- 多任务学习：数据是一次给完的，强调同时学习多个任务；
- 元学习：“学会学习”的角度更高，不只关注如何解决当前所有任务，还试图提取学习经验，泛化到新的任务。


# 二、持续学习关心的问题

以下先从较抽象的角度介绍持续学习关心的问题，之后再给出形式化定义和具体的例子。

## 灾难性遗忘

上面已经说过，在持续学习中，使用新任务数据训练模型使其在该新任务上效果好，是很容易做到的，只需应用成熟的单任务机器学习的算法即可；相反，很多时候学习新任务后，模型在旧任务上的效果会变差，这就是**灾难性遗忘**（Catastrophic Forgetting, CF），也是持续学习关心的核心问题。解决这一问题的方法是引入**防遗忘机制**，使模型保持旧任务上的效果。

从哲学上来说，假设模型的学习能力是固定的，模型在新任务上效果好，则在旧任务上效果会变差；反之，模型保持了旧任务上的效果，则在新任务上效果就不会好。前者是模型学习新知识的能力，称为**可塑性**（plasticity）；后者是旧知识的记忆能力，称为**稳定性**（stability）。可塑性与稳定性是内在相互矛盾的，术语叫**可塑性-稳定性困境**（Stability-Plasticity Dilemma），这是机器学习的一个天然的哲学约束，类似于 “没有免费午餐定理”。持续学习的目标是在所有任务上表现都好，即同时追求可塑性和稳定性；但这个困境说明了实现这一目标没有捷径，持续学习场景不是伪命题，并不是无脑加防遗忘机制、加强防遗忘的力度（例如调大防遗忘正则项超参数）就可以了，必须切实地提高模型的真本领。

## 后向迁移与前向迁移

除了灾难性遗忘作为核心问题，持续学习还关心算法是否具备：

- **后向迁移**（backward transfer）能力：学习后面的任务时，能否帮助到前面的任务；
- **前向迁移**（forward transfer）能力：学习前面的任务时，能否帮助到后面的任务。

试问，后向迁移与灾难性遗忘说的是一回事吗？因为学习后面的任务时，通常不会对前面的任务有正向的帮助，反而是负面的帮助——遗忘。算法的防遗忘机制加得狠，是否等价于提高后向迁移能力？

> 请注意用词：在后向迁移和前向迁移术语中，“前”是指旧任务方向，“后”是指新任务方向。而我平时习惯说“后”是指新任务。
{: .prompt-tip }


## 模型容量分配问题

持续学习的一大特点是学习任务的类型和数量没有预定义。在学习每个任务的期间，永远不知道未来有多少个任务、它们是什么样子的。之前所说的：每个任务学习一个独立的模型，其模型大小随任务量线性地增加。这样，模型尝试学习、记下每个任务所有的知识，对应的算法也是与普通机器学习没有差别，是持续学习不允许的。

我们不希望模型大小无序地膨胀，而是**固定模型容量**（capacity），让算法在固定容量的模型下完成持续学习（偶尔也会允许少量的膨胀）。这里所说的模型容量更多的是一个抽象概念，指模型的**表示能力**。当然，对于深度学习，模型的表示能力也与参数量成正相关。

很显然，固定容量的模型，随着任务越来越多，模型也不能容纳所有的知识，知识必须有所舍弃，各任务上的效果也会打折扣，灾难性遗忘也就越严重。这个问题是不可能解决的，但是可以缓解这个问题。一个好的持续学习算法能让模型尽量记住任务重要的知识，在需要舍弃知识时舍弃不重要的，减缓遗忘的速度。

在持续学习中，每个任务会占据模型的一部分容量，任务之间也会共用部分容量（根据任务相似性）。但是如果不加限制，每个任务学习后就会很自然地占满所有模型容量，这样不仅容易导致任务的过拟合（因为通常适合持续学习多个任务的模型要比适合某个任务的模型要大很多），也让后面的（与该任务不太相似的）任务无处占据模型容量，导致后面的任务效果都变差。所以，需要在算法中加入一些**稀疏化**（sparsity）机制来解决模型容量不够的问题。



# 三、任务：分类问题

持续学习也分监督学习、无监督学习等，也有判别模型、生成模型。目前大家研究最多的是监督学习，且更多地关心分类问题。本文只讨论**分类问题**。

在分类问题中，研究者公认的持续学习场景有以下几个（主要是前两个）：

- **类别增量学习**（Class Incremental Learning, CIL）：每个任务包含若干不重复的类别；
- **任务增量学习**（Task Incremental Learning, TIL）：每个任务对数据的类别等信息不作要求，但数据中包含“属于哪个任务”这个信息；
- **领域增量学习**（Domain Incremental Learning, DIL）：每个任务包含的类别相同，但背后的领域不同。

与类别增量学习相对的是**每个任务类别相同的场景**，DIL 一定是，TIL 有可能是，也可能不是。

## 形式化定义

下面给出几个场景的形式化定义。设有任务 $$t=1,2,\cdots$$，每个任务的数据集为 $$\mathcal{D}^{(t)}$$，其中 $$\mathcal{D}^{(t)} =\{(\mathbf{x}_i,y_i)\}_{i=1}^{N_t} \in (\mathcal{X}^{(t)},\mathcal{Y}^{(t)})$$。算法在每个时刻 $$t$$ 利用 $$\mathcal{D}^{(t)}$$ 将 $$f^{(t-1)}$$ 更新 $$f^{(t)}$$，希望 $$f^{(t)}$$ 能完成目前涉及到的**所有**分类任务，即输入 $$\mathbf{x} \in \mathcal{X}^{(1)}\cup\cdots\cup\mathcal{X}^{(t)}$$，输出所有涉及到的类别 $$\hat{y} \in \mathcal{Y}^{(1)}\cup \cdots \cup \mathcal{Y}^{(t)}$$。

- 类别增量学习：$$\mathcal{Y}^{(t)}$$ 之间互不相交。可以记 $$\mathcal{Y}_1 = \{C_1,\cdots,C_{k_1}\}, \mathcal{Y}_2 = \{C_{k_1 + 1}, \cdots, C_{k_2}\}, \cdots$$；
- 任务增量学习：知道了输入的任务 ID $$t_{\mathbf{x}}$$（主要是对测试数据说的，训练数据天生是知道任务 ID 的），即目标简化为输入 $$\mathbf{x}\in \mathcal{X}^{(t_\mathbf{x})}$$，输出 $$\hat{y} \in \mathcal{Y}^{(t_\mathbf{x})}$$；
- 领域增量学习：$$\mathcal{Y}^{(1)}=\cdots=\mathcal{Y}^{(t)}$$，但强调 $$\mathcal{X}^{(1)},\cdots,\mathcal{X}^{(t)}$$ 的不同。


注意点：

- 之前说过，持续学习过程中永远不知道之后有多少个任务。但在实际实验中，持续学习数据集是固定的，总任务数是固定的 $$T$$ 个（也为了计算指标方便），以上 $$t=1,\cdots,T$$，但是持续学习算法在任何时刻都禁止使用 $$T$$ 这个信息。
- 一定要强调上面加粗的“所有”二字。对于 CIL，很多人的误区是以为任务 $$t$$ 只在 $$\mathcal{Y}^{(t)}$$ 中分类，而事实是在 $$\mathcal{Y}^{(1)}\cup\cdots\cup\mathcal{Y}^{(t)}$$ 中分类（就是下图多头模型有无灰色箭头的区别）。所以 CIL 场景是比 TIL 场景要困难的。
- CIL 第一个任务至少要包含 2 个类，之后的任务没有限制。




## Baseline：多头模型

对于非每个任务类别相同的场景，类是越来越多的。而且系统不知道未来有哪些类，无法在一开始就把所有类包括进来，构造出输出头固定的分类器；只能每当出现新类，临时加入该类的输出头。

所谓的**多头模型**是指模型的主要部分（特征提取器 $$\varphi$$）由各任务共用，但输出端不固定，随着新类别的引入，随时会引入新的输出头。因此模型参数会包含共享参数和类别独有的参数两部分，后者的比例应该是非常小的，所以即使它的数量线性增长问题也不大，是允许的。

下图是以多头模型为基础的持续学习最简单的算法，算是所有持续学习算法的 **baseline**。它用最简单的学习方式，并不是每个新任务都从头开始训练，而是用上一个任务的训练结果作为下一个任务的初始化。具体来说，上图模型参数分为网络共享权重 $$\mathbf{w}_0$$ 和每个类别独有的权重 $$\mathbf{w}_1,\mathbf{w}_2,\cdots$$。每遇到新类别都会引入新的 $$\mathbf{w}_i$$，都作（随机）初始化。$$\mathbf{w}_0$$ 在算法的最开始（随机）初始化，且在每个时刻 $$\mathbf{w}^{(t)}_0$$ 都会用 $$\mathbf{w}^{(t-1)}_0$$ 初始化。

![](continual_learning_baseline.pdf)


这个算法在持续学习论文里习惯叫做**微调（fine-tuning）**，因为直接拿上一个任务初始化的方式有微调上一个任务的意思。直观上看这种方式直接覆盖了上个任务的成果，重新把所有的模型容量让新任务占满，很容易灾难性遗忘。（尽管有类别独有的参数能防止遗忘，但它们占的比例太小，起的作用是远远不够的。）因此这个算法可以认为**没有任何防遗忘机制**，是一个白板算法，大家研究的持续学习算法都是在其基础上引入自己的防遗忘机制的。

下图描述了持续学习算法与微调白板模型参数更新路径的对比，取自 [OWM 论文](https://www.nature.com/articles/s42256-019-0080-x)
，图中展示的是参数空间，上面的点是参数，两个圈分别代表任务 1、任务 2 的分类损失函数（等高线）。训练任务 1 后参数位于右上方点，在训练任务 2 时，采用微调白板算法参数会直接更新到左上方点，而采用有防遗忘机制的持续学习算法会更新到下方点。

![](continual_learning_update_routes.png)

## 数据集

持续学习分类问题的常用**数据集**是通过机器学习的标准数据集划分、构造出来的。标准数据集例如常用的 MNIST、CIFAR-10、CIFAR-100、ImageNet 等。划分方式主要有两种：

- **分割**（split）：按照类别划分数据集为任务，用于 CIL；
- **置换**（permute）：对原数据集所有数据做一次相同的变换，得到一个任务，可以用于 TIL、DIL 等每个任务类别相同的场景。

以 MNIST 为例，可以构造 Split MNIST、Permuted MNIST 两种数据集。Split MNIST 按类别划分成（以 5 个任务为例）0v1, 2v3, 4v5, 6v7, 8v9；Permuted MNIST 每构造一个任务时就按相同方式打乱各图片像素的顺序。


# 四、持续学习的指标

持续学习的主要目标是让模型在所有任务上表现都好，因此持续学习的**指标**首先是**各任务平均指标**，其次关注其他关心问题上的表现，如**后向迁移能力**、**前向迁移能力**。这些指标都是持续学习过程训练的各模型在各任务上的单个指标计算出来的[^footnote]：

记 $$R_{\tau,t}$$ 表示时刻 $$\tau$$ 训出的模型在第 $$t$$ 个任务上的指标（例，对分类问题是准确率），注意每个任务都有自己的测试集 $$\mathcal{D}^{(t)}_{test}$$，$$R_{\tau,t}$$ 是用 $$\mathcal{D}^{(t)}_{test}$$ 做测试的。有以下指标：

- 各任务平均指标：$$ ACC = \frac1T \sum_{t=1}^T R_{T,t}$$，即**最后**得到的模型在所有任务上的平均表现；
- 平均后向迁移：$$ BWT = \frac1{T-1} \sum_{t=1}^{T-1} (R_{T,t}- R_{t,t}) $$，即任务刚开始学（$$t$$ 时刻）与学到最后（$$T$$ 时刻）效果之差，对所有非最后一个任务取平均。这个指标只衡量了最后一个任务的后向迁移情况。
- 平均前向迁移：$$ FWT = \frac1{T-1} \sum_{t=1}^{T-1} (R_{t-1,t} - \bar{b}_t)$$。$$\bar{b}_t$$ 是随机初始化并用 $$\mathcal{D}_t$$ 训练的模型效果（多次实验取平均），有点 $$R_{0,t}$$ 的意思，但不太一样。指标表示到在任务刚开始学但还没有学（$$t-1$$ 时刻）期间累积的知识（比较的对象是不使用 $$\mathcal{D}_1,\cdots, \mathcal{D}_{t-1}$$ 前向迁移的知识、而只使用 $$\mathcal{D}_t$$ 自己知识的结果 $$\bar{b}_t$$），对所有非第一个任务取平均。

在实验中，有人会观察这些指标随 $$T$$ 的变化曲线。也就是说，测试过程通常是每训练完一个新任务就对已涉及的所有任务测试一遍。

> 第一个指标的定义方式是公认的，后两者可能还有待探索。举个例子，FWT 中的 $$R_{t-1,t}$$ 在 CIL 场景下是无法计算的，因为在 $$t-1$$ 时刻压根就没有 $$t$$ 时刻新出现的类别，FWT 需要另外定义。
{: .prompt-tips }

> 以上指标都是以任务为单位作算数平均计算出来的，通常要求任务划分得比较均衡（事实上多数数据集是这样的，例如 CIL 每个任务的类数量相等），否则最好根据任务规模/难易加权平均。还有的指标转而以类别为单位算平均。
{: .prompt-tips }

在训练过程中需要监视**学习曲线**，持续学习有多个任务，就有多个独立的学习曲线（下图对角线上的图）。但持续学习的目标不只是让当前任务学好，更关注是否会灾难性遗忘，所以还需要监视模型在旧任务上的表现，于是得到更多的学习曲线（下图对角线上方的图）。下图是一个例子（来自 [EWC]() 论文），我暂且称为 “三角图”：

![](CL_learning_curve.png)

> 注意这些图画的是模型于各训练阶段在**整个测试集**上的表现。为了画这张图，需要每个一段时间就做一次完整的测试，其实很耗时间，但这时间不是算在训练时间内的，无所谓。
{: .prompt-tip }

一个好的持续学习算法应当在任务切换后（并不是瞬间）在旧任务上效果不变差太快，例如图中对任务 A，在训练结束切换至 B 时，准确率曲线 EWC 几乎不下降，而不加防遗忘机制的 SGD 就会迅速下降，说明 EWC 防遗忘性能比较优秀。

# 五、防遗忘机制概论

目前学界普遍承认防遗忘机制可以分成三大类：重演数据法、正则化法、网络结构法。


每个持续学习算法都应有一个对象在随任务数增加不断迭代地积累知识，这个对象统称为**记忆**（memory），记为 $$\mathcal{M}$$。在重演数据法中记忆中存放的是数据，而在其他方法中通常也需要记忆来记忆梯度、mask 等非数据信息。在防遗忘机制中，通常有一个支配**防遗忘程度的超参数**可供人工调节，例如正则化法中的正则项系数，实验时一般会比较不同的防遗忘程度。每个方法我都会讲清楚记忆是如何迭代积累的（如果有记忆），以及调节防遗忘程度的超参数是什么。





## 重演数据法

防止遗忘最直接的方式是在记忆中旧任务的训练数据，称为**重演**（replay）数据。重演数据不能太多，当然不允许使用全部的旧任务数据（否则就不是持续学习了），通常要求限制固定的重演数据记忆容量，需要精炼旧任务数据的信息。


重演数据法的三个要素：

- 在旧任务结束、新任务到来之前，获取重演数据的算法；
- 重演数据空间如何管理，即记忆如何迭代积累；
- 如何在新任务上使用重演数据。

由此可以划分出各类**重演数据法**。例如获取重演数据可以直接从旧任务抽取代表元（exemplar），也可以用生成模型生成（后者称为伪（pseudo）重演数据法）；重演数据在新任务上使用，可以直接当作普通数据构造 random batch，也可以有其他方法。


### iCaRL

论文链接：[iCaRL: Incremental Classifier and Representation Learning](https://openaccess.thecvf.com/content_cvpr_2017/papers/Rebuffi_iCaRL_Incremental_Classifier_CVPR_2017_paper.pdf)

**iCaRL** 是最早提出的重演数据法，也是非常 naïve 的想法，解决 CIL 场景。三要素：

- 获取重演数据：近邻法。对于任务 $$t-1$$，（在特征空间上）选取离真实数据中心 $$\mu=\sum_{i=1}^{\mathcal{D}_{train}^{(t-1)}} \varphi(\mathbf{x})$$ 最近的若干个点（不需要存储标签）构成任务 $$t-1$$ 的重演数据 $$\mathcal{M}^{(t)}_{t-1}$$。注意特征空间时刻都在随训练更新，不是固定的。
- 空间管理：总记忆容量固定，大小平均分配给任务。设记忆容量为 $$K$$ 条数据：$$t=1$$ 时全部分配给任务 1，获取 $$\mathcal{M}^{(1)}_1$$ 时上述“若干个”为 K 个；$$t=2$$ 时分配给任务 1,2 各 $$K/2$$ 条，获取 $$\mathcal{M}^{(2)}_2$$ 时为 K/2 个， $$\mathcal{M}^{(2)}_1$$ 要从 $$\mathcal{M}^{(1)}_1$$ 中舍弃 $$K/2$$ 条（按照获取时离中心的近邻顺序，舍弃较远的）；……以此类推。
- 使用重演数据：重演数据也当作普通数据构造 random batch 训练。区别在于，重演数据的标签不是存储下来的真实标签，而是输入到训练完任务 $$t-1$$ 的模型输出的标签。 

调节防遗忘程度的超参数：记忆容量 K。

### GEM

论文链接：[Gradient Episodic Memory for Continual Learning](https://proceedings.neurips.cc/paper/2017/file/f87522788a2be2d171666752f97ddebb-Paper.pdf)



## 正则化法

**正则化法**是对损失函数下手，对任务 $$t$$ 的分类损失函数加**防遗忘正则项**，引导训练过程考虑防遗忘。正则项不能直接是旧任务的损失函数（因为无法获取旧任务的数据），而是某种代理损失（proxy loss）。引导的方向不同，就导致了不同的正则化法。

$$\min_{\theta} L^{(t)}(\theta) = L_{FINETUNE}^{(t)}(\theta) + \lambda L_{REVIEW}^{(t)}(\theta)$$

其中 $$\theta$$ 指代持续学习的所有参数（可能随任务越来越多，也可能固定）；$$L_{FINETUNE}^{(t)}(\theta) = \sum_{(\mathbf{x},y)\in \mathcal{D}_{train}^{(t)}} L(f(\mathbf{x};\theta),y)$$ 即任务 $$t$$ 正常的分类损失；$$L^{(t)}_{REVIEW}(\theta)$$ 是需要设计的防遗忘正则项；调节防遗忘程度的超参数：$$\lambda$$。

纯的正则化法禁止正则项中使用到重演数据，正则项只能从模型本身出发构造，对模型参数施加限制。



### LwF

论文链接：[Learning without forgetting](https://arxiv.org/abs/1606.09282)

**LwF**（Learning without Forgetting）是一个非常简单的防遗忘机制：在任务 t 训练开始前，先让任务 t 的数据 $$\mathcal{D}_{train}^{(t)}$$ 过一遍旧模型，得到旧模型分类的结果；在正式训练时，引导分类结果与旧模型分类结果靠近，模仿旧模型，达成防遗忘的作用。即加正则项：

$$L_{REVIEW}^{(t)}(\theta) = \sum_{\mathbf{x}\in \mathcal{D}_{train}^{(t)}} L(f(\mathbf{x};\theta),f(\mathbf{x};\theta^{(t-1)}))$$

注意，在这个过程中没有用过重演的旧数据，用的是任务 $$t$$ 训练之初自然继承下来的旧模型。整个算法不需要任何记忆来记住某些信息。

这种简单的方法缺陷是致命的：旧模型的信息全部浓缩到了分类结果这个小小的标签中，信息量太少——因为达成一个分类结果的方式有很多，这样最终可能使得模型与旧模型只有 “形似” 而没有 “神似”。另外一个角度，模型会对同一个数据参考两个标签（旧模型的分类标签、真实标签），若二者不同，这种冲突不太合理；若相同，就没有引入正则项的必要了。


### EWC

论文链接：[Overcoming catastrophic forgetting in neural networks](https://www.pnas.org/doi/10.1073/pnas.1611835114)

**EWC**（Elastic Weight Consolidation）是正则化法中第一个取得重大影响的算法。

$$L_{REVIEW}^{(t)}(\theta) = $$

记忆如何积累：


## 梯度操控法

正则化法通过修改损失函数影响反向传播，间接地改变了参数更新过程。我们也**直接规定、操控训练的更新过程**。

最常用的做法是直接操纵梯度，修改梯度的计算、梯度下降公式等，这里我称为 “**梯度操控法**”。

### 正交梯度下降（OGD）

论文链接：[Orthogonal Gradient Descent for Continual Learning](https://arxiv.org/abs/1910.07104) (Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics, 2020)

这是一种直接修正梯度的方法。在训练新任务时，让参数向着**垂直于旧任务更新方向**更新。即对任务 $$t$$，旧任务在更新时的梯度方向 会张成子空间 $$V_{t-1}$$，让新任务更新时的梯度方向限制在该空间的正交补空间 $$V_{t-1}^\perp$$ 中进行（不用担心子空间占满了参数空间导致没有正交补，因为参数空间往往是很大的）。

当然这个梯度方向不是任意 $$V_{t-1}^\perp$$ 上的方向，首先要契合训练新任务，所以最终是将新任务原始梯度（分类损失正常反向传播计算的梯度）$$g$$ **投影**到 $$V_{t-1}^\perp$$，使用这个**修正**后的梯度 $$\tilde{g}$$ 作新任务的梯度下降参数更新：

$$\theta \leftarrow \theta - \eta \tilde{g}$$

这个子空间是属于旧任务的，所有必须在记忆中记住。下面讨论如何表示旧任务梯度子空间，即需要记住什么内容。表示一个子空间，通常是用空间中的向量。子空间由**旧任务梯度**张成的（以任务 $$t-1$$ 为例）：$$\{\nabla_\theta L(f(\mathbf{x};\theta),y)\}_{(\mathbf{x},y)\in \mathcal{D}_{train}^{(t-1)}}$$（注意 $$\theta$$ 一直在变化，每个求导点 $$\theta$$ 都不一样；目标函数由于 $$\mathbf{x}$$ 不同也不同）。

我们不能直接记住这些 $$\nabla_\theta L(f(\mathbf{x};\theta),y)$$，而应转换为正交基 $$S$$（用 Gram-Schmidt 公式，参考线性代数），原因有二：

- 正交基是空间的代表，数量较少，可减少记忆量；
- 算正交投影的公式（参考线性代数）只能用正交基计算：

$$ \tilde{g} = g - \sum_{v\in \mathcal{M}} proj_v(g)$$


记忆如何积累：记忆 $$\mathcal{M}$$ 里记录了旧任务子空间的正交基，随任务数增加是不断扩充的，因为 Gram-Schmidt 公式是迭代的。

还需要考虑以下问题：

- 为减少计算量和存储量，可以选用$$\{\nabla_\theta L(f(\mathbf{x};\theta),y)\}_{(\mathbf{x},y)\in \mathcal{D}_{train^{(t-1)}}$$的一部分而不是全部，例如 $$\mathbf{x}$$ 选 $$\mathcal{D}_{train^{(t-1)$$ 的一部分；对 $$C$$ 分类问题，$$ L(f(\mathbf{x};\theta),y)=[\nabla f_1(\mathbf{x};\theta), \cdots,\nabla f_C(\mathbf{x}_C,\theta)] [a_1-y_1, \cdots, a_C- y_C]^T$$，由于 $$y$$ 总是一个 one-hot 向量（即 $$y_1,\cdots,y_C$$ 只有一个 1，其他全是 0），可以只要 $$y_c = 1$$ （ground truth label）的，即 $$L(f(\mathbf{x};\theta),y) = \nabla f_c(\mathbf{x}; \theta)(a_c - 1)$$，原论文中称为 OGD-GTL；
- 可以用固定的旧任务训练好的 $$\theta^{(t-1)}_\star$$ 来代替旧任务梯度中一直变化 $$\theta$$，这样就可以在旧任务训练结束后再统一计算记忆的梯度，而不用边训练边算。

调节防遗忘程度的超参数算是选用的旧梯度数量。



### 递归最小二乘法（RLS）

论文链接：[Continual Learning of Context-dependent Processing in Neural Networks](https://www.nature.com/articles/s42256-019-0080-x) (Nature Machine Intelligence 2019)

这也是一种修正梯度的方法，它是将变换矩阵 $$P^{(t-1)}$$ 作用在梯度上：

$$ \theta \leftarrow \theta - \eta P^{(t-1)} g$$

这个变换矩阵记录了旧任务的某种知识，而且能够随任务 $$t$$ 迭代地积累知识。

假设任务采用线性回归模型 $$f(\mathbf{x}) = \mathbf{w}^T\mathbf{x}$$，由回归分析知识可知，在平方损失下，使用最小二乘法可得到参数的最优解公式：

$$\mathbf{w}^{\star} = (\mathbf{X}^T\mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}$$

这个结果可以看作汇聚了该任务的所有知识。本方法就利用此式构建变换矩阵。首先不能直接拿来当作 $$P^{(t-1)}$$，它是参数，乘在参数梯度 $$g$$（量纲与参数一致）上是没有意义的。应当取 $$(\mathbf{X}^T\mathbf{X})^{-1}$$，它是一个方形矩阵，维度等于输入维数，也等于参数维数，所以能作用在参数梯度 $$g$$ 上。（它在统计学上应该有某种意义，但我不太清楚，欢迎大佬指点）

众所周知，直接计算这个矩阵是非常麻烦的，主要是因为 $$\mathbf{X}$$ 包含样本数太多。有近似算法——**递归最小二乘（RLS）算法**可以让 $$\mathbf{X}$$ 中的数据一个一个地来，迭代地计算。设 $$\mathbf{X} = [\mathbf{x}_1,\cdots, \mathbf{x}_n]$$，前 $$i$$ 个记为 $$\mathbf{X}_i$$，$$(\mathbf{X}_i^T\mathbf{X}_i)^{-1}$$ 的近似 $$P_i$$ 迭代计算公式：

$$ k_i = \frac{P_{i-1}\mathbf{x}_i}{\alpha + \mathbf{x}_i^T P_{i-1} \mathbf{x}_i}$$

$$P_i = P_{i-1} - k_i  \mathbf{x}_i^T P(i-1)$$

$$P_0$$ 初始化为单位矩阵 $$I$$。（该算法的原理是线性代数中子矩阵、矩阵分块的计算，感兴趣可以自己推导试试，不再详述。）


该方法的一大优点是所需记忆固定的，即一个固定大小的矩阵 $$P$$，不会随任务量增长。

记忆如何积累：面对持续学习，这种迭代的计算方式起到的作用不仅是简化了计算，也让任务之间可以继承，即在计算完毕任务 $$t-1$$ 的 $$(\mathbf{X}^T\mathbf{X})^{-1}$$ 后，可以以此为下一个任务迭代的初值，继续积累下去。也就是说，$$P^{(t-1)}$$ 不是只记录了任务 $$t-1$$，而是记录了所有旧任务 $$1,\cdots, t-1$$。



还需要考虑以下问题：

- 模型不一定是线性模型：对于深度为 $$L$$ 的网络，可以看成 $$L$$ 个线性模型，每层 $$l$$ 都有独立的 $$P$$ 作用在该层的梯度上（**layerwise**），这些 $$P$$ 的更新也是独立的。注意迭代公式里只用到了数据的输入（不用标签），所以 $$\mathbf{X}$$ 用每层的输入即可；
- 数据不是一个一个来的，而是一个 batch 来的：可以简单取 batch 的平均，当作一个数据，当然也有其他更好的方法；
- 可以在 $$P^{(0)}$$ 初始化时引入调节防遗忘程度的超参数 $$\lambda$$：$$P_0 = \lambda I$$。


### GPM

论文链接：[Gradient Projection Memory for Continual Learning](https://openreview.net/forum?id=3AOj0RCNC2)(ICLR 2021)


与 OGD 类似，区别在构造旧任务梯度的方式不一样。它不是就地取材从旧任务实际使用的梯度出发构造子空间，而是从数据下手，提取旧任务数据的信息来构造。也设任务采用线性回归模型，旧任务数据为 $$\mathbf{X} \in \mathbb{R}^{N\times p}$$。提取数据矩阵信息的一大工具是**奇异值分解（SVD）**：

$$\mathbf{X}_{N\times p} = \mathbf{U}_{N\times N}\mathbf{\Sigma}_{N\times p}\mathbf{V}^T_{p\times p}$$

我们要的是与梯度量纲一致的量，观察可以发现是右奇异向量 $$\mathbf{V}^T = [\mathbf{v}_1,\cdots, \mathbf{v}_p]$$，它的维度与参数梯度一致，可以在参数空间中张成子空间。与 OGD 一样，在训练新任务时，让参数在该子空间的正交补中更新即可。


记忆如何积累：由于奇异向量本身是正交的，直接当作正交基存到记忆里即可。


还需要考虑以下问题：

- 对于深度为 $$L$$ 的网络，也需要 layerwise，每层 $$\mathbf{X}$$ 用该层的输入即可；
- 数据不是整个来的，而是分 batch 来的：那计算奇异值分解也分 batch 来即可；
- 为减少计算量和存储量，可以取奇异向量的一部分。$$\mathbf{V}^T$$ 天生就按重要程度（奇异值）排序了，按照一定的准则取前 $$k$$ 个即可。个数 $$k$$ 算是调节防遗忘程度超参数。



## 网络结构法

网络结构法从网络结构下手，将网络划分成各部分并按某种机制分配给各任务，因此又称**参数隔离法**（Parameter Isolation）。它显式地体现了模型容量分配问题，将模型容量这一概念显化到模型各部分参数了。这也是一种直接规定、操控更新过程的方法。

注意，对网络划分的意思并不是为每个任务使用单独的模型，各模型之间完全独立的独立式学习。各部分之间一定有着某种联系。

### 人工的 Mask 机制：PackNet

论文链接：[PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning](https://openaccess.thecvf.com/content_cvpr_2018/papers/Mallya_PackNet_Adding_Multiple_CVPR_2018_paper.pdf), CVPR 2018

如果要固定模型大小，最自然的方式对固定数量的网络（指特征提取器部分的网络，不算输出头）的参数按任务进行划分：训练某任务时，从可用参数中划分出一部分给它，构成子网络训练这部分参数。但这样有独立式学习之嫌。为了避免这种情况，PackNet 论文里采用的方式是：**先将可用参数全部训练，再剪掉（prune）一部分参数预留给后面任务使用**，剩下的留给当前任务。剪掉参数的行为势必会引起效果的断崖式下跌，需要**重新训练**（re-train）保留下来的参数，但训练力度就不必像之前正式训练那样了，可以少些 epoch。过程如下图，注意该图每个圈代表一个权重，这可能表示的是中间某全连接层（5维到5维）的权重。

![](PackNet_training.png)

该模型只适用于 TIL 场景，测试时，知道了测试数据的任务 ID $$t$$，就使用任务 $$t$$ 对应参数的子网络用来作预测即可。

剪参数的方法有很多，都是人工的：文章的做法是直接保留参数若干个绝对值最大的，剪掉剩下的（剪掉的比例固定，作为超参数）。

这个方法显然会带来可用参数随任务越来越少不够用的问题，即模型容量问题。


### Progressive NN

论文链接：[Progressive Neural Networks](https://www.deepmind.com/publications/progressive-neural-networks), arXiv 2016

该方法不固定模型大小，即模型大小可以随任务线性增加。它是每来一个新任务，就将网络（指特征提取器部分的网络，不算输出头）**扩充一列**（一列包括每层若干个神经元），如图：

![](Progessive_NN.png)

它与独立式学习的区别是多了图中斜向右上箭头代表的权重，使新的一列网络与旧网络建立了联系——旧知识（旧网络的输出）可以通过该权重迁移过来。训练时，固定旧网络部分的参数（虚线）不动，训练新加入的参数（实线）。图中 $$a$$ 论文额外加入的非线性函数，可以更加强调斜向右上箭头权重的特殊性。

该模型也只适用于 TIL 场景，测试时使用测试数据任务 ID $$t$$ 对应的部分网络。

该方法主要的问题是模型大小线性增长，它以此为代价解决了模型容量问题。


### 可学习的 Mask 机制：HAT

论文链接：[Overcoming Catastrophic Forgetting with Hard Attention to the Task](https://proceedings.mlr.press/v80/serra18a.html)，ICML 2018

在一个固定大小的网络，上面 PackNet 实现参数隔离的方式实际上是以加在每个参数上的 mask 来规定旧任务占据的参数，本论文是在每个神经元位置规定一个 mask，而且它不是人工选择的，而是融合在训练过程学习出来的。

Mask 实现为二值的 $$m \in {0,1}$$，在每个神经元位置规定一个 mask：由此每层得到一个 mask 向量 $$\mathbf{m}_l, l=1, \cdots, L-1$$。注意，不在最后一层加 mask，否则（根据以下前向传播过程）会导致输出为 0。需要一块记忆存放各个任务的 mask。

本文适用 TIL 场景，即需要任务 ID $$t$$。 在测试阶段，根据数据的任务 ID $$t$$，mask 将每个神经元的输出 $$o_{i}^l$$ 乘以该处任务 $$t$$ 的 mask $$m_{l,i}^t$$，再向前传播。若 $$m_{l,i}^t = 0$$，则$$o_{i}^l = 0$$，相当于从神经元发射的权重 $$w_{ij}^{k+1}$$ 不起作用。这就是相当于用 mask 掩盖部分神经元发射出去权重的**子网络**做测试。


以下讨论训练阶段。在训练新任务时，要考虑如何构造该任务的 mask。它不是算法手动规定的，而是被当作可学习的参数自动学习出来的，学习方法就是将其代入到模型中当作参数随模型参数正常训练。每个 mask 都是借助一个 embedding 计算出来的，学习目标转而变为 embedding $$\mathbf{e}_l$$：

$$ \mathbf{m}_l = \mathbf{1}_{>0}(\mathbf{e}_l)$$

$$\mathbf{1}_{>0}(\cdot)$$ 是一个二值函数，大于 0 取 1，否则取 0。它可以看作 S 型函数 $$S(\cdot) = \sigma(s \cdot)$$ 在 $$s\rightarrow +\infty$$ 的极限。我们不用 Sigmoid 函数是因为需要输出是二值的。但是这个二值函数不光滑，为了训练，应当使用光滑的 $$S(\cdot)$$ 去近似它。但不能直接取一个很大的 $$s$$ 就完事了，这里用到的策略是**退火**（annealing），在训练过程中让每个 batch 取的 $$s$$ 越来越大：

$$s = \frac1{s_{max}} + (s_{max} - \frac1{s_{max}}) \frac{b-1}{B-1}, b = 1,\cdots, B$$

$$s_{max}$$ 是预先设定好的很大的数，据此公式，最后一个 batch 取的 $$s$$ 就是 $$s_{max}$$。（在测试阶段不需要近似，直接用 $$\mathbf{1}_{>0}(\cdot)$$ 或者取 $$s = s_{max}$$。）

> 可以看到，Mask 机制实际上就是一种**注意力机制**，更准确来说是 Hard Attention。[注意力机制](https://en.wikipedia.org/wiki/Attention_(machine_learning))对输入（或者中间层）的每个元素考虑赋予不同的注意力得分，让模型更加关注输入中得分高的部分，得分低的则看作无用信息被抑制。重要的是，这个得分可以通过梯度下降自动地学习出来，而不用手动规定。
> 记住，注意力机制是一种思想的统称，可以与各种模型混合，不单单用在 RNN 中的 Encoder-Decoder 结构。这里的 mask 机制可能是比较简单的一种用法。
{: .prompt-info }

Mask 机制直接面临了模型容量分配问题：每个任务总是倾向于学到一个全是 1 的 mask（某种程度上是 “奥卡姆剃刀” 原理的体现）。它采用的**稀疏化机制**是在损失函数上加一个正则项：

$$ R(\mathbf{m}^{(t)}, \mathbf{m}^{(\leq t-1)})= \frac{\sum_{l=1}^{L-1}\sum_{i=1}^{N_l  } m_{l,i}^{(t)}(1- m_{l,i}^{(\leq t-1)}}{\sum_{l=1}^{L-1}\sum_{i=1}^{N_l} (1- m_{l,i}^{(\leq t-1)}}$$

这个正则项的意思是，在旧 mask 为 0 的位置，新 mask 为 1 的尽量地少（为 0 的尽量地多），尽量使得新 mask 与旧 mask 重合。该函数的自变量是 $$\mathbf{e}^{(t)}$$，包含在 $$\mathbf{m}^{(t)}$$ 中。

最终训练一个任务 $$t$$ 过程如下（Compensation 是文章额外引入的机制，不打算讲解）：

![HAT_forward_and_backward_passes](HAT_forward_and_backward_passes.png)

注意，训练新任务时并没有实际使用旧任务的 mask 进行前向传播，而只是用在了稀疏正则项（下面的 $$\mathbf{m}^{(\leq t-1)}$$）。

## 优缺点讨论

- 重演数据法最大的缺点是重演数据量不够；
- 直接操控更新过程的方法（包括梯度操控法、参数隔离法）有更好的可解释性，但也会带来僵硬的问题。（二者有冲突）



# 六、前沿方向

这里列举一些持续学习的其他前沿方向，不作详细介绍。

- 小样本持续学习：
- 持续异常检测；
- 带粗细粒度的持续学习；
- 持续学习 + Transformer（ViT）：DyTox。
- ...


# 参考资料

以下列举一些持续学习相关的参考资料与学习资源。

1. 课程 Continual Learning（比萨大学）
2. 持续学习社区 Continual AI



<br>


[^footnote]: [Gradient Episodic Memory for Continual Learning]()