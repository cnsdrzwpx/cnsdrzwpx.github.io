---
title: 科研笔记：越来越细的持续学习
author: Shawn Wang
date: 2022-04-27
categories: [科研]
tags: [科研笔记, 持续学习]
math: true
---


这是导师提给我的想法，基于 [IIRC](https://pengxiang-wang.github.io/posts/papernotes_IIRC) 这篇论文。该论文把粗细标签引入到持续学习场景中，允许标签有粗细粒度之分，且要求只能越来越细，把类别增量学习推广到了他们提出的 IIRC 场景。

但是此论文工作较粗略：首先，他们的 IIRC 场景非常 general，新来的任务除了可以是旧任务的子类，还可以是与其并列的超类（参见论文笔记）。此论文初衷是解决越来越细这种关系，可是提出了一个更 general 的模型，也包含了并列关系。其次，论文提了新场景后，只把现有的模型套用到新场景下对比效果，没有提出针对新场景的模型。

我的工作应是，把这个 general 的场景进一步特殊化——只考虑越来越细，并提出适合此场景的模型。

## 场景定义：IIRC 特殊化

对于任务序列 $$\tau_1, \cdots, \tau_N$$，每个任务 $$\tau_k$$ 暂时简化成只来一个类。现在只考虑越来越细，即 $$\tau_t$$ 是 $$\tau_{t-1}$$ 的子类。

插播一个事情：IIRC 场景中一个很关键的问题是，训练阶段并不知道新来的类是超类还是之前超类的子类（可以看成一片森林）。这种层次信息是这个场景新出现的重要信息，模型现在只是知道有这种信息可以利用，但并不知道具体的。我们不能人工地告诉训练阶段，这是作弊。所以设计针对此场景的模型，关键是去学这种信息。ZGZ 师兄正在研究这个问题并与我们讨论。

我的场景不需要考虑这个问题。


## 关于模型的想法

我从持续学习的三大类方法中寻找灵感。*但切记，提想法不应该框死在这三类方法中。*

### 重演方法：对 iCaRL 改进

更有代表性的数据，



### 正则化方法：


### 结构方法：

一个很直观的想法是，对一个大网络，参数更新的集合也越来越细。




## 动手实现

IIRC 的代码已经公开在 GitHub，我已研读过，见另一篇[代码笔记]()。最好的方式是重构、改写此代码，构造出我的特殊化场景。




