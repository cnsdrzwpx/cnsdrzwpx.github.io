[ { "title": "编配：宫崎骏电影主题曲，手风琴二重奏", "url": "/posts/accordion_transcribed_Miyazaki-Hayao-Movie-Themes/", "categories": "音乐", "tags": "手风琴, 乐谱, 音乐编配", "date": "2023-11-28 00:00:00 +0800", "snippet": "这是一年前疫情在家搞的一个大工程。我将宫崎骏导演的所有十部电影的主题曲扒谱成了手风琴二重奏（其中特别喜欢《魔女宅急便》，增加一首）。为了尽量还原原版录音，不知道听了多少遍。虽然是二重奏，但我在编配的时候始终保证主旋律在一声部上，所以！把谱子中二声部右手扔掉，就可以当独奏！以下是曲目列表。乐谱已上传至 MuseScore 和 B 站，有实时的声音与乐谱对照。 从天而降的少女，电影《天空之城》：MuseScore（该曲版权原因无法上传）, B 站，B 站独奏 阿席达卡传奇，电影《幽灵公主》； 人生的旋转木马，电影《哈尔的移动城堡》； 旅路（梦中飞行），电影《起风了》； 那个夏天，电影《千与千寻》； 娜乌西卡安魂曲，电影《风之谷》； 风之甬道，电影《龙猫》； 在晴朗的日子里，电影《魔女宅急便》； 临海小镇，电影《魔女宅急便》； 一去不复返的时光/猪与女郎，电影《红猪》； 深海牧场/空空的水桶/崖上的波妞，电影《崖上的波妞》现在还不全，我会整理好，慢慢地把这些谱子全发布出来，敬请期待！乐谱PDF请扫码获取或访问：https://disk.pku.edu.cn:443/link/D5F5BEB9667011C9AAD63F6FE3409E2E，密码 3Og9。" }, { "title": "四音列及其应用", "url": "/posts/tetrachord/", "categories": "音乐", "tags": "乐理", "date": "2023-04-14 00:00:00 +0800", "snippet": "四音列（tetrachord）在音乐历史上是一个很古老的概念，早在古希腊就用它来发展乐理；但现在对我们仍然有用，可以帮助我们理解和记忆很多东西，包括分解各种七音音阶、记忆和弦连接的 walking bass 等。本节就讨论四音列及其对乐理理解的帮助。定义四音列顾名思义即四个音的序列，可以看做迷你的音阶。音与音之间的间隔一般不超过小三度，所以它前后一般跨四度左右，而不是音阶通常的八度。四音列有三个间隔，根据间隔的不同可作如下分类： 类型 间隔 跨度 大（major） W-W-H 四度 小（minor） W-H-W 四度 上小（upper minor） H-W-W 四度 和声（harmonic） H-3H-H 四度 减（diminished） H-W-H 大三度 吉普赛（gypsy） W-H-3H 减五度 还有两种平凡的四音列： 类型 间隔 跨度 半（chromatic） H-H-H 小三度 全（whole-tone） W-W-W 减五度 七音音阶的四音列分解音阶常见的是七音的，七音音阶可以拆成两个四音列的连接，其中第一个称为下四音列，第二个称为上四音列。以下是常用七音音阶的四音列分解。两个四音列反映了音阶的性质，可以看到，很多音阶的名字与分解出四音列的类型有关。 音阶 上四音列类型 连接间隔 下四音列类型   自然大调 大 全 大   自然小调 小 全 上小   和声小调 小 全 和声   旋律小调 小 全 大   Dorian 小 全 小   Phrygian 上小 全 小 W Lydian 全 半 大   Mixolydian 大 全 小   Locrian 上小 半 全   旋律大调 大 全 上小   和声大调 大 全 和声   双和声大调 和声 全 和声   四音列充当旋律四音列本身即可作为旋律，可以在高音旋律中，也可以作为低音线条，例如连接和弦等。最应该了解的是自然大调音阶中的四音列。其实它们对应了各调式的上四音列：| 四音列 | 四音列类型 |哪个调式的上四音列 || 1234 | 大 | Ionian || 2345 | 小 | Dorian || 3456 | 上小 | Phrygian || 4567 | 全 | Lydian || 5671 | 大 | Mixolydian || 6712 | 小 | Aeolian || 7123 | 上小 | Locrian |有人会问，记住这些除了装逼有什么用？其实在乐器上实现时是很有用的。我们可以记住乐器上各种四音列实现的模式（pattern，例如手风琴的左手模式、吉他指法等），再通过上述理论，就能在脑海中把音阶、旋律线等与实现方式快速对应起来，方便练习与记忆。" }, { "title": "手风琴教程", "url": "/posts/accordion_tutorial/", "categories": "音乐", "tags": "手风琴", "date": "2023-03-25 00:00:00 +0800", "snippet": "这是我个人整理的手风琴教程。注意本文不讨论广义的乐理，只讨论手风琴的技术与乐理在手风琴上的实现。关于乐理知识请参考我的其他博文。本学期我担任北大手风琴协会零基础班的教学任务，本教程也可方便我的学员查阅，在最后附上课程安排。目录 1. 附录：北大手协零基础班课程安排 1.1. 第一课：入门 1.2. 第二课：基本的音符 1.3. 第三课：大三和弦伴奏 1.4. 第四课：音阶与旋律 1.5. 第五课：小三和弦伴奏 1.6. 第六课：属七和弦与更多伴奏模式 1.7. 第七课：和声 1.8. 第八课：音阶与旋律（其他调） 1.9. 第九课：和声（其他调） 2. 手风琴常识 2.1. 分类 2.2. 构造与原理 2.2.1. 发声原理 2.2.2. 放置与使用 2.2.3. 音色 2.3. 选购指南 3. 演奏姿势 4. 右手：键盘 4.1. 右手触键姿势 4.2. 音阶 4.2.1. 自然大调音阶 4.2.2. 小调音阶 4.3. 和弦的实现 5. 左手：键钮 5.1. 左手触键姿势 5.2. 手风琴五线谱左手记号的变化 5.3. 音程：探索单音排列规律 5.3.1. 八度（等音） 5.3.2. 五度 = 向下四度 5.3.3. 大三度 5.3.4. 小三度 5.3.5. 四度 = 向下五度 5.3.6. 六度 = 向下小三度 5.3.7. 大二度（全音） 5.3.8. 小二度（半音） 5.3.9. 小七度 = 向下大二度（全音） 5.3.10. 大七度 = 向下小二度（半音） 5.3.11. 减五度（三全音） 5.3.12. 增五度 5.3.13. 总结 5.4. 和弦的实现 5.4.1. 大三和弦 5.4.2. 小三和弦 5.4.3. 属七和弦 5.4.4. 大七和弦 5.4.5. 小七和弦 5.4.6. 半减七和弦 5.4.7. 挂留和弦 5.4.8. 减七和弦与减三和弦 5.4.9. 增三和弦 5.4.10. 和弦进行与转换 5.4.10.1. 功能和弦的位置 5.4.10.2. 和弦连接 5.5. 节奏型 5.5.1. 柱式和弦 5.5.2. 分解和弦：单音+和弦键钮 5.6. 音阶 5.6.1. 四音列记忆法 5.6.2. 自然大调音阶 5.6.3. 自然小调音阶 5.6.4. 和声小调音阶 5.6.5. 半音阶 5.6.6. 其他调式音阶 6. 风箱 6.0.1. 推拉风箱姿势 6.0.2. 风箱的转换与行程规划 7. 左右手的配合1. 附录：北大手协零基础班课程安排我的零基础课程每周一次，每节课 1 小时。课时安排的主要参考资料：《少儿手风琴集体课教程》，李聪编著，上海音乐出版社（这是手协零基础班指定的教材）。1.1. 第一课：入门本节课主要对应书第一课、第五课。 知识介绍 手风琴常识 演奏姿势、左手触键姿势、右手触键姿势、推拉风箱姿势 课堂练习 1：姿势练习 不开风箱，摆出正确的演奏姿势和左右手触键姿势 课堂练习 2：启动风箱练习 书第 5 页练习，全音符开关风箱，左手用中指（或无名指）按 C，右手拇指按 C（左右手可以分开，也可以一起） 作业 1：查找手风琴资料，培养兴趣 去 B 站、YouTube 关注一些手风琴区 up 主：风流先森、仲凯手风琴、小王的手风琴、修仙大神李小肠、刘宽音乐工厂、毛茸茸的热水袋；Moshe Zuchter、Jo Brunenberg、amarcordeon、CrazyAccordion Trio、Just Duet 阅读我写的手风琴的各种风格一文，观看其中的视频 作业 2：复习课堂练习 1、2 练会为止 作业 3：自由探索手风琴 把手风琴当做玩具，自由探索它的各部件（变音器、排气孔等），找到摸琴、拉琴的感觉1.2. 第二课：基本的音符本节课主要对应书第二课、第三课。 知识介绍 乐理基本知识：五线谱音的位置、时值、拍子、速度等 C、F、G 在手风琴上左手的位置（先死记硬背下来） 练习 1：右手单音练习 只涉及 C 到 G 五个音 书第 8-9 页练习 1-5，第 10-11 页练习 1-3 练习 2：左手单音练习 只涉及 C、F、G 三个音 书第 7-8 页练习 1-4，第 10 页练习 1-2 练习 3：左右手单音练习 右手只涉及 C 到 G 五个音，左手只涉及 C、F、G 三个音 书第 11-13 页练习 1-3，两首乐曲 作业：复习练习 1、2、3 课堂上只示范了一小部分练习，作业要求尽量把上述书中的练习练会（根据自己的时间来）1.3. 第三课：大三和弦伴奏本节课主要对应书第四课。 知识介绍 和弦的概念、简单的种类（大三和弦、小三和弦、属七和弦）、根音、记号 和弦功能、和弦进行的概念 手风琴左手 Stradella 系统 三和弦进行（1、4、5 级和弦）在手风琴左手的实现（C 大调） 练习 1：大三和弦节奏型 单独练习 C、G、F 和弦 练习柱式和弦 练习分解和弦（只用根音）：4/4 拍的 |o x x x|、3/4 拍的 |o x x|、2/4 拍的 |o x| 节奏型 练习 2：大三和弦转换 C、G、F 和弦的转换 书第四课各个练习的左手部分 练习 3：大三和弦左右手配合 用 C、G、F 三个和弦伴奏 C 大调歌曲 书第四课乐曲练习 “Oh, Susanna” version 1 作业：复习练习 1、2、3，并巩固第二课的练习 注意体会只用单音伴奏和用和弦伴奏的区别 最重要的是通过不断的练习，找到自如地拉手风琴的节奏感1.4. 第四课：音阶与旋律 知识介绍 十二平均律，八度 音阶的定义：根音、排列方式（比较详尽的理论见我的这篇有关音阶的博文） 升降号 自然大调音阶、自然小调音阶、关系大小调；和声小调音阶 手风琴的左右手指法 练习 1：右手音阶 C 大调与 A 小调自然音阶、A 和声小调音阶 书第 36、70 页，注意指法，上下行 练习 2：左手音阶 C 大调与 A 小调自然音阶 两种模式，选择适合自己的指法 练习 3：右手旋律 C 大调与 A 小调右手旋律 书第九课、第十二课、第十五课旋律视奏（注：第十二课不拉有调号的，第十五课不拉带升降号的） 作业 1：复习练习 1、2、3 进阶练习：用左手旋律拉试试1.5. 第五课：小三和弦伴奏 知识介绍 三和弦进行（1、4、5 级和弦）在手风琴左手的实现（A 自然小调、和声小调） 练习 1：小三和弦节奏型 单独练习 Am、Em、Dm 和弦 练习柱式和弦 练习分解和弦（只用根音）：4/4 拍的 |o x x x|、3/4 拍的 |o x x|、2/4 拍的 |o x| 节奏型 练习 2：小三和弦转换 Am、Em、Dm 和弦的转换 Am、E、Dm 和弦的转换 练习 3：小三和弦左右手配合 用 Am、Em、Dm 三个和弦伴奏 A 自然小调歌曲 “敖包相会” version 1；”天堂” version 1 练习 4：小三和弦左右手配合 用 Am、E、Dm 三个和弦伴奏 A 和声小调歌曲 “喀秋莎” version 1 作业：复习练习 1、2、3、4 可以试验一下练习 3、4 反过来用（即：用 Am、E、Dm 三个和弦伴奏 A 自然小调歌曲，而用 Am、Em、Dm 三个和弦伴奏 A 和声小调歌曲），体会二者的区别。1.6. 第六课：属七和弦与更多伴奏模式 知识介绍 属七和弦的作用 练习 1：属七和弦节奏型 练习柱式和弦 练习分解和弦（只用根音）：4/4 拍的 |o x x x|、3/4 拍的 |o x x|、2/4 拍的 |o x| 节奏型 练习 2：属七和弦转换与左右手配合 C、F、G7 和弦的转换 Am、E7、Dm 和弦的转换 将 C、F、G 与 Am、E、Dm 伴奏的曲子换用相应的属七和弦练习 练习 3：交替低音（五音） 三角手型，练习分解和弦（五音交替低音）：4/4 或 2/4 拍的 |1 x 5 x|、3/4 拍的 |1 x x|5 x x| 节奏型 将之前的曲子换用交替低音伴奏练习 作业：复习练习 1、2、3 注意体会属七和弦与大三和弦、只用根音与交替低音伴奏的区别。1.7. 第七课：和声 知识介绍 C 调常用和弦（C、Dm、Em、F、G、Am、Bdim）的具体功能 常见和弦进行 练习 1：不同跨度的和弦转换练习 练习不同跨度的和弦转换（包括向上和向下），对比不同跨度的感觉，锻炼快速定位的肌肉记忆，例： 跨 1 个位置：C 到 G，G 到 C；Dm 到 Am，Am 到 Dm；Dm 到 G 等； 跨 2 个位置：F 到 G，G 到 F；Dm 到 Em，Em 到 Dm；Am 到 G，G 到 Am 等。甚至更难的：F 到 G 到 Am，Am 到 G 到 F； 跨 3 个位置：C 到 Am，Am 到 C；F 到 Dm，Dm 到 F 等； 跨 4 个位置：C 到 Em，F 到 Am，Am 到 F 等； 跨 5 个位置：F 到 Em，Em 到 F 等。 练习 2：和弦进行练习 （根据自己的能力）练习 C 调常见的和弦进行，使用不同的拍子和节奏型。必须要求不能断，卡在拍子上（错了应能够即时纠正，而不是停下来） 1451（大调）：C-F-G-C 1451（小调）：Am-Dm-Em-Am 1645、1564、6415：C-Am-F-G、C-G-Am-F、Am-F-C-G 4566：F-G-Am-Am 五度圈进行 4536(251)：F-G-Em-Am-Dm-G-C 卡农进行：C-G-Am-Em-F-C-Dm-G 顺阶低音进行：C-G/B-Am-G-F-Em-Dm-G 练习 3：C 调歌曲左右手配合 用 C 调常用和弦或上述和弦进行伴奏 C 调歌曲 作业 1: 复习练习 1、2、31.8. 第八课：音阶与旋律（其他调） 知识介绍 升降号、乐曲的调、五线谱的调号 五度圈 练习 1：右手音阶 练习 C 调以外其他 12 个调的右手音阶，按照五度圈由近及远顺序 G 大调与 e 小调自然音阶（#） F 大调与 d 小调自然音阶（b） D 大调与 b 小调自然音阶（##） Bb 大调与 g 小调自然音阶（bb） A 大调与 f# 小调自然音阶（###） Eb 大调与 c 小调自然音阶（bbb） E 大调与 c# 小调自然音阶（####） Ab 大调与 f 小调自然音阶（bbbb） B 大调与 g# 小调自然音阶（#####） Db 大调与 bb 小调自然音阶（bbbbb） F#(Gb) 大调与 eb 小调自然音阶（######/bbbbbb） 练习 2：右手旋律 练习来自其他 12 个调的旋律: 练习 3：转调 给一段旋律，练习转到任意 12 个调 练习 4（进阶）：相对音感练习 听一段旋律，在手风琴上实现出来（扒谱），并识别是哪个调作业 1：复习练习 1，自己找素材练习 2、3作业 2：自己找素材，练习 4。这个练习可能会花大量时间，请按照自己的能力来。1.9. 第九课：和声（其他调） 知识介绍 其他调的和声 左手 Stradella 系统的相对不变性 练习 1：G 调和声 和弦进行练习：练习 G 调常见的和弦进行，使用不同的拍子和节奏型； 左右手配合：用 G 调常用和弦或上述和弦进行伴奏 G 调歌曲。: 练习 2：D 调和声 同上，练习 D 调: 练习 3：F 调和声 同上，练习 F 调: 练习 4：转调 给一首曲子（包括旋律与和声），练习转到任意 12 个调 练习 5（进阶）：给旋律配和弦 听一段旋律，识别它是哪个调，给它配和弦 可以参考原曲的和弦谱来检查，但请注意，配和弦本身没有正误之分，需要做的是练耳朵，体会不同和弦配法的感觉作业 1：推导并练习其他 12 个调的常用和弦与和弦进行作业 2：找自己喜欢的歌曲的和弦谱，按照自己的感觉搭配节奏型，在手风琴上练习作业 3：自己找素材，练习 4作业 4：自己找素材，练习 5。这个练习可能会花大量时间，请按照自己的能力来。2. 手风琴常识2.1. 分类手风琴的种类繁多，最常见的手风琴有键盘式手风琴（piano accordion）和键钮式手风琴（button accordion）两种，它们的区别是右手的结构，前者为钢琴键盘，后者为键钮。以下如无特殊情况，默认为键盘式手风琴。除此之外，还有流行于特定地区的手风琴变种，如六角手风琴（concertina）、班多钮手风琴（bandoneón），它们不在本文的讨论范围内，感兴趣的可以自行了解。2.2. 构造与原理手风琴内部的机械结构非常复杂，这里只讲够用的基础的原理。2.2.1. 发声原理以上是键盘式手风琴的结构图。手风琴，顾名思义，是用手带动风来发声的，具体来说，手风琴是一种簧片（reed）乐器，风引起簧片振动产生声音。手风琴内部有大量调成不同音高的簧片，手风琴上的每个键（左手键钮、右手键盘）都对应某簧片，按下之后，该簧片的进/出气口（以下简称“气口”）打开，推拉风箱（bellows，即存储空气的箱子），风从该气口进入风箱或排出引起该簧片的振动，产生对应的音。也就是说，手风琴发声的两个要素是：气口打开（按住某个键），有风流动（推拉风箱）。只按住键，是不会出声的，因为没有风的流动；不按键，风箱也是推拉不动的，因为气口没有打开。 不按键时，千万不要硬推拉风箱，这样相当于强迫手风琴打开一个气口，会损坏手风琴的内部结构，导致风箱漏气。 除了左手键钮和右手键盘，手风琴还有一个键可以充当气口的作用，它位于左手键钮的上侧面。它与其他键的区别是它不对应任何簧片的气口，因此按它推拉风箱不会发声。它用于演奏完毕时无声地关闭手风琴的风箱，所以一般称为排气孔。2.2.2. 放置与使用放置手风琴时，需要把风箱完全关闭，扣上风箱两侧的卡扣（起到固定作用，防止重力导致硬拉风箱），临时情况可以竖直放置（右手键盘平行于地面），更稳的放置方法是水平放置（右手键盘朝上垂直于地面，如第一张图的键盘手风琴）。最好不要躺着放。使用时记得把两侧的卡扣都打开。2.2.3. 音色手风琴的音色可以模拟出不同乐器的效果：如长笛、短笛、巴松等。手风琴的音色是有固定的档位的，通过变音器（register）开关（switches）来选择（单项选择）。变音器开关是通过联动背后的机械结构，选择不同音色的簧片来控制音色的。左手键钮与右手键盘分别有各自的独立的变音器开关控制音色，均位于它们的上方。手风琴变音器的音色用如下格式的记号表示，主要元素是圆圈、横线划成的格子、格子里的点。一个格子表示一种基础音色的簧片，上面的格子音调高，下面的低（八度关系）；格子里的点代表选择了该基础音色的簧片，即打开了其气口。以 3 个格子为例，虽然只有 3-4 种基础音色，但可以组合它们得到更多种类的音色（理论上 2 的 4 次方），例如图中第 1 个与第 2 个合成相当于第 6 个。 基础音色越多（即记号中的点越多）的音色，实际上声音也会越大，因为打开的气口更多。因此，第 6 个音色用同样的力气拉声音会比第 1、2 个的大。 有的手风琴右手键盘下侧有一个宽阔的“空格键”，拍下它后所有变音器开关弹起，代表所有基础音色的簧片的气口都打开。在上图的手风琴中，它等价于最后一个音色。2.3. 选购指南手风琴是一个很贵的乐器，起步几千元，一般要几万。在选购的时候要慎重。手风琴有大小之分，是按照左手的键钮（bass）数来命名规格的，普通大小的手风琴一般为 48、60、72 bass，最大的（全尺寸）手风琴为 120 bass。手风琴大小的影响是： 越大也就越重； 对右手键盘来说，手风琴越大，可演奏的音域越广； 对左手键钮来说，小的手风琴会缺一些音与和弦，也难以实现一些调的和声（学完左手的音就会理解）； 越大可以选的音色也越多，太小的手风琴只有一种音色。手风琴还有普通手风琴和回声手风琴的区别，回声手风琴内部结构更加精密，音色也比普通手风琴细腻、响亮，但其价钱要贵一倍左右。知名的国际品牌有霍纳（Hohner）、Pigini、Bugari、Polverini、Victoria、罗兰（Roland），国产品牌有百乐（Baile）、鹦鹉（YINGWU）、金杯（GOLDENCUP）。国产琴质量比进口琴差，但便宜很多。3. 演奏姿势手风琴演奏的姿势分为坐着和站立两种，参考下图。坐着演奏的要领： 像“双肩包背在前面”那样前背着琴； 调节左右肩带到合适的位置：一般右肩带比左肩带略长，个子高的需要更长的肩带；使得琴身自然松弛地贴近身体即可，不能太近也不能太远。 左手插进左边的带子里，这样才能向外拉风箱； 调节左手键盘上方的滚轮可以调节左手带子的松紧程度，可以参考此标准：拉风箱时手掌根不离开风箱板，推风箱时皮带不脱离手背，手上下活动时既不松松垮垮也不产生较大的阻力； 不要靠在椅背上，靠前一点坐（1/2 或 1/3 处），这样更方便用力； 琴的重心落在左腿，使得左手的风箱能以左腿为支点自由开合； 右大腿内侧抵住右手键盘的右下角，用于固定琴的前后位置（小琴可以不用）； 脚正常地平放在地上，不要踮脚或盘腿，右脚可以打拍子（不要用左腿，会引起风箱晃动）。所有乐器演奏的精髓是放松不紧张，这里也重复一下这个原则，练琴时应时刻注意身体的放松，保持愉快的心情。4. 右手：键盘手风琴的右手键盘负责高音旋律声部。它与钢琴是一致的，本章的学习内容也与钢琴的右手相通。4.1. 右手触键姿势对键盘式手风琴，其触键姿势与钢琴基本一致： “握鸡蛋”手型，自然弯曲成圆弧形，手背保持与键盘的大致平行； 指尖外侧垂直触键（不宜留指甲），不要折指（第一关节向外侧弯曲），触键要果断但不要击键（指高抬手指击打琴键）； 触键时其他指头不要翘指（指离开琴键，翘起）；但由于手风琴的键盘是竖直放置的，在一些细节上与钢琴有所区别： 手腕可以略高一些，这样方便竖直的演奏； 拇指一定要保证指尖触键，不能一整根指头躺在琴键上； 右肘、右肩应自然下垂，不抬肘、压肘、耸肩。 手风琴键盘与钢琴键盘最重要的区别是没有力度反馈，力度是靠左手对风箱的控制实现的，这一点需要会弹钢琴的同学慢慢适应。4.2. 音阶音阶的理论知识已经在另一篇博文中详细讨论过，这里我只讨论在手风琴右手的指法。该部分知识与钢琴通用。音阶安排指法的原则是： 将音阶中的音划分为若干组（每组不超过 4 个音），分组来弹，组间运用穿指和跨指技巧（上行穿指、下行跨指）。理由：1. 音阶大部分是 5 音以上的，人只有 5 个手指，必须分组弹；2. 小指穿指是非常困难的，所以一组最多 4 个音，因为 5 个音会涉及小指穿指。 不用拇指或小指弹黑键。理由：拇指或小指较短，位置较低，用拇指或小指弹黑键会升高手的位置（当然，如果全是黑键一直不涉及白键，也是可以用的）；以下讨论每个音阶的指法，数字代表指头，- 代表分组。注意不要去死记硬背，而是在练习的过程不断感受它是如何符合原则与科学性。4.2.1. 自然大调音阶首先讨论自然大调音阶，这是最基础、必须掌握的。根音在白键的自然大调音阶有 7 个，其中： 除 F 以外，其他的 C、D、E、G、A、B 音阶都是默认的 123-1234； F 音阶：1234-123。理由：避免拇指弹 4 音黑键。根音在黑键的自然大调音阶有 5 个。由于根音在黑键，所以音阶的起点都不是拇指。这些音阶组织指法的原则是：在黑键过渡到白键的位置分组。 C#：23-1234-1； Eb：3-1234-12； F#：234-123-1； Ab：34-123-12； Bb：4-123-123。4.2.2. 小调音阶对自然小调、和声小调、旋律小调音阶，结果是类似的。根音在白键的音阶与自然大调音阶遵循相同的指法，即除 F 外使用 123-1234，F 用1234-123。根音在黑键也是类似的，不再赘述。以上音阶是大部分音乐需要的，其他音阶不常用不在此介绍，可能会在风格专题中涉及。4.3. 和弦的实现本节只介绍右手和弦的琶音（Arpeggio）。琶音与音阶安排指法的原则是相同的。琶音一般只涉及大小三和弦。首先讨论上下行（上下行指法相同）。根据上述安排指法的原则，同样可以得到： 根音在白键的：123-123 ...； 根音在黑键的： 都是黑键的只有 F#，指法同上：123-123 ...； 只有五音在白键的只有 Bbm，指法为 231-231 ...； 其余全部为：2-124-124... 5. 左手：键钮手风琴的左手键钮负责低音和声声部，它与右手键盘逻辑不一样的点在于，一个键钮不仅可以对应一个单音，还可以对应多个音，即和弦。左手的键钮设计为平行四边形（而不是正方形）排列，有较少的列和较多的排，不同的音排列在排与列相交的键钮中。音的排列有很多方式，常见的有斯特拉德拉（Stradella）低音系统、自由低音（free-bass）系统两种，其中自由低音系统还分为 B 系统、C 系统。一般的手风琴都是 Stradella 系统（通常称为传统低音系统）的，自由低音要难很多，本文不介绍。上图为 Stradella 系统的排列方式。每一排上的键钮代表了以某音为根音的一系列单音与和弦： 第二列：根音（单音）； 第一列：根音的大三度音（单音），一般称为对位低音（counterbass）； 第三列：根音构成的大三和弦，即根音+大三度音+五度音； 第四列：根音构成的小三和弦，即根音+小三度音+五度音； 第五列：根音构成的属七和弦（缺五度音），即根音+大三度音+属七度音； 第六列：根音构成的减七和弦（缺五度音），即根音+小三度音+六度音。各排是按五度圈的关系排列的，即相邻排之间的音相差五度。 手风琴左手的五度圈的排列方式使其有一个巨大的优点——调无关性，即左手音的相对顺序与调无关，转调只需串动手的上下位置，而不需要改变按法或模式。在学习手风琴左手时，只需要学习一个调的模式，其他的调自然就会学会；相比之下，钢琴键盘有黑键和白键之分，导致 12 个调的模式完全不同，每一个调都需要单独学一遍。下文对左手的教程会始终贯穿这一概念，我只介绍 C 调，其他调可以自然推出；记号也会由绝对的音名（C、D…）改为相对的数字（1、2…）。需要强调的是，手风琴左手的音一律不区分八度。根据十二平均律，左手只能发出 12 个音，左手键钮每隔 12 排是一个周期，同列跨 12 排的键钮音是完全相等的。上图是只区分相对位置的左手图，我以相对的数字代替绝对的音名，便于表述构造和弦的过程。注意这里 1 代表根音，其他数字代表距离根音相对音程的音（不需要区分八度，共 12 个音），而不是绝对的唱名。由于左手有两列交错为大三度的单音，我分别将每列的根音置于中心，得到了两张图，便于观察。下面音程、和弦、音阶等的构造均由此图得来。5.1. 左手触键姿势左手触键姿势与右手键盘类似：自然弯曲；指尖垂直触键，不折指、翘指，触键要果断但不要击键；左肘、左肩自然下垂，不抬肘、压肘、耸肩。它的不同之处在于键是更小的排列更密的键钮，所以拇指是用不上的（因为难以独立地按一个键），它通常自然并拢在食指的第二关节处。也因为更密，所以对垂直触键的要求更高，否则就容易误触其他键。5.2. 手风琴五线谱左手记号的变化一份完整的乐谱有以下几个作用： 表示音高（乐理层面）； 表示音高乐器上的哪个位置，例如钢琴琴键的位置、吉他的弦和品等（乐器实现层面）； 表示如何演奏，如指法等（乐器实现层面）。对于钢琴，五线谱就能完全包含上述三个作用，因为音高与钢琴琴键是一一对应的，最多只需在五线谱的音符上标记指法。而对吉他等有多种实现方式的乐器，五线谱是不够用的，它无法表示音高在乐器上的实现方法，因此出现了六线谱（指法谱）。手风琴用的是五线谱，它与普通的（钢琴）五线谱基本是一样的，但是有细微的差别。键盘式手风琴的右手即钢琴键盘，高音谱是完全一样的；而左手由于有的键代表多个音、不区分八度、存在等音等原因，普通的低音谱无法完美地表示，因此需要引入一些记号和规范，使谱子可以唯一确定左手按的键钮： 以中线为分界线，中线及以下的音为单音（第一、二列），以上的音为和弦（第三到六列），如下图。单音与和弦各有 12 个，因此左手谱音符最低不低于下加一线，最高不高于上加二线； 和弦以音符的位置表示根音，在旁边标记 M、m、7、dim（中文用：大、小、7、减；俄文用：Б、М、7）区分和弦类型（第三到六列的第几列）； 单音用横杠表示用第一列音，不带横杠表示用第二列音（一般标注在指法下方）。 我的手风琴编配谱是用打谱软件制作的，因为要展示效果，所以会把和弦完整表示出来（柱式），而不是上面这种形式。俄罗斯的手风琴谱一般也会把和弦完整表示出来。5.3. 音程：探索单音排列规律手风琴左手通过第二列和第一列发出单音。由于左手有“调无关性”，我们从音的相对位置入手，即音程。在学习左手时，首先要牢记各种音程的左手手型/指法，形成肌肉记忆，并了解音程可能的应用场景（构造音阶或和弦）。以下我将探讨这几个话题。 从相对位置开始学习，不代表左手音的绝对位置不重要。为了更快的视奏等要求，音的绝对位置也是要牢记的。由于左手音不区分八度，我们只需讨论八度及以下的音程。手风琴左手的排列顺序是科学的，其纵向的五度圈排列和横向的大三度错位排列，让常用的音程排在了方便的位置。我将按照常用程度的顺序介绍。5.3.1. 八度（等音）手风琴的一个周期内（12排），分布有两个等音，分别位于一二两列，间隔 4 排、8 排，如上图。在寻找等音时，最简单的方式是： 找第二列音的等音：向下 4 排，跨到第一列； 找第一列音的等音：向上 4 排，跨到第二列。 我们当然也可以： 找第二列音的等音：向上 8 排，跨到第一列； 找第一列音的等音：向下 8 排，跨到第二列；甚至： 同列向上或向下 12 排；但这些离得都太远了，对看不见的左手而言，精确定位这么远的距离是非常困难的。上述所谓“最简单的方式”，就是指键钮的距离最近（一般小于 5 排）。以下我总是介绍最简单的方式，想要所有实现方式，见本节最后的汇总表。 5.3.2. 五度 = 向下四度手风琴左手的五度圈排列使得五度是最容易实现的音程。寻找五度音，只需： 同列向上 1 排；对第二列，还有一种可行的方式： 找第二列音的五度音：向下 4 排，跨到第一列。五度是最常用的音程。5.3.3. 大三度大三度对第二列来说是对位低音： 找第二列的大三度音：跨到第一列；寻找大三度音的通用方式如下，但比较远： 同列向上 4 排；对第一列，还有一种和通用方式一样远的方式，更不常用： 找第一列音的大三度音：向下 4 排，跨到第二列。5.3.4. 小三度小三度对第二列不是很友好，只能用通用的方式： 同列向下 3 排；但对第一列音比较方便： 找第一列音的小三度音：向上 1 排，跨到第二列。5.3.5. 四度 = 向下五度手风琴左手的五度圈排列使得四度也是最容易实现的音程。因为四度和五度是互补的，所以音的排列也互为镜像关系，建议和五度一起记忆。寻找四度音，只需： 同列向下 1 排；对第一列，还有一种可行的方式： 找第一列音的五度音：向上 4 排，跨到第二列。5.3.6. 六度 = 向下小三度六度和小三度是互补的，所以音的排列也互为镜像关系，建议和小三度一起记忆。六度对第二列音有两种实现方式： 找第二列音的六度音：向下 1 排，跨到第一列； 同列向上 3 排；对第一列音只能用同列通用的方式。5.3.7. 大二度（全音）大二度音即全音，对第二列音有两种同样实现方式： 同列向上 2 排； 找第二列音的大二度音：向下 2 排，跨到第一列；在下文自然大调音阶的两种构造方式就对应于这两种。对第一列音只能用同列通用的方式。5.3.8. 小二度（半音）小二度音即半音，对第二列音： 找第二列音的小二度音：向上 3 排，跨到第一列这种方式是下文构造半音阶的主要元素。对第一列音有一个比较方便的方式： 找第一列音的小二度音：向下 1 排，跨到第二列。这是构造自然大调音阶中的半音的方式。5.3.9. 小七度 = 向下大二度（全音）小七度和大二度是互补的，所以音的排列也互为镜像关系，建议和大二度一起记忆。对第二列音只能用同列通用的方式： 同列向下 2 排；对第一列还有一种可行的方式： 找第一列音的小七度音：向上 2 排，跨到第二列；5.3.10. 大七度 = 向下小二度（半音）大七度和小二度是互补的，所以音的排列也互为镜像关系，建议和小二度一起记忆。大七度对第二列音是很方便的： 找第二列音的大七度音：向上 1 排，跨到第一列对第一列音只能： 找第一列音的大七度音：向下 3 排，跨到第二列。5.3.11. 减五度（三全音）减五度即三全音，对第二列音： 找第二列音的减五度音：向上 2 排，跨到第一列对第一列音只能： 找第一列音的减五度音：向下 2 排，跨到第二列。5.3.12. 增五度增五度和大三度是互补的，所以音的排列也互为镜像关系，建议和大三度一起记忆。增五度和大三度反过来，对第二列来说非常麻烦： 找第二列音的增五度音：向上或向下 4 排，跨到第一列；对第一列非常方便，可以理解为反对位低音： 找第一列音的增五度音：跨到第一列。增五度是最不常用的音程。5.3.13. 总结我将所有音程的所有可能的最简单的实现方式汇总于此表。其中 +、- 分别表示向上或向下跨的排数。标粗的为常用的。 音程 同列 二列跨一列 一列跨二列 小二度（半音） -5 +3 -1 大二度（全音） +2 -2 +6,-6 小三度 -3 +5 +1 大三度 +4 0 -4 四度 = 向下五度 -1 -5 +3 减五度（三全音） +6,-6 +2 -2 五度 = 向下四度 +1 -3 +5 增五度 -4 +4 0 六度 = 向下小三度 +3 -1 -5 小七度 = 向下大二度（全音） -2 +6,-6 +2 大七度 = 向下小二度（半音） +5 +1 -3 八度（等音） +12 或 -12 -4 +4 总体记忆时，可以参考下图，分根音在第二列还是第一列来记。这两种情况是不同的，有的音程对前者友好，有的对后者友好。这里“友好”的意思是位置近或实现方式多（2个）。5.4. 和弦的实现手风琴左手的第三到六列直接提供了四种和弦，只需按一个键钮即可实现，起到“自动”的效果。除此之外，大部分其他种类的和弦也可以通过组合不同的键钮（可以是单音或和弦）间接实现。本节将详细讨论各种和弦在手风琴左手所有实现的可能性，描述其常用指法和实现难度。这些实现方式中，有的根音位置较高（2 指按，向下伸展），有的位置低（5 指或 4 指按，向上伸展），有的在中间位置（3 指或 4 指按，上下均匀伸展）。在实际中如何选择，需要根据当时手的位置选择更方便的、能顺利衔接上的、不需要大幅挪动的，这种即时的反应能力需要对每种方式及其相互的组合作大量的练习。和弦的构造是从根音开始，分别添加相应音程的音。以下为了方便叙述和弦记号，均默认为 C 和弦。每种和弦我会先介绍带和弦键钮的方法，再介绍只有单音键钮的方法（后者我会在括号中备注），它们都有不同的应用，在“节奏型”一节会讲述。5.4.1. 大三和弦观察此图，可以得出结论：大三和弦一般适合根音在第二列时构造，因为在第一列时，包含关键的大三度音的键钮（浅黄色）离根音都太远。这个道理同样适用于所有大和弦，即以大三和弦（根音+大三度音+五度音）为基础的和弦，包括属七和弦、大七和弦、大九和弦等。 方法一 根音在第二列时，大三和弦可以按根音下的第三列“一键”实现：C+C大，指法为 4+3。大三度和五度音都在根音旁边，非常方便，可以在上述指法基础上，用 4 指按三音，2 指按五音。 在少数节奏欢快且通篇为大三和弦的曲子中，可以用指法 3+2，目的是获得更多的手指力量。 方法二（只有单音） 根音在第二列时，只有单音最简单的实现方式是：C+E+G，指法为 3+4+2。 方法三（只有单音） 另一种只有单音的实现方式是同列实现。根音在第二列时，不用第一列的三音，而是上方较远的第二列的三音：C+E+G，指法为 5+2+4。（这种同列实现方法当然也适用于根音在第一列：C+E+G。） 方法四（只有单音） 根音在第一列时，还有一种实现方式是：C+E+G，指法为 2+5+2。5.4.2. 小三和弦小三和弦可以根音在第二列或第一列时构造，但第二列时更方便。 方法一 根音在第二列时，小三和弦可以按根音下的第四列“一键”实现：C+C小，指法为 4+2。如果这样按，虽然可以方便地用 3 指按五音，但三音（小三度音）在下方较远的位置，即使用 5 指也是够不到的。 我们需要切换成指法 3+2，然后用 5 指按三音，此时根音上方的五度音可以用 3 指摸。与大三和弦同理，3 指按根音、2 指按小三和弦的指法也常用于节奏欢快的曲子，获得更多的手指力量。对于小三和弦，这种指法需要把 2 指和 3 指伸展开来。 方法二 根音在第二列，还有一种方法，要利用到第一列的小三度音：C+Eb+G。由于它离第二列的根音非常远，指法只能是 ???5+2+4，小指按根音手指力量够用吗???，能不能是 4+2+3??? 方法三（只有单音） 根音在第一列时，小三和弦无法“一键”实现，但仍然可以在向上一排方便地找到小三度音和五度音：C+Eb+G，指法为：4+2+3。这也是只有单音的小三和弦最简单的实现方式。 方法四（只有单音） 第二种只有单音的实现方式是同列实现：C+Eb+G，指法为 3+5+2。 方法五（只有单音） 对方法四，还可以使用上方较远的第一列的三音：C+Eb+G，指法为 5+2+4。5.4.3. 属七和弦属七和弦是在大三和弦基础上加入小七度音，根据大三和弦的结论，只讨论根音在第二列。 方法一 根音在第二列时，属七和弦可以按根音下的第五列“一键”实现：C+C7，指法为 4+2。如果这样按，可以用 4指按三音，3指按五音，七音在根音向下两排的位置，要用 5指按七音。同上理，3+2 的指法也可以用，但需要更多的伸展，按起来也有点别扭，除特殊情况一般不用。 方法二 属七和弦还可以直接在大三和弦上加属七度音实现，即：C+C大+Bb，指法为 3+2+4。 方法三（只有单音） 根音在第二列时，只有单音最简单的实现方式是：C+E+G+Bb，即大三和弦的方法二加入了第二列下方的七音。指法为 3+4+2+5。 方法四（只有单音） 根音在第一列时有实现方式：C+E+G+Bb，即大三和弦的同列实现（方法三）在第一列时加入了第二列的七音。指法为 5+2+4+3。 方法五（只有单音） 根音在第一列时，另一方向还有一种实现方式：C+E+G+Bb，即大三和弦的方法四加入了第一列的七音。指法为 2+5+2+4。5.4.4. 大七和弦大七和弦是在大三和弦基础上加入大七度音，根据大三和弦的结论，只讨论根音在第二列。构造大七和弦的大七度音位于根音斜上方，方便归方便，但大七和弦整体没有“自动”实现方式，只能间接实现。 方法一 大七和弦可以通过“根音+以大三度音为根音的小三和弦”实现，即 C+E小，指法为 5+2。由于距离很远，该方法只能用这个指法，对手指的伸展要求很高。 方法二 大七和弦还可以直接在大三和弦上加大七度音实现，即：C+C大+B，指法为 4+3+2。这种方式键位密集，容易卡住指头，不推荐。以下建议与属七和弦只有单音的实现方法一起记忆，它与属七和弦只有七音的差别。 方法三（只有单音） 根音在第二列时，只有单音最简单的实现方式是：C+E+G+B，即大三和弦的方法二加入了旁边的七音。指法为 3+4+3+4 或 2+3+2+3。 方法四（只有单音） 大七和弦有同列实现：C+E+G+B，即大三和弦的同列实现（方法三）加入了上方的七音。指法为 5+2+5+2。 方法五（只有单音） 根音在第一列时，另一方向还有一种实现方式：C+E+G+B，即大三和弦的方法四加入了第二列的七音。指法为 2+5+2+5。5.4.5. 小七和弦 方法一 根音在第二列时，小七和弦可以直接在小三和弦上加小七度音实现：C+C小+Bb，指法为 3+2+4。可以在该指法的基础上，5 指按三音，3 指按五音。 方法二 根音在第一列时，小七和弦可以通过“根音+以小三度音为根音的大三和弦”实现，即：C+Eb大，指法为 4+2。可以在该指法的基础上，3 指按三音或七音，5 指按五音。 方法三（只有单音） 根音在第一列时，只有单音最简单的实现方式是：C+Eb+G+Bb，即小三和弦的方法三加入了旁边的七音。指法为 4+2+4+2 或 3+2+3+2。 方法四（只有单音） 小七和弦有同列实现：C+Eb+G+Bb，即小三和弦的同列实现（方法四）加入了中间的七音。指法为 2+5+2+5。 方法五（只有单音） 根音在第二列时，另一方向还有一种实现方式：C+Eb+G+B，即小三和弦的方法五加入了第一列的七音。指法为 5+2+5+2。5.4.6. 半减七和弦 方法一 根音在第一列时，小七和弦可以通过“根音+以小三度音为根音的小三和弦”实现，即：C+Eb小，指法为 4+2。可以在该指法的基础上，3 指按三音或七音，4 指按五音。以下建议与小七和弦只有单音的实现方法一起记忆，它与小七和弦只有五音的差别。 方法三（只有单音） 根音在第一列时，只有单音最简单的实现方式是：C+Eb+F#+Bb，指法为 4+3+5+2。 方法四（只有单音） 根音在第二列时，一种实现方式：C+Eb+F#+Bb，指法为 3+5+2+5。 方法五（只有单音） 根音在第二列时，另一方向还有一种实现方式：C+Eb+F#+Bb，指法为 5+2+4+2。5.4.7. 挂留和弦挂留和弦是基于四度音程构造的和弦，在手风琴上，四度的单音是相邻的，用连续的相邻音可以构造出如下几种和弦，它们是只有单音的和弦： 挂二和弦：根音加向上的两个单音：C+G+D； 挂四和弦：根音和上下的两个单音：C+G+F； 属七挂四和弦：挂四和弦再加入小七音：C+G+F+Bb，但这样需要用四根指头，通常省略五音，简化为根音加向下的两个单音：C+F+Bb。指法均为连续自然的 4+3+2。5.4.8. 减七和弦与减三和弦八度（12个半音）平均分成四份（3个半音，即小三度）：1、b3、b5(#4)、6，这四个音可以组合出减三和弦（1b3b5）、省略了五音的减七和弦（1b36）、省略了三音的减七和弦（1b56)，它们可以互相转位得到，声音是相似的。由于这种周期性，和弦内音单音的排列成周期性，和弦也成周期性，见下图。按图中任何单音或和弦都能发出相似的减和弦声音。手风琴上位于第六列可以一键实现的减和弦是省略了五音的减七和弦，而不是减三和弦。我们可以看成真正的减三和弦列向下串动了 3 排的位置。以下是可能组合出的实现方式及其指法。 方法一 减七和弦：C+C减，指法为 4+2（3+2跨度太远，是不可行的）或 4+1???； 方法二 真正的减三和弦：C+Eb减，指法为 3+2??? 3+1???； 方法三 真正的减三和弦，另一种实现方式：C+Eb减，指法为 4+2； 方法四（只有单音） C+Eb+A，指法为 2+4+3 或 2+5+3； 方法五（只有单音） C+Eb+A，指法为 4+3+2； 方法六（只有单音） C+Eb+F#，指法类似方法四； 方法七（只有单音） C+Eb+F#，指法类似方法五。 减和弦位于最下面一列，在此情况下，用拇指按键是可行的，因为不会触碰到其他键。5.4.9. 增三和弦增三和弦没有一键实现方式，由于是三和弦，所以只能用三个单音来实现。以下只讨论单音，即第一、二列如何得到增三和弦。八度（12个半音）平均分成三份（4个半音，即大三度）：1、3、b6，这三个音无论如何组合都是增三和弦，它们可以互相转位得到。由于这种周期性，和弦内音单音的排列成周期性，见下图（此图实际是大三度和增五度音程图的合成）。按图中任何三个单音都能发出增三和弦的声音。将此图与减和弦对比，可以发现它是减和弦图的错位对齐，但要求更远的键位。由于此键位严格的对称性，可以组合出很多种，但实际上只有两种模式，如下图，它们指法分别为 2+3+5、2+4+5。5.4.10. 和弦进行与转换和声的乐理知识（见此文）告诉我们，很多曲子的和声是有套路的，有一些和弦会经常用到，甚至很多曲子都有固定的和弦进行。本节的目的是理解和弦进行这一套实用的乐理在手风琴的实现，方便大家快速地用手风琴伴奏歌曲。5.4.10.1. 功能和弦的位置以 C 调为例，其各级功能和弦为 C（1）、Dm（2）、Em（3）、F（4）、G（5）、Am（6）、Bdim（7）。可以看到，在手风琴左手中分成了明显的两个区域：大和弦区（1、4、5，以 1 为中心，浅黄色），小和弦（2m、3m、6m，以 6 为中心，橙色）。如果不介意小和弦用小七和弦（2m7、3m7、6m7）而不是小三和弦的话，可以用右图实现小和弦。此时小和弦区落在大和弦区（实际上就是大和弦加上 6 音低音的斜杠和弦）。这样上下跨度变小，更容易实现。有一些视频称其为新手秘诀。对于非常小的只有三列没有第四列的琴，也可以用这个方法制造出小和弦的效果。5.4.10.2. 和弦连接在和弦转换时加入起到连接作用的单音，可以丰富左手伴奏的表现。本节总结常见的和弦转换（以 C 调为例）适合加入哪些连接音，及其在手风琴左手的实现。相差五度的和弦连接，可以用四音列（见下文）： C-G, F-C; C-F,G-C: 用四音列（大）模式1； Dm-Am, Am-Dm: 四音列（小）模式1； Em-Am, Am-Em, E-Am, Am-E,: 用四音列（上小）实际上是不容易实现的。其中 E-Am 也可以用四音列（大）。其他情况： C-Am: 176 F-G, G-F: 用半音连接 C-F: 除了1234（四音列大），还可以 1534, 15#44平时可以在乐谱、歌曲中注意听 bass 的声音，可以发现积累很多类似的和弦连接。5.5. 节奏型手风琴演奏的节奏型一般是通过排布左手的按键顺序实现的，这是本节讨论的主题。手风琴不像吉他，手风琴对于不同的和弦节奏型的实现方式会有差别，我会在每个节奏型中讨论它在不同和弦上的应用和适合的指法。本节主要讨论最常用的、应该掌握的基础节奏，更多的将在后面的风格专题中涉及。5.5.1. 柱式和弦柱式和弦是指所有和弦内音同时发声。对于可以“一键”的大三、小三、属七、减七和弦，可以直接按根音和下面的第三到六列音实现。与柱式和弦相对的是分解和弦，即各个和弦内音分批次地发声。手风琴与钢琴一样，分解和弦的方式多种多样，但仍然有很大的差异。5.5.2. 分解和弦：单音+和弦键钮手风琴上的分解和弦最方便的是组合单音键钮（第一、二列）和和弦键钮（第三到六列），这也是手风琴音乐的特色。我以 o 代表单音，x 代表和弦，以下是各种节拍典型的实现方式： 2/4 拍：|o x|o x|； 3/4 拍：|o x x|o x x|； 4/4 拍：|o x x x| 或 |o x o x|。 其他复杂的拍子的律动不容易用手风琴的左手表现，例如 6/8 拍的叙事感就难以通过手风琴这种节奏感强的乐器表现，这种节拍一般用柱式和弦等简单代替。以上 o 可以只用根音，也可以用其他和弦内音。如果用其他和弦内音与根音交替进行，就是所谓的交替低音（alternative bass）。 交替低音并不一定比只有根音效果好，学会了交替低音后，不要一股脑都用它，要根据乐曲的感觉来选择。5.6. 音阶本节介绍音阶在手风琴左手的实现。5.6.1. 四音列记忆法常见的音阶是七音音阶，它可以用四音列分解成两部分，相关知识请见这篇博文。我建议先学习四音列在左手的实现，之后在学习具体的七音音阶时，多按照“七音音阶=两个四音列连接”的思维思考，关注它们是如何构造出七音音阶，有助于记忆。常见的四音列有四种，以下是手风琴左手上的实现。5.6.2. 自然大调音阶左手的自然大调音阶如上图。最常见的是模式 1，也是一开始默认学习的模式。指法有多种： 欧洲指法（对模式 1）：42453534； 美式指法（对模式 1）：32342423。该方法优点是用不到小指，但要求指头的伸展；由于二音相对根音有两个距离相等的位置，如果把二音换到另一个位置，就是模式 2。其指法为： 欧洲指法（对模式 2）：35342423； 美式指法（对模式 2）：24232423。5.6.3. 自然小调音阶左手的自然小调音阶如上图。由于自然小调是自然大调的一个调式，可以通过轮换自然大调音阶得到它的模式。下面的模式 1、2 对应自然大调音阶的模式 1、2。 可以发现，自然大调音阶大部分根音在第二列，自然小调音阶大部分根音在第一列。如果我们仔细观察本章开头画的两张分别以第二列与第一列为中心的图，就可以看到，第二列根音周围都是不带降号的，在这里实现大调色彩的音阶方便；第一列根音周围都是带降号的，在这里实现小调色彩的音阶方便（关于音阶的色彩，参考 David Bennett 的视频：The Modes Ranked by Brightness）。其实自然小调音阶也可以根音在第二列实现，自然大调也可以在根音第一列实现，但这样的实现跨度会大很多，演奏困难且不常用，我这里也不打算涉及。5.6.4. 和声小调音阶左手的和声小调音阶如上图，指法为：54254245。5.6.5. 半音阶5.6.6. 其他调式音阶自然大调的七个调式中，已经介绍了 Ionian（自然大调）和 Aeolian（自然小调），本节是其他调式。与自然小调同理，这些调式都可以通过轮换自然大调音阶得到它的模式，它们有的根音在第二列，有的在第一列。6. 风箱风箱是左手来控制的，是手风琴的“发声器官”。推拉风箱可以理解为人的呼吸，它是手风琴表达音乐独有的元素。本章介绍与风箱有关的知识。6.0.1. 推拉风箱姿势以下是推拉风箱的姿势要领： 平稳，保持力度均匀，声音不出现晃动； 推拉风箱时，身体不跟着左右摇摆； 风箱的前后方向的水平位置不变，不要前后晃，也不要像拉伸运动一样向左后方拉。初学时只需练习“倒扇形”推拉风箱：风箱下部不开，只有上部开合，呈扇形。随着学习的深入，再练习关风箱时抬风箱。6.0.2. 风箱的转换与行程规划在风箱拉到头和推到底时。必须要转换风箱，类似于人的换气。以下是转换风箱的要领： 在音的分隔出不能在一个音处转换，会把它断掉； 要在奏完一个音，按下下一个音的同时转换风箱； 尽量保持没有转换的痕迹：转换时要迅速但平和，转换前后的力度应保持一致。转换风箱的时机是需要规划的，否则很难拉出好听的感觉。对于简单的曲子，一般以乐曲时间（小节、几拍）为单位。对于复杂的曲子（例如有跨小节音），需要做出更合理的安排。风箱行程可以在谱子中以表示“推”、“拉”的符号标记。实际情况中，我们不可能对所有曲子都规划风箱行程，在拉琴时也不能保证时时刻刻对风箱的关注，因此转换风箱主要靠感觉和经验，让其成为自然的习惯。以下几条可以帮助养成： 将其想象成自己呼吸的换气，是非常有用的； 不要过于频繁地转换风箱（开合风箱幅度过小）； 不要太随意地转换风箱，例如在乐句没结束时突然转换； 遇到风箱过剩或不够用的情况：风箱过剩时，多用一点力让风箱正好合上；风箱不够用时，省着点力气。7. 左右手的配合和钢琴一样，手风琴是左右手配合的乐器，右手负责高音的旋律，左手负责和声、低音，组合而成各种饱满的歌曲。同样地，手风琴左右手的协调是需要大量练习的。手风琴比钢琴轻松的点在于左手没有那么复杂（仅限传统低音系统），但是它需要配合风箱，这是比钢琴难的地方。练习左右手配合的方法一般是：单独练左右手，再合起来练习。速度必须要从慢逐渐加快，不能一开始就求快，这样练习效率非常低（本人亲自试验过）。" }, { "title": "Python 命令行解析参数", "url": "/posts/studynotes_Python_argparse/", "categories": "科研", "tags": "学习笔记, 技术, Python", "date": "2023-03-20 00:00:00 +0800", "snippet": "本文介绍 Python 程序的命令行参数的定义和使用方法，主要为参数解析库 argparse 、处理配置文件 YAML 的库 PyYAML 的接口。当运行的代码有多个参数并需要多次运行时，简单的处理方法是将参数放在代码里面的全局变量中，每次运行前修改变量的值。但这样需要大量的手动操作，非常费力且不够优雅；且这些参数本身在逻辑上属于程序的输入，将其与代码主体分离是更加合理的。argparse 官方文档：https://docs.python.org/zh-cn/3/library/argparse.htmlsys.argv在 Python 中，最简单的实现方式是借助 sys 库下的 argv 变量，这个在 Python 笔记中已经见过：??/??/python ??/??/prog.py arg1 arg2 ...在程序内这些参数通过 sys.argv 来接收（需要 import sys 模块）。它是一个字符串列表，存放了解释器的执行命令的所有参数。以上为例，sys.argv 的内容为 [&#39;??/??/prog.py&#39;,&#39;arg1&#39;,&#39;arg2&#39;,...]。argparse上述方法有很多缺点，例如： 无法指定参数类型：只能接受字符串输入，其他类型需要通过与字符串的类型转换函数转换； 只能实现位置参数，处理默认参数、可变参数等类型的参数非常麻烦； 没有独立健全的报错机制，需要自行处理输入格式错误； …上面的功能 argparse 库都有相当完善的接口实现了。argparse 模板如下：import argparseparser = argparse.ArgumentParser(description=&#39;prog.py help&#39;)parser.add_argument(&#39;--arg1&#39;, ...)parser.add_argument(&#39;--arg2&#39;, ...)# 调用命令行参数args = parser.parse_args()args.arg1; args.arg2...??/??/python ??/??/prog.py --arg1=?? --arg2=?? ...每调用 add_argument 方法一次，就定义了一个命令行参数，add_argument 方法的第一个参数 name 是参数名。在代码中调用的方式是用 args = parse_args() 方法获取命令行参数 args，这个 args 是一个 argparse 封装的容器类型（argparse.Namespace）（类似于 Python 字典），调用格式为 args.参数名。add_argument 函数的参数定义了关于该命令行参数的细节。指定数据类型add_argument 的 type 参数指定了对应命令行参数的数据类型，取值就是 Python 的各种类型类（不是字符串）。默认为字符串。参数列表argparse 让 .py 文件的命令行参数拥有 shell 指令那样的参数格式（见 Linux 笔记）： 位置参数：是默认的参数类型，按照 add_argument 的添加顺序； 注意这里的位置参数允许不传，此时参数为 None，如果强制要求该参数应当设置 add_argument 的 required 为 True； 允许接受（固定的）多个参数，在 add_argument 的 nargs 参数中设置数量，以列表的形式存放。 默认参数：参数的默认值在 add_argument 的 default 参数中设置； 可变参数：在 add_argument 的 nargs 参数中设置 设置为 &#39;*&#39; 或 &#39;+&#39;，它们二者的区别是是否允许 0 个参数。 命名关键字参数：必须以 参数名= 的形式传的参数，在参数名中前加 -- 或 - 来标识，分别称为长名与短名； 允许规定多个名（包括长名和短名），并列地列在 add_argument 开头的参数中即可，在 args.参数名 调用时，这个参数名取为第一个长名（如果没有长名，则取第一个短名）。 上面可能没有特别详细地解释解析命令行参数的规则，但已经够用了，完整的规则都定义在 parse.args() 方法中，请参考文档：https://docs.python.org/zh-cn/3/library/argparse.html#the-parse-args-method一般来说，除了简单的程序使用位置参数，一般常用的是命名关键字参数。子命令实现参数分组上述定义的程序的逻辑是，程序只做了一件事，这件事（命令）有一系列参数。有时候程序需要做多件事（命令），简单的方式可以，指定一个参数来选择，但是有时候命令要求的参数可能不共用甚至完全不相同。argparse 提供了利用子命令对参数隔离与分组的功能，通过“子 parser”来实现：import argparseparser = argparse.ArgumentParser(description=&#39;prog.py help&#39;)parser.add_argument(&#39;--arg1&#39;, ...)...subparsers = parser.add_subparsers(help=&#39;sub-command help&#39;)parser_cmd1 = subparsers.add_parser(&#39;cmd1&#39;, help=&#39;cmd1 help&#39;)parser_cmd1.add_argument(&#39;--cmd1_arg1&#39;, ...)...parser_cmd2 = subparsers.add_parser(&#39;cmd2&#39;, help=&#39;cmd2 help&#39;)parser_cmd2.add_argument(&#39;--cmd2_arg1&#39;, ...)...# 调用命令行参数args = parser.parse_args()args.arg1cmd1_args = parser_cmd1.parse_args()cmd1_args.cmd1_arg1...这里为该程序创建了几个子命令：cmd1,cmd2,…，它们对应于自己的 parser，该 parser 下的参数只有在打出相应的子命令时才会生效，从而实现参数隔离与分组：??/??/python ??/??/prog.py --arg1=?? cmd1 --cmd1_arg1=?? ...最外层 parser 的是全局的参数，例如上面的 --arg1。子 parser 也是 ArgumentParser 类，从而可以递归下去，形成更细分的子命令，从而形成一颗子命令树。生成帮助文档argparse 库可以自动生成如何使用 py 文件命令行参数的帮助文档，因为它自动创建了一个 -h 或 --help 命令行参数：??/??/python（解释器） ??/??/prog.py -h（或--help）调用此命令即可在命令行显示如下形式的文档：usage: test_argparser.py [-h] [--arg1 ARG1] {cmd1,cmd2} ...prog.py helppositional arguments: {cmd1,cmd2} sub-command help cmd1 cmd1 help cmd2 cmd2 helpoptional arguments: -h, --help show this help message and exit --arg1 ARG1 arg1调用子命令的 -h 道理是一样的：??/??/python（解释器） ??/??/prog.py cmd1 -h（或--help）usage: test_argparser.py cmd1 [-h] [--cmd1_arg1 CMD1_ARG1]optional arguments: -h, --help show this help message and exit --cmd1_arg1 CMD1_ARG1 cmd1_arg1_help这些帮助信息的文本定义在上面接口的参数里： ArgumentParser 的 description 参数：整个帮助文档的描述； add_argument 的 help 参数：每个参数的使用方法； add_subparsers 的 help 参数：子命令的总体使用方法； add_parser 的 help 参数：每个子命令的使用方法。帮助文档也可以通过 argparse 的 IO 接口输出为别的形式，如文本文件等，详见文档。argparse + YAML 配置文件使用 argparse 已经相当优雅，但还有一个最大的缺点，就是在参数数量特别多时，在命令行中敲一长串。笨方法可以把长命令复制出来，在调用的时候粘贴又显得不够优雅，而且在一行内打出来所有的参数也不直观。解决方案是把长串参数放在格式化的文本文件中，在调用时解析文本文件里面的参数。对于 Python，常用的这种文本文件格式是 YAML，它的语法我不在这儿写了，放一个菜鸟教程在这儿，只要知道 YAML 可以表示 Python 大部分的数据类型与结构，用的时候现查即可。YAML 对应的解析库是 PyYAML （注意不是 Python 自带的，需要安装）。这个库只是一个文本解析器，它不和命令行参数打交道，需要配合 argparse 传入 YAML 文件路径，将路径交给 PyYAML 解析参数。模板如下：import argparse, yamlparser = argparse.ArgumentParser()parser.add_argument(&#39;--cfg&#39;, ...)with open(parser.cfg) as f: config = yaml.load(f.read()) ??/??/python ??/??/test.py --cfg=(YAML文件路径)YAML 文件中的参数通过 load 方法传递到了程序的 config 变量中，它是一个字典，通过 config[&#39;参数&#39;] 的方式调用。可以看到，YAML 可以与 argparse 定义的普通参数结合，例如在 argparse 的普通参数中定义重要的、通用的，而在 YAML 中定义次要的、麻烦的。YAML 也可以定义多个。总结来看，需要根据自己的需求，选择以上三种方式。三种方式依次实现了更完善的功能，但配置和写代码的成本也依次增加。 如果只做临时用，只需要一两个简单的位置参数，选第一种； 参数类型复杂，要分类，需要完善地交给用户，选第二种； 参数类型复杂、个数极多，选第三种。" }, { "title": "我的 Songbook 项目（长期更新）", "url": "/posts/my_songbook_project/", "categories": "音乐", "tags": "乐谱, 长期更新", "date": "2023-02-17 00:00:00 +0800", "snippet": "我的 songbook 链接如下： Songbook PDF 版：https://pengxiang-wang.github.io/my-songbook/output/songbook.pdf Songbook HTML 版：https://pengxiang-wang.github.io/my-songbook/output/songbook.html GitHub 项目：https://github.com/pengxiang-wang/my-songbook我一直想拥有属于我自己的 songbook。Songbook 是将一系列歌曲的歌词和和弦进行装订在一起，形成的一个小册子，方便在非正式的演奏中翻阅。可以理解为公园里演奏乐器的老大爷带的那种手写的简谱册哈哈。它不太好用一个精炼的中文词形容，叫“歌曲集、歌本”都感觉怪怪的，所以我就叫英文好了，别怪我说话夹英文装起来了啊哈哈。既然网上能立刻搜到歌曲的歌词和和弦，为什么还要自己建一个 Songbook 呢？去网上现搜是我一开始的想法，但是很快发现自己没有任何进步。所谓进步，是指了解歌曲的含义、熟悉歌词、跟得上歌曲的框架、分析明白其和声、能在乐器上自如地实现。对一首歌，如果没有自己剖开，靠反复地听去熟悉是需要很长时间的。建立 songbook 的意义就在于此。具体来说，我的 Songbook 的每一首歌包含以下几部分： 元信息：歌手、专辑、年份、其他信息，我的喜欢程度（5星评分标准）； 歌曲的原调与选调（适合我唱的调）； 歌词及标注在上面的和弦进行。我会用两种标识，一种是与调无关的数字和弦（即罗马数字的几级几级和弦，把罗马换成阿拉伯数字），另一种是转换到原调上的和弦。标注数字和弦能更清楚地看出和弦进行的本质； 歌词解析。我之前听歌从不看歌词，不了解歌曲的内涵，也就无法把握唱出来的情感。相信借助 songbook，对自己喜欢的歌曲歌词了解起来，会给我带来一些改变； 和声分析：分析和弦进行属于什么套路，对复杂和弦进行的歌曲再作进一步分析。这一部分相当于流行音乐和声理论与配和弦方法总结这篇笔记对应的实践篇； 在（和声）乐器上的实现。我有确定把握的乐器包括吉他、手风琴，希望有一天也能加入钢琴。歌曲的和弦进行和节奏型在每种乐器都有自己的实现方法，我会把自己最喜欢的实现方式记录在此部分。总之，Songbook 就是属于自己的音乐乐园，我可以把所有感兴趣的优秀的流行歌曲收入囊中，分析它，体会它，实现成适合自己的样子。如果你愿意了解我，也可以通过 songbook 了解我的听歌兴趣。希望我在 songbook 中玩得开心。" }, { "title": "LaTeX 入门（给高数 D 的同学参考）", "url": "/posts/LaTeX_introduction/", "categories": "其他", "tags": "技术", "date": "2022-11-23 00:00:00 +0800", "snippet": "LaTeX 我很早就会了，本文目的是我作为助教引导《高等数学D》课程要求用 LaTeX 排版课程论文的同学快速入门 LaTeX。我将尽量以文科生友好的语言、尽量简单地描述 LaTeX 的概念和用法，以够写课程论文用为原则，不讲解写课程论文用不到的东西。一、LaTeX 是什么假设现在你要写一篇比较正式的文章，做成一个文件发给老师。其实你通常会分成两步来做： 先打一个草稿，这个草稿只有纯文字，就像在 Windows 的记事本或微信聊天框里打出来的那样，很丑，不能交差；高数D论文作者：XXX学号：XXX院系：XXX摘要bla, bla ...1 引言引言 bla, bla ...2 XXXbla, bla ...3 结论结论是 bla, bla ... 然后把草稿扔到 Word 里，选一选、点一点上面的调字体、字号、颜色、居中之类的按钮，就可以把不好看的纯文字变得视觉观感很好的样子，很好看，可以交差了：LaTeX 简言之，是一个排版工具，地位和 Word 类似。也就是说，你在完成第 1 步后，除了扔到 Word 里，还可以交给 LaTeX，它也能让纯文字变成想要的带格式的样子。但是 LaTeX 的用法和 Word 很不一样，它不是在软件的上面点一点，而是在你草稿的纯文本里加一些特殊的标记符号（见下图，有颜色的是标记符号，其他的是原草稿的内容），这些标记符号代表着格式（以下称加了这种标记的文本为 LaTeX 代码）。例如把“高数D论文”用 \\title{} 和 \\textbf{}括起来，就表示这一段字是标题（字体会自动变大并居中），而且要用加粗的样式。\\documentclass{ctexart}\\title{\\textbf{高数D论文}}\\author{作者：XXX \\\\ 学号：XXX \\\\ 院系：XXX}\\begin{document}\\maketitle\\begin{abstract} bla, bla ...\\end{abstract}\\section{引言}引言 bla, bla ...\\section{XXX}bla, bla ...\\section{结论}结论是 bla, bla ...\\end{document}你可以想象软件里有一个翻译官，它是认识这些 LaTeX 标记的，让它翻译一下，就变成带格式的样子：翻译官是如何区分哪些是标记、哪些是你的正文呢？其实这些标记符号都被设计成了平常不会打的东西，这就避免了冲突。看看上面加的那些标记，什么 \\title{} 呀、\\section{} 呀，你写的东西不可能里面有这些怪异的符号吧？此外，如果翻译官发现你的标记打错了（例如 \\title{} 打成 \\tile{} 或者少了一个括号），它不会自动给你纠正，没有那么智能。既然比较笨，那就谨慎一点好了（又笨又鲁莽的人会搞砸事情的！）——翻译官会立即停止当前的翻译，直接告诉你在哪里打错了，是什么原因让它翻译不出来（俗称“报错”），而不是自以为是地猜你的意思。只有你手动地找出错误并改正过来才可以继续。 这种在纯文本里加特殊的标记符号实现排版的方式可以看作一种编程语言，称为标记语言。LaTeX 是一种标记语言。此外，你现在看到的这篇文章是有格式的吧，其实也是用一种叫 MarkDown 的标记语言排版出来的。（MarkDown 比 LaTeX 语法简单很多，强烈推荐喜欢在电脑上打字记笔记的文科同学用来记笔记～）总结一下，用 LaTeX 排版就是两步： 给纯文本草稿作 LaTeX 标记； 找翻译官翻译。下面就分别介绍这两件事情，LaTeX 标记的语法和翻译 LaTeX 的软件。这两件事情搞定了，就会用 LaTeX 排版了。二、LaTeX 标记的语法LaTeX 的发明者规定了各种各样的标记，正如上面看到的，每种标记都有自己的写法，对应了不同的格式。本节就来学习这些格式。有同学此时会问，试想世上的格式有那么多，每一种格式都有一个 LaTeX 标记，难道都要像背单词一样记下来吗？天啊这 LaTeX 入门门槛好高啊。如果你这样想，你可能忘了一件事情——学 LaTeX 不是为了考试（也没有人会考 LaTeX 这种东西…），而是为了完成一个实际的排版，所以遇到不会的可以找百度呀！但前提是你得知道你要做什么，会给百度讲明白。也就是说，学习 LaTeX 语法（也适用于任何计算机技术性知识的学习，例如各种软件、各种编程语言）的正确方式应该是： 先大体了解 LaTeX 能做哪些事情，能排版出什么样的效果，但不必记忆对应的 LaTeX 标记是什么； 在实际需要时，能够回想起来这些效果，挑选自己需要的，再去百度现查它的 LaTeX 标记。实际操作遇到报错时，说明语法不对，要试图理解错误信息里面描述的内容，尝试解决，自己解决不了的，可以把错误信息复制下来问百度。（这一过程就是编程的 debug 过程，也是用 LaTeX 排版让人头疼的问题）既然这样，那是不是不用学习 LaTeX 了？也不是的，还是需要一些概念性的知识储备，否则你会为一些很基本的问题感到诧异。下面就介绍一些我觉得必要了解的： LaTeX 标记一般有如下几种形式（xxx代表标记的名称）： \\xxx{你的文字} \\begin{xxx}你的文字\\end{xxx} \\xxx{设置性的描述文字} 前两者括在你的文字身上，起到括号、选定的作用，表示施加在“你的文字”上xxx格式；第 3 个可以看成对整个文章的一种设置命令，并不针对某一部分你的文字； LaTeX 代码中，被 \\begin{document} \\end{document} 括起来的是你的正文，外面的（一般在开头）是一些对整个文章的格式设置，例如默认什么字号、行间距等，文章的题目、作者等信息也是在这里设置。最前面一般用 \\documentclass{} 规定了文档的类型，写中文文章要用 \\documentclass{ctexart}； \\section{章节题目} 用于分章节，小节为 \\subsection{小节题目}，小小节为\\subsubsection{小小节题目}； 换行必须在两段之间用一个（或多个）空行隔开： 这是第一段 这是第二段 否则翻译官会按同一段显示出来； LaTeX 有大量表示数学公式的标记，所有数学公式都能通过给数或字母加这类标记用 LaTeX 排版显示出来，例如牛顿-莱布尼茨公式 \\(\\int_a^b f(x)\\mathrm{d}x = F(b)-F(a)\\) 对应的 LaTeX 代码是 \\int_a^b f(x)\\mathrm{d}x = F(b)-F(a)。 还是那个原则，不要试图背下来数学公式的 LaTeX 标记，在用到的时候现查（不过记一些非常常用的或者找一个符号汇总表摆在旁边会提高效率）； 数学公式一定要用以下符号括起来，告诉翻译官我是数学公式，否则会按普通的文字处理（排出来不好看）或者报错： 美元符号：$ 数学公式 $ 两个美元符号：$$ 数学公式 $$ 或者 \\begin{equation} 数学公式 \\end{equation} 二者的区别是，后者会让公式单独起一行居中显示，而前者会像普通文字一样嵌入在段落中（行内显示）； 一行 LaTeX 代码中跟在 % 后面的，翻译官会直接忽略，可以在这里写注释（对代码进行解释的文字）； 到此为止可以看到，LaTeX 使用了很多特殊字符如 \\,% 作为标记的一部分，翻译官不会把这些特殊字符显示出来。如果确实需要打出这些字符（即看成普通文本），请自行搜索 LaTeX 转义字符的用法。 如果你对这几条感到疑惑，请先耐心看完本文，待掌握 LaTeX 软件的操作后，去软件里实际看看我提供的模板（在最后一节“相关资源” 里）里面 LaTeX 代码对应的显示效果。三、翻译 LaTeX 的软件翻译 LaTeX 的翻译官是一个软件，它核心的部件叫“编译器”（计算机科学习惯称翻译编程语言为“编译”），软件就是在此核心功能基础上包装一些其他的功能。从文件角度看，软件是把 .tex 格式的文件编译成 .pdf 格式的文件：tex 文件里面放 LaTeX 代码，pdf 文件就是排版出来的可以直接交的成品。 编译器有 LaTeX、pdfLaTeX、XeLaTeX 等，可以在软件里选择。写中文论文时，一般用 XeLaTeX，其他的编译器有的不认中文字符！常见的软件有 Tex Live、MiKTeX、CTeX 等，我这里推荐在线的 Overleaf，因为本地的软件安装配置起来很麻烦，而且还占用电脑空间（同学们大概率写完这篇论文就不会再用了）。下面是 Overleaf 软件的用法： 先注册一个账号，进入主界面（下图是我的账号，新账号进去应该是空白的）； Overleaf 有网盘的功能，文件放在 Overleaf 上不会丢，工作全程线上进行。它里面的文件是一个一个的项目（project），每个项目就是一篇排版项目（即文章）； 左上角新建项目（New Project），进入项目的排版界面：（点左上角的小房子可以返回主界面） 左上角的菜单（Menu）可以对在线软件做一些设置。值得注意的是 Compiler 选项，写中文论文要调成 XeLaTeX； 最左边是项目的文件列表，可以看到只有一个 tex 文件。可以把自己电脑上的文件上传到这里； 中间就是打开的选中的文件，当前左边选中了那个 tex 文件，所以你应该把 LaTeX 代码输入到这里面； 右边是软件把中间 tex 文件翻译得到的 pdf 文件的预览。注意翻译不是实时的，需要点击重新编译（Recompile）按钮才能看到最新的结果； 重新编译按钮右边的右边——下载按钮可以把 pdf 文件下载到自己电脑里。相关资源如果你想进一步了解 LaTeX 的所有知识与语法，请充分利用搜索引擎，网上有大量和我同类的博客写文章介绍 LaTeX 的各方面。系统了解的话可以参考刘海洋的《LaTeX 入门》。和 Word、PPT 一样的道理，网上也有大量现成的 LaTeX 模板（是 tex 文件），把它们 copy 到你的项目里，你要做的就只剩在 LaTeX 代码里找到对应的位置填空了。Overleaf 社区有很多用户上传模板（包括北大的同学）。我也提供一个我常用的模板，使用方法如下： 保存我发给大家的压缩包到电脑，在 Overleaf 新建项目时选择 Upload Project，上传此压缩包即可； 或者从 Overleaf 模板库中找到我的模板，点击 Open as Template，模板就作为一个项目复制到你的 Overleaf 项目列表里了。（如果编译报错，右边红红的一片，检查一下有没有像上一章说的调成 XeLaTeX 编译器）再推荐一个工具：Mathpix Snipping Tool，可以把手写的或截图里的公式自动识别为 LaTeX 代码。在你不会打数学公式的时候可以直接手写让它识别，复制到你的 LaTeX 代码中即可（记得用美元符号括起来）。祝大家写论文愉快！" }, { "title": "使用训练信息的持续学习", "url": "/posts/papernotes_continual_learning_using_training_info/", "categories": "科研", "tags": "论文笔记, 机器学习, 持续学习", "date": "2022-11-15 00:00:00 +0800", "snippet": "有一类持续学习方法的想法是从旧任务的训练过程中获取信息，存放在记忆中，作为新任务防遗忘的参考。本文统一介绍这种思路。这类方法是为了防止遗忘，属于防遗忘机制的另一种分类法。此类方法的两要素： 有哪些训练信息可以利用？ 获得的训练信息如何使用？下面依次讨论，并给出几篇论文使用的例子。训练信息我所谓的训练信息是指随训练过程得到的中间产物，而不是原始的训练数据等信息。这类思路的好处是不会带来额外的计算量，因为所用信息是从训练过程中就地取材的，训练过程是无论如何都要进行的。可以利用的训练信息通常有每步更新时的： 梯度； 前向传播得到的损失； 参数值，更新前后参数的变化（可以通过梯度的大小反映）； 其他指标的变化量； 特征； 网络的 Lipschitz 性（用 Lipschitz 常数来刻画）； 训练的 mask 重合程度； 结果性指标，如准确率等；有的会在训练过程中人为引入一些额外的过程（会带来一些额外的计算量）。一个常见的是向前看（look-ahead）的方法：向前看一步的更新，计算完相关的信息后，退回不实际实施这一步更新。这些信息一般不会全部加以利用，需要整合或筛选。整合的简单方法是简单地将各步平均，筛选可以简单地随机抽取，也可以设计一定的规则。使用方法获得的训练信息如何使用到新任务的训练中是第二个要点。以下是几种方式。 直接限制更新方向，训练信息通常是梯度； 在损失函数中构造正则项，例如用特征构造蒸馏损失； 特征可以直接作为某种训练数据； 构造其他的信息，使用此信息间接地作用新任务训练，如任务相似度。论文例子OGD正交梯度下降（OGD）是基于梯度的方法，见持续学习基础知识笔记。使用的训练信息是旧任务的梯度，用于限制新任务的更新方向——投影到垂直于旧任务梯度张成的空间。Task GroupingTask Grouping 方法原论文为Efficiently Identifying Task Groupings for Multi-Task Learning，是多任务场景，可以改造成迭代的持续学习场景。训练信息是使用旧任务的损失，引入了额外的过程，即向新任务更新方向（损失降低方向）向前看一步，以在各旧任务上损失降低的大小作为任务相似度信息，据此划分给新任务不同的任务分组影响新任务的训练。我的方法Mask 的重合程度也可以看成一个训练信息。" }, { "title": "快慢网络式持续学习", "url": "/posts/papernotes_continual_learning_fast_and_slow/", "categories": "科研", "tags": "论文笔记, 机器学习, 持续学习", "date": "2022-11-04 00:00:00 +0800", "snippet": "本文介绍快慢网络式持续学习，即构建一个两部分的网络，慢网络负责粗略特征的学习，快网络负责任务特定的细节特征的学习。它们适用于 TIL、CIL 场景不限。它们都借鉴自神经科学中的互补学习系统（complementary learning systems, CLS）理论。快慢网络一般要利用人为的规定来区分开，通常是规定训练方式，让二者的训练速度有差别：即让一个学得快，另一个学得慢。论文信息DualNet: Continual Learning, Fast and Slow 会议：NIPS 2021 作者：新加坡管理大学Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System 会议：ICLR 2022 作者：荷兰四维图新DualNet此方法应当看成重演方法，它主要的机制还是重演数据，只是将其用在了加的自监督正则项，由此附赠引入了一块附带的网络，即文章中的“慢”网络。快网络就是主网络、原来的特征提取器；慢网络是逐层附加到快网络上的，起到了逐层修改快网络特征 \\(h_l\\) 的作用（修改后的 adapted feature 为 \\(h&#39;_l\\)）。训练时，除了新任务的分类损失和常规的重演损失，还在慢网络上使用重演数据的 BarlowTwins 自监督损失进行自监督学习，即对一批重演数据作两种不同的变换 A,B，使得同一个样本变换后过完网络也尽量相似，不同的样本尽量不相似（参见之前无监督持续学习的笔记）。区分快慢网络的方法：训练方式上，慢网络用的是 look-ahead 策略，即快网络每更新 K 步，慢网络才更新一步。Learning fast, Learning slow此方法的快慢网络是独立于主网络的两套网络，且二者是并列关系。这就是一个基于知识蒸馏的重演方法，特殊在知识蒸馏参考的网络有两个——快慢网络，训练时根据每个 batch 的具体情况选择其中一者加到蒸馏损失中（选择的方法可以是实用主义的，哪个效果好选哪个），蒸馏损失用于防止遗忘。在主网络一步更新后，快慢网络也要更新，用的是一种蒸馏网络的方法：exponential moving average（EMA）。区分快慢网络的方法：快慢网络更新时，让其有一定的几率强制不更新，以不更新的几率大小来区分，快网络不更新的几率要小一些。借鉴应用到任务相似性的方法我希望从快慢网络式持续学习中得到启发，结合到基于任务相似性的方法。落脚点暂定任务分组的 mask 方法（见 Task Grouping 论文 PPT 最后一页：应用到持续学习）。快慢网络的方法应当看成一种解决遗忘的方法，它与基于任务相似性的方法其实是独立的，二者可以独立地共存于一个模型中，即在上述两个模型中引入任务相似性机制。参考思路： 在 DualNet 快慢网络上引入任务分组的 mask，由于网络分成了两部分，mask 怎么分配到两部分也要作细节设计； 在 Learning fast, Learning slow 的主网络上引入任务分组的 mask。这种结合方式比较生硬，最好是从思想上结合。快慢网络方法最重要的思想是区分网络各部分的训练速度。这一点我认为可以结合于任务相似性上：首先，不硬性地隔离参数训练（即某些参数硬性固定不更新），而是允许所有参数更新；在此逻辑基础上，把任务相似度这一信息用在训练速度上。参考思路： 相似任务分组 mask 的部分快一点，不相似的慢一点； 借鉴 Learning fast, Learning slow 的方法，网络扩张法：每个任务分组给一个网络，有多个网络而不是两个快慢网络。新加入的任务分组开辟新的网络。选择蒸馏损失时的网络的标准、更新几率都使用任务相似度。甚至可以把选择网络也改成软的：相似度与蒸馏损失项的正则化系数成正比。## 两个网络，训练信息更多在快慢网络的方法中，有至少两套网络，直观上能获得加倍的信息（如果两套网络比较独立），很适合使用这种方法。这些方法直接套用到快慢网络中是没有意义的，必须对快慢网络有所区别地获取信息，否则不如直接不分快慢网络，而构造一个参数量二倍的网络。如何有区别地获取训练信息是重点。在上面的两种快慢网络方法中，Learning fast, Learning slow 看起来是不太适合的，因为是快慢网络是跟在主网络屁股后面训练的，三个网络本质上不独立。但是，本质上将训练轨迹在折线内测多找了几个参数点（形成了月牙形区域），也是多了一些信息的，例如用在OGD，可以把快慢点连起来多一些给 Schmidt 正交化的梯度。DualNet 中，快慢网络是以参与构造损失函数的不同项区分的，所以最好利用损失这一训练信息，使用其他的会显得生硬。训练任务 t-1 时，得到的损失函数分成了两部分，一部分反映了 1,…,t-1 的信息，另一部分自监督损失反映了 1, …, t-2 的信息。自监督损失是用计算的 Barlow Twins 矩阵整合出来的，可以使用该矩阵作为训练信息，而不是自监督损失。" }, { "title": "代码学习笔记：从论文 HAT 学会写持续学习代码", "url": "/posts/codenotes_HAT/", "categories": "科研", "tags": "PyTorch, 代码笔记, 机器学习, 持续学习", "date": "2022-10-09 00:00:00 +0800", "snippet": "本文是我看一篇论文代码整理的总结。下面这篇论文（简称 HAT）是持续学习领域的一个经典工作，我曾经以此论文为样板，完整仔细地看过整个项目的代码，从而理解并开始自己实现持续学习的实验的，现将心得整理于此。这篇总结将详细介绍代码的各个细节，目的是一站式搞懂一个持续学习项目乃至深度学习项目的写法，也提供了一种阅读他人代码的思路，供刚入门该领域的同学参考。我将按照自外到内的顺序，先介绍整个工程的逻辑，到主函数，再到具体函数或类的细节，剥洋葱式地讲解。这个项目的代码有些地方写的乱或不规范，注意取其精华，掌握思想，并了解代码不合理的地方在哪。看懂这篇笔记的先修条件是掌握 Python 和 PyTorch，以及 Linux 系统的基本使用，并了解深度学习和持续学习的基础知识，请参考我的相关笔记： Python 学习笔记； PyTorch 学习笔记系列； Linux 学习笔记； 持续学习基础知识； 基于 mask 的持续学习。论文信息Overcoming Catastrophic Forgetting with Hard Attention to the Task 会议：ICML 2018 作者：西班牙巴塞罗那的大学 内容：持续学习模型 HAT，是将 mask 机制加到持续学习的第一篇论文，提出了一个很简单的、每个神经元引入一个任务 mask 的方法，并给出了训练方法，和一个解决模型容量问题的稀疏正则项，让新旧任务 mask 重合。它属于参数隔离方法，之后很多带 mask 机制的持续学习论文以此篇为基础。论文代码：https://github.com/joansj/hat工程逻辑我们从根目录开始，src 文件夹存放的是真正的代码，我们稍后讨论。根目录下的其他文件都是与代码没有直接关系的： LICENSE: 一个文本文件（可以看到没有后缀名），打开可以看到，里面的文字是在声明版权，告诉他人可以怎么用此项目、禁止怎么用（否则追究责任）。一般项目都会在根目录放一个这种名为 LICENSE 的文件，里面声明性的文字不需要自己写，去网上查查各种常见的选项（如 CC 4.0、MIT License 等），选一个合适的复制过来就好。在 Github 创建项目时有个选项可以选 license，之后会自动在根目录里创建 LICENSE 文件文本，很方便。在项目代码中大家一般是这样做，在其他例如文章、博客中也有其他方式声明版权，起到相同的作用，例如，可以看看我这个博客每篇文章结尾有一段话：“本文由作者按照 CC BY 4.0 进行授权，转载请注明”，点击就能跳转到 CC 4.0 指示的版权声明文本。 readme.md: 顾名思义就是“请先读我”，它是作者写的对项目的描述性文本，给用户看的。可以在这里写任何想跟用户说的，例如使用说明等信息。在 Github 项目首页也是默认展示这段文本（在 Github 创建项目时可以选择是否 add a README file）。众所周知在电脑上做笔记用 Markdown 格式很方便，现在的代码项目也是用它写（语法请自行学习）readme，而不是 word 之类的文件，因为技术上可以方便地接到网页上显示（例如我的博客每一篇文章都是 Markdown 格式写的）。本项目由于是一篇论文的代码，所以作者主要在这里写了论文的信息，并简要介绍了程序的安装和运行方法。 requirements.txt: 一个文本文件，列举了代码所需要的环境，放在项目根目录中，告诉别人运行此项目需要装什么第三方库。注意这个不一定是手打的或复制的，而是可以通过 Pip 或 Conda 自动生成的，此时具有一定的语法（当然有时候自动生成的太过详细反而不合适，于是只需要手打一些重要的即可。本项目就做的不太好，把其他无关的环境里的包也包含进来了）。以下是相关命令： pip freeze &amp;gt;requirements.txt 或 conda list -e &amp;gt;requirements.txt: 生成环境列表到 requirements.txt； pip install -r requirements.txt 或 conda install --file requirements.txt 或 conda create --name XXX --file requirements.txt: 安装 requirements.txt 的环境到当前环境或新建环境。 .gitignore: 一个文本文件，表示在上传到 Github 时应该忽略哪些文件（语法请自行学习），注意它用 . 开头，在 Linux 或 Mac 系统中代表隐藏文件，下同。这些文件通常是临时的、runtime 的或结果性质的非代码、非必要的文件，不需要上传占用空间让别人看到。在 Github 创建项目时可以选择是否 add .gitignore。本项目忽略上传的文件有（其中一些文件在运行之后才会出现）： logs 文件夹: 存放保存的实验结果文件（见“处理实验结果”一节）。它是非代码性质的数据文件，无需上传； dat 文件夹: 存放深度学习的数据集。它是非代码性质的数据文件，并且占用大量空间，不能上传； res 文件夹: 存放另外一些实验结果（见“处理实验结果”一节）； old、temp 文件夹: 作用未知，但看名字应该是临时文件； 其他无关紧要的文件：src/.idea 文件夹:（使用 IDE PyCharm 的配置文件）；src/__pycache__ 文件夹（Python 缓存文件）；pyc 类型文件（py 文件编译后的二进制文件）；.DS_Store 类型文件（苹果电脑的文件系统配置文件）；.png 格式的文件；两个脚本文件（src/work.sh、src/immalpha.sh），可能是作者写了试的但最终没用。 下面看代码的 src 文件夹。有四个文件起到程序入口的作用： run.py: 是最基本的主函数，负责运行一次深度学习实验； run_multi.py: 是用 run.py 改写的，负责运行多次深度学习实验； work.py：作者写的另一个可以运行多次深度学习实验的程序； run_compression.sh: 负责运行模型压缩（compression）实验的程序，见论文 4.4 节。它是 Linux 系统的 shell 脚本命令（即命令行中的单个命令组合成的打包命令）。可以看到，这里它包含了多行固定的运行 run.py 的命令行命令，当运行 run_compression.sh 时，相当于运行了里面写的这些命令。其他的是具体的模块代码： approaches 文件夹：定义各种持续学习算法，每个算法是一个 py 文件； dataloaders 文件夹：定义了数据预处理方法，每个数据集是一个 py 文件。由于持续学习数据集一般是现有数据集构造的，这里也定义了如何构造数据集； networks 文件夹：定义了神经网络结构，每个结构是一个 py 文件； plot_results.py: 可视化实验结果的工具（见“处理实验结果”一节）.本项目是先把结果存下来，再在需要时单独对其可视化。可视化与核心业务分离，这个 py 文件就是单独的可视化程序； utils.py 文件：存放各种工具函数，为了不让主要部分的代码过长，如打印函数，计算某个量等。主程序run.py 是整个项目的核心，它完成一次深度学习实验的整个流程。解析命令行参数一次深度学习实验需要指定很多东西：数据集、网络结构、学习算法、各种超参数，还有一些细节的配置如随机数的种子、输出格式等。这些信息一般是不出现在代码里的，而是作为运行程序时用户指定的参数，即命令行参数。关于如何使用 Python 命令行参数，我在这篇博文有详细讨论。run.py 定义命令行参数的部分在 9-20 行，解析命令行参数在 29-97 行。可以看到，它定义了如下 7 个命令行参数： --seed: 见“随机数种子”一节； --experiment: 解析时通过 if 语句手动对应选择选项。根据命令行选项，把 dataloaders 文件夹中的模块统一解析到名为 dataloader 的变量中，在下面统一调用； --approach: 解析时通过 if 语句手动对应选择选项。根据命令行选项，把 approaches 文件夹中的模块统一解析到名为 approach 的变量中，在下文统一调用； --nepochs: 训练轮数，是比较重要的超参数，需要用户手动指定； --lr: 学习率，是比较重要的超参数，需要用户手动指定； --parameter: 为其他超参数的预留位置（因为每个 approach 的超参数都可以有所不同），具体是解析成几个、什么超参数，要看具体 approach 的定义； --output: 指定输出结果文件名路径，见“处理实验结果”一节。深度学习流程接下来的代码对应深度学习流程： 读取数据集（99-102行）：可以看到，所有 dataloaders 中的模块都只有一个 get 函数，在这里统一调用，用于得到数据集（包括训练集、验证集、测试集，作者的处理办法是先打包成一个 data 变量，再在训练或测试时抽离出来，见125、154等行），以及每个任务有几个类、输入维度等信息（用于定义网络）； 网络结构初始化（104-107行）：可以看到，所有 networks 中的模块都只有一个 Net 类，在这里统一实例化为要训练的网络结构。实例化需要确定每个任务有几个类、输入维度等信息，来自上面数据集 get 函数的返回值； 定义学习算法（109-112行）：可以看到，所有 approaches 中的模块都只有一个 Appr 类，在这里统一实例化为学习算法。粗略阅读其代码可发现，这种 Appr 类： 不仅定义了持续学习的机制（因此实例化时需要传入持续学习有关的超参数，作者的做法是把 args 整个传进去，例如 /approaches/hat.py 中27-31行解析了 args.parameter 为 lamb 和 smax 两个超参数，用户在传 --parameters 时就知道 --parameter 代表这两个超参数）； 还把优化器和损失函数一并包进来定义，因此实例化时需要指定优化器的超参数、训练轮数等，这些都在命令行参数 args 里； 请注意，Appr 类还把网络也包进来作为实例属性了，从这里开始程序不再出现网络 net 变量； 训练（148-149行）：统一调用 Appr 类的 train 方法，它接受上面抽离出来的训练集和验证集，以及第几个任务这个信息。注意不需要传网络，它在 Appr 里面，这个训练函数本质上也是在修改更新它； 测试（152-159行）：统一调用 Appr 类的 eval 方法，它接受上面抽离出来的测试集，以及第几个任务这个信息。注意这里外层有个 u 循环，是要测试所有任务的。仍然不需要传网络。Dataloaders项目在 dataloaders 文件夹定义了数据集、预处理方法和构造持续学习任务的代码，每个数据集是一个 py 文件。每个文件都只定义了一个 get 函数，我们以持续学习经典的、较为简单的 pmnist.py（Permuted MNIST）为例来讲解。get 函数它返回如下内容： 数据集变量为 data：是一个嵌套字典，即字典的值还是字典 第一层（11行）为任务，键为任务 ID； 有一个额外的键 ‘ncla’ 存放所有任务的类数之和（80行）； 第二层（34行）为任务的元信息，包括： 任务名字 ‘name’：作者命名为 ‘pmnist-任务ID’（35行）； 类的数量 ‘ncla’：在 Permuted MNIST 中，每个任务类的数量固定为 10（36行）； 训练、验证、测试数据 ‘train’, ‘valid’, ‘test’； 第三层（39行）在数据集 ‘train’, ‘valid’, ‘test’ 里面： 输入 ‘x’：一个大 Tensor，事实上本项目输入模型的数据集并不是用的 PyTorch 的 Dataloader，作者是手动划分 batch 的，例如，见 approaches/sgd.py 的 81-82 行； 标签 ‘y’：一个大 Tensor； 每个任务有几个类 taskcla（78行）：是一个列表，对 Permuted MNIST，它固定是 [10,...,10]； 输入维度 size（13行）：直接定义为常量 [1,28,28]。从 get 函数参数可以看到，作者没有为用户提供什么选择，一个 Permuted MNIST 数据集基本是固定的，用户只能设置： 控制随机数种子的 seed 和 fixed_order：见“随机数种子”一节； pc_valid：验证集数据比例。下面来看 data 变量第三层的数据集是如何一步步构造的： 首先通过 torchvision.datasets 将原始的 MNIST 数据集下载到 dat 变量中（27-30行），再一步步解析到 data 中； 用原始数据集 dat 构造 batch=1 的 Dataloader（38行，应该是为了方便写循环），逐张图片作 permute 操作（41-43行），添加到 data 中。注意此时 data 的数据部分 ‘x’,’y’ 现在是列表； 将此时的列表数据保存下来（20-21、51-52行），以后可以直接读取（55-67行）。究其原因，是前面逐张图片的处理操作太慢了，哪怕保存读取也更节省时间； 将列表转换为可以输入到 nn.Module 的 Tensor（48-52行）； 注意上面只是分了训练集和测试集，还要从训练集 ‘train’ 中划分验证集 ‘valid’（70-73行）。注意作者在 pmnist.py 中验证集是直接复制了训练集，也就是说模型选择是按照训练集上最好的来选的。我不知道是因为懒还是别的原因，但这样是容易过拟合的，越小的数据集更是如此。其他的数据集大同小异，我简要介绍之，主要关注区别： mnist2.py：是 2 个任务的 Split MNIST 数据集。由于不涉及逐张 Permute 操作，作者也没有在中间设计保存读取； cifar.py：是 10 个任务的 Split CIFAR 数据集，前 5 个任务用 CIFAR10 数据集，每个任务有 2 个类；后 5 个任务用 CIFAR100 数据集，每个任务有 20 个类。对此数据集作者终于随机划分了训练集给验证集（79-90行），比例 pc_valid 在 get 函数参数由用户指定； mixture.py：是很多种数据集的混合，有 8 个任务，每个任务是一种数据集，分别是 CIFAR10、CIFAR100、MNIST、SVHN、FashionMNIST、TrafficSigns、Facescrub、notMNIST（不是按顺序，而是固定地随机打乱）。这里有些数据集是 torchvision.datasets 没有的，作者在下面定义了相应的数据集类（相当于自定义 Dataset 类）。下面浅看一下作者是怎么自定义数据集的（在 mixture.py）： FashionMNIST：实际上 torchvision.datasets 是有这个数据集的，可能是作者在使用其 API 时遇到了 bug，然后自己重写了一个（249-257行）； TrafficSigns、Facescrub、notMNIST：都继承自 Dataset 类，写法遵从此笔记讲的自定义方法。__init__() 函数中从本地文件读取整个数据集到 data 和 labels 变量，然后在 __getitem__() 直接索引。与 MNIST 类似，download 和 train 参数控制下载和选择训练还是测试集。下载操作需要复杂的网络通讯和纠错机制，也是打包在一个 download() 函数。通过判断 train=True 的条件语句，选择读取训练还是测试数据集。Networks项目在 networks 文件夹的代码定义了网络结构，每个模型是一个 py 文件。每个文件都只定义了一个 nn.Module 名为 Net 的类。会写这些 nn.Module 类是深度学习的基础，请参考此笔记。每个深度学习项目都大同小异，大都用到 MLP、AlexNet、ResNet 等网络，写法也差不多。我们更需要关注的是网络结构是如何适配持续学习的场景或方法的。在我看来有两点： 问题一：如何处理持续学习场景中新任务新来的类（输出头）； 问题二：对 HAT 这种 model-based 的持续学习方法，涉及修改网络结构，怎么改。我们以较简单的 MLP 为例来看作者是如何处理的，见 mlp.py 和 mlp_hat.py。对于问题一，作者是事先把所有任务的输出头都定义好（19-21行），前馈时也会输出所有输出头结果的拼接（30-32行）；而不是每来一个新任务动态地增加输出头，因为这是在做一个固定的实验，这样写代码比较方便。什么时候、什么任务用那个输出头，这些都定义在持续学习方法 approach 的训练和测试函数中。另外，即使像 Permuted MNIST 这样所有任务类别相同的数据集，也是每个任务给一个自己的输出头，而不是共用相同的输出头。对于问题二，不可避免地要对每种网络结构衍生出一个修改版本，例如本项目中 mlp.py 衍生出 mlp_hat.py，alexnet.py 衍生出 alexnet_hat.py, alexnet_pathnet.py, alexnet_progressive.py, alexnet_lfl.py 等，者都是涉及修改网络结构的 model-based 方法，在使用这些 model-based 方法时，要求使用相应的网络结构。来看一下 mlp_hat.py，它定义了 HAT 方法的网络结构，即加了 mask 的 MLP。mlp.py 提供了一个 3 个隐藏层的 MLP，每层神经元个数相同；而 mlp_hat.py 提供了 1、2、3 个隐藏层的 MLP（由 __init__ 函数的参数 nlayers 指定，观察下文代码它实际上只能取 1、2、3）。以 3 层 MLP 为例： __init__() 函数中，比普通 MLP 多了三个 efc1、efc2、efc3（20、23、26行），即在每层神经元上的 task embedding，可见它们实现为 nn.Embedding，这个类用于表示一组长度相同的模型参数（称为 embedding），第一个参数 num_embeddings 为 embedding 的个数，第二个参数 embedding_dim 为 embedding 长度。这个类一般用于词向量表示（一组词库，每个词用一个 embedding 表示），但在这里作者用于表示各个任务的 task embedding 向量，注意每个 efc1、efc2、efc3 各自都是预定义好了所有任务各层的 embedding，而不是单个任务。 有了 task embedding，通过论文中的公式(1)：乘以尺度参数 \\(s\\) 再过 gate function 即 Sigmoid，得到 mask。这个过程打包成了一个 mask 函数（65-71行）； forward 函数中，比普通 MLP 多了 mask 的步骤：在每个神经元激活后乘以 mask（53、56、59行）。注意，这个 forward 函数不仅接受输入 \\(x\\)，还包括了任务 \\(t\\)和计算 mask 用的 \\(s\\)。也就是说，模型是在这里提供接口给训练和测试时确定第几个任务这个信息的，这个 Net 类是预定义好了所有任务，然后通过 forward 函数区分任务。此外还有一些细节。有 alexnet_hat.py 和 alexnet_hat_test.py 两个，它们区别在 43-50 行：是否对 task embedding 作归一初始化，与模型压缩实验有关。另外，有些带 hat 的 Net 类写了 get_view_for 函数，它使用 torch 的拉直操作（view）将一个 mask Tensor 拉直，属于 HAT 算法的一个工具函数，写在了模型类里，在 HAT 的 train 函数调用，见下文。Approaches项目在 approaches 文件夹的代码定义了各种持续学习算法的代码，每个算法是一个 py 文件。每个文件都只定义了一个名为 Appr 的类，其中都有训练和测试函数 train、eval以及定义的优化器 self.optimizer、损失函数 self.criterion。我们先来看不加持续学习防遗忘机制的微调算法 sgd.py，理清楚基本的训练和测试流程的细节。 损失函数定义在 self.criterion 中。对于 sgd.py 这种简单的，直接在 __init__ 函数规定了是 nn.CrossEntropyLoss()，对于 hat.py 等，在类中自定义了 criterion 函数； 优化器定义在 self.optimizer 中，是用 __init__ 传入的优化器超参数 lr 等构造的（作者还多写了一层 _get_optimizer，可能是嫌定义优化器的代码太长）； train 函数是训练一个任务，的核心工作在 40 行调用 train_epoch 函数，定义在 72 行，它的任务是训练任务 t 的一个 epoch。它的流程与普通的深度学习训练过程没什么区别，唯一要注意的是它是在训练任务 t（这个信息当做函数参数传入），体现在 88 行结果截取输出头 t；其他部分代码都在做一件事——动态调整学习率，这是深度学习的训练技巧，我放在后面的章节专门讨论。 eval 函数是测试当前模型（训练了任务 t 后）在一个任务上（反映在测试集上）的准确率，注意 run.py 有个外循环测试所有任务。这个写的和普通深度学习的测试没什么差别，最后返回测试 loss 和准确率。接下来看 HAT 算法 hat.py，它是在 sgd.py 基础上改的，并且要求传入的 model 必须是 _hat 版本的： criterion 函数（196行）定义了 HAT 论文的 sparse 正则项（公式(5)）和分类损失。这个正则项需要用到旧任务 &amp;lt;t mask 的合并，存放在 self.mask_pre。为了这个正则项，损失函数 criterion 不仅接受模型输出 outputs 和真实标签 targets，还要 masks。注意 if 语句区分了第一个任务的情况； 核心的 train_epoch 函数调用了加 mask 的 HAT 版 forward 函数，并计算了上述 criterion 定义的损失，再反向传播计算梯度。在更新之前： 首先屏蔽不更新旧任务 mask 掉的参数，实现方式是梯度置 0。在 135 行，梯度乘以的 mask_back 就是旧任务 mask_pre 的反转（1-x），它在上一步就由 mask_pre 计算了出来（97-102行）； 接着应用论文 2.5 节的梯度补偿机制，将梯度乘以了一个补偿因子； 更新后再把训练好的 task embedding 统一收缩（clamp）到一个较小范围，这个在论文 2.5 节的最后一段提到。 train 函数（训练一个任务）最后包含了 task embedding 到 mask 的转换（87-90行）并计算旧任务 mask_pre（91-95行）的过程。可以看到，作者用同样的 float 类型的数据结构同时存放了 task embedding 和二元 mask：训练前是前者，训练后就用此处的代码转换成后者（因为 task embedding 再也不用，没必要存储了）； eval 函数：没有太大区别，多了对正则项的统计。除了上述算法，为了 baseline 的比较，还实现了其他与 HAT 类似的参数隔离方法如 PathNet、Progressive NN 等，也有其他方法如 EWC、IMM、LwF 等。它们的区别就在训练、测试、损失函数、优化器以及用到的相关变量，它们全都可以在一个 Appr 类写明白。注意，这些方法只有 HAT 有 _test 版本的，它的意思是正式跑的程序，可以看到里面的代码更完善，它在 __init__() 函数定义了 logs、logpath（hat_test.py 29-59行，从命令行参数 --parameter 解析），如果定义了它们，从 run.py 的最后一段可以看到，会把测试的详细结果存作处理（见“处理实验结果”一节）。此外，_test 版本还有两个 criterion 函数，它们是一样的，这是作者整理代码时的疏忽，但可以看出来他是想在 _test 版本处理更多信息的。其他细节打印调试信息作者穿插了各种调试信息在代码中。一般是用 print 语句实现，对于复杂的，为了不想让主函数过长，代码打包在了 utils.py 中 print 开头的函数来调用。我在这里按顺序整理一下作者穿插的调试信息，也可以帮助梳理总结一下上面的内容： 23-27行：打印了用户指定的命令行参数，供用户确认； 102 行：打印数据集信息和持续学习的任务信息； 107 行：打印模型信息； 110-111行：打印损失函数和优化器信息； 119、176 行：打印训练进程、时间信息； 157、166-174 行：打印每一次测试结果、测试汇总结果； 162 行：打印结果保存的信息。除了上面讲述的整个流程，代码中还有很多细节需要我们注意。它们往往是很重要的事情。处理实验结果纵观整个代码，作者把以下实验结果保存到了本地文件中： 命令行 --output 参数：它定义的路径存放的是测试准确率上三角矩阵 acc（161-163 行），t行u列表示训练完第t个任务时模型在任务u的准确率，这个是持续学习最主要的指标（见持续学习基础知识笔记）。命令行参数如果为空，作者定义了默认的文件路径（21-22 行），可以看到是用--experiment、--approach等元信息命名的，用于区分不同实验； Appr 类的 logpath 参数：只有 hat_test.py 出现，如果在 --parameter 包含这一部分，run.py 在 178 行之后把一些结果信息用 pickle 保存下来（pickle 是一个 Python 内置库，可以完整保存、还原任意 Python 变量），在需要画图的时候被 plot_results.py 还原调用。随机数种子在 run.py 对种子做了全局设定（31-33行），在代码内部也有一些局部随机数变量的种子设定，如 pmnist.py 18 行的任务顺序。使用 GPU这种规模的深度学习实验一定是用 GPU 跑的。在代码里： 34-35 行检查能不能用 GPU，不能用则强制退出； 106 行把模型放到了 GPU 上； 142-145、154-155 行把数据集放到了 GPU 上。实验细节代码中有一些细节，是深度学习实验经常要做的： 要做数据标准化：先手算了均值方差，再在构造数据集时应用 transforms.Normalize 变换，例如 pmnist.py 24-30 行；以下是代码中使用的一些调参技巧： 动态调整学习率：每一轮都在验证集上测试（调用 eval 函数）一下 loss，如果连续 lr_patience 个 epoch 验证集 loss 一直不下降，则把学习率调小一点：除以 lr_factor。 Dropout 层防止过拟合：例如 mlp.py 第 15 行； 这就是一个科研用深度学习项目的全貌。看完了代码，也能感受到其中的不足之处，例如： 调用格式不太统一； 用户可以指定的东西太少； 一些常用的参数藏得太深（例如 --parameter 的解析规则，尤其是 appr.logs），用户必须非常仔细阅读代码才知道怎么用；当然，科研用的代码以做出实验结果为目的，自己方便能看懂就行，不是产品，不需要呈现给用户，自己需要什么就写什么，挂在网上的目的只是在别人质疑的给他一个参考。这种性质也决定了作者没有必要写的更健全、完美，我们看下来也就理解其代码逻辑和思想即可，无需追究细节。" }, { "title": "论文笔记：Queried Unlabeled Data Improves and Robustifies Class-Incremental Learning", "url": "/posts/papernotes_Queried-Unlabeled-Data-Improves-and-Robustifies-Class-Incremental-Learning/", "categories": "科研", "tags": "论文笔记, 机器学习, 持续学习", "date": "2022-09-23 00:00:00 +0800", "snippet": "论文信息Queried Unlabeled Data Improves and Robustifies Class-Incremental Learning 期刊：TMLR 2022 作者：德州大学奥斯汀分校等本文在类别增量（CIL）场景的简单模型 LwF 基础上做了改进，并使用了三个机制，提升了模型的效果：无标签查询数据（QUD）、辅助分类器平衡训练、对抗样本训练。本质上持续学习的重演方法和正则化方法。本文用的几个机制其实是独立的、平行的，作者将其堆叠到持续学习场景中，有点缝合怪行为。无标签查询数据（QUD）持续学习重演方法最大的问题是受记忆容量限制，重演数据量不够导致的训练样本不均衡的问题。本文的特色是利用了外部数据库（例如 Google 图片）的数据帮助防止遗忘。查询数据（query data）是一个数据库概念，是指从数据库中按照一定的查询条件抽取的一些数据。具体来说，在 \\(t\\) 时刻从外部数据库查询大量与旧任务 \\(\\tau_1, \\cdots, \\tau_{t-1}\\) 相似的数据。查询的依据是与已经存储的极少的重演数据，拿它当作 anchor（诱饵）钓出与其相似的数据。这些 anchor 设计为每个旧任务固定的数量（由于基数小，线性增长问题不大），每个旧任务的查询数据量也是固定的。注意查询数据都是无标签的。查询有专门的算法（参考信息检索领域知识），不再介绍（本文使用 Google 图片可以使用 Google 相似图片搜索的 API）。获得了大量无标签查询数据后，最常用的是知识蒸馏或知识迁移方法，通过在损失函数中加入以下正则项： 知识蒸馏（KD）：\\(\\mathcal{L}_{\\mathrm{LwF}}\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}}\\right):=\\mathbb{E}_{\\mathbf{x} \\in \\mathcal{U}}\\left[\\mathcal{K D}\\left(\\rho\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}}, \\mathbf{x}\\right), \\rho\\left(\\hat{\\boldsymbol{\\theta}}, \\hat{\\boldsymbol{\\theta}}_{\\mathrm{c}}, \\mathbf{x}\\right)\\right)\\right]\\)，\\(\\mathcal{U}\\) 为无标签查询数据，\\(\\rho\\) 为分类器最后输出的结果（概率值），此正则项让模型在查询数据（代表了旧任务数据）上预测结果尽量向旧模型靠近； 知识迁移（KT）：\\(\\mathcal{L}_{\\mathrm{LwF}}(\\boldsymbol{\\theta}):=\\mathbb{E}_{\\mathbf{x} \\in \\mathcal{U}}[\\mathcal{F} \\mathcal{T}(\\varphi(\\boldsymbol{\\theta}, \\mathbf{x}), \\varphi(\\hat{\\boldsymbol{\\theta}}, \\mathbf{x}))]\\)。它与 KD 的差别在只是让共有网络 \\(\\varphi(\\theta)\\) 输出结果靠近，整个损失函数不会更新旧任务的输出头。这里与 LwF 类似，因为都用了知识蒸馏，但是不一样。LwF 手中只有等当前任务的数据，将其当作旧任务数据作蒸馏。这样不适合 CIL 场景，因为旧模型还没有新任务类别的输出头，无法完成新任务。关于此方法，我认为可能存在的问题：每次查询的数据是不能存下来的，而每个旧任务 anchor （即查询的依据）是固定的，所以会重复查询相同的数据，查询量也是线性增长的，其实将重演记忆的空间代价转化成了查询的时间代价。辅助分类器平衡训练这个机制也是为了解决重演的训练样本不均衡的问题。先不管查询数据，将 anchor 看作重演数据，最简单的重演方法是将新数据和重演数据混合，随机采样 batch 拿来训练。这些重演的 anchor （旧类别）占新数据（新类别）的比例是悬殊的。而有其他的采样方式可以使采样的 atch 类别是均衡的，称为 class-balanced batch，具体见论文中引述的工作。然后，并不是直接使用 class-balanced batch，随机采样的 random batch 也要用，但单独给它开一个分类头。训练时二者同等重要。这样做的目的是防止 class-balanced batch 过分突出不平衡的那部分少量的数据使其过拟合（起到了隐式的正则化的作用）。损失函数：\\[\\min _{\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}, 1}, \\boldsymbol{\\theta}_{\\mathrm{c}, 2}} \\mathbb{E}_{(\\mathbf{x}, y) \\in \\mathcal{B}_{\\mathrm{CB}}}\\left[\\mathcal{L}_{\\mathrm{CB}}\\left(f\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}, 1}, \\mathbf{x}\\right), y\\right)\\right] \\quad+\\mathbb{E}_{(\\mathbf{x}, y) \\in \\mathcal{B}_{\\mathrm{RS}}}\\left[\\mathcal{L}_{\\mathrm{RS}}\\left(f\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}, 2}, \\mathbf{x}\\right), y\\right)\\right]\\]但在测试阶段并不参与到分类结果中，即测试阶段的输出只用 class-balanced batch 对应的分类头，称为主分类器（primary classifier），random batch 对应的分类头称为辅助分类器（auxiliary classifier）。个人认为这样做训练与测试阶段不一致，合理性有待讨论，但实际上很多论文都有过这种现象，例如上次的 CAT。将此机制结合到 QUD 机制，得到了本文的 CIL-QUD 模型：最终的损失函数：\\[\\begin{aligned}\\min _{\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}, 1}, \\boldsymbol{\\theta}_{\\mathrm{c}, 2}} \\mathbb{E}_{(\\mathbf{x}, y) \\in \\mathcal{B}_{\\mathrm{CB}}}\\left[\\mathcal{L}_{\\mathrm{CB}}\\left(f\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}, 1}, \\mathbf{x}\\right), y\\right)\\right] &amp;amp;+\\mathbb{E}_{(\\mathbf{x}, y) \\in \\mathcal{B}_{\\mathrm{RS}}}\\left[\\mathcal{L}_{\\mathrm{RS}}\\left(f\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}, 2}, \\mathbf{x}\\right), y\\right)\\right] \\\\&amp;amp;+\\lambda \\cdot\\left[\\mathcal{L}_{\\mathrm{LwF}}\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}, 1}\\right)+\\mathcal{L}_{\\mathrm{LwF}}\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}, 2}\\right)\\right]\\end{aligned}\\]对抗样本训练对抗训练是用来提高模型鲁棒性的一种手段，通过设计在原数据 \\(\\mathbf{x}\\) 上的扰动 \\(\\mathbf{\\delta}\\) 得到训练样本（标签 \\(y\\) 不变），并使用扰动样本训练。一般形式为如下 Min-Max 式：\\[\\min _{\\boldsymbol{\\theta}} \\mathbb{E}_{(\\boldsymbol{X}, y) \\sim \\mathcal{D}}\\left[\\max _{\\|\\boldsymbol{\\delta}\\| \\leq \\epsilon} L\\left(f_{\\boldsymbol{\\theta}}(\\boldsymbol{X}+\\boldsymbol{\\delta}), y\\right)\\right]\\]加到上面的损失函数中，得到本文的 Robust 版模型 RCIL-QUD ：\\[\\begin{aligned}&amp;amp;\\min _{\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}, i}, \\boldsymbol{\\theta}_{\\mathrm{c}, 2}} \\mathbb{E}_{(\\mathbf{x}, y) \\in \\mathcal{B}_{\\mathrm{CB}}}\\left[\\max _{\\|\\boldsymbol{\\delta}\\|_{\\infty} \\leq \\epsilon} \\mathcal{L}_{\\mathrm{CB}}\\left(f\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}, 1}, \\mathbf{x}+\\boldsymbol{\\delta}\\right), y\\right)\\right]+\\mathbb{E}_{(\\mathbf{x}, y) \\in \\mathcal{B}_{\\mathrm{RS}}}\\left[\\max _{\\|\\boldsymbol{\\delta}\\|_{\\infty} \\leq \\epsilon} \\mathcal{L}_{\\mathrm{RS}}\\left(f\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}, 2}, \\mathbf{x}+\\boldsymbol{\\delta}\\right), y\\right)\\right]\\\\&amp;amp;+\\gamma_1 \\cdot \\mathcal{L}_{\\mathrm{LwF}}^{\\mathrm{R}}\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}}\\right)+\\gamma_2 \\cdot \\mathcal{L}_{\\mathcal{R} \\mathcal{T C}}\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}}\\right)\\end{aligned}\\]其中查询数据构造的正则项 \\(\\mathcal{L}_{\\mathrm{LwF}}^{\\mathrm{R}}\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}}\\right)\\) （注意这里把 \\(\\boldsymbol{\\theta}_{\\mathrm{c}, 1},\\boldsymbol{\\theta}_{\\mathrm{c}, 2}\\) 合为一项了），也一样分两种： Robust 版知识蒸馏（RKD）：\\(\\mathbb{E}_{\\mathbf{x} \\in \\mathcal{U}}\\left[\\max _{\\|\\boldsymbol{\\delta}\\|_{\\infty} \\leq \\epsilon} \\mathcal{K} \\mathcal{D}\\left(\\rho\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}}, \\mathbf{x}+\\boldsymbol{\\delta}\\right), \\rho\\left(\\hat{\\boldsymbol{\\theta}}, \\hat{\\boldsymbol{\\theta}}_{\\mathrm{c}}, \\mathbf{x}\\right)\\right)\\right]\\) Robust 版知识迁移（RFT）：\\(\\mathbb{E}_{\\mathbf{x} \\in \\mathcal{U}}\\left[\\max _{\\|\\delta\\|_{\\infty} \\leq \\epsilon} \\mathcal{F} \\mathcal{T}(\\varphi(\\boldsymbol{\\theta}, \\mathbf{x}+\\boldsymbol{\\delta}), \\varphi(\\hat{\\boldsymbol{\\theta}}, \\mathbf{x}))\\right]\\)最后又额外引入了一个正则项，能让无标签数据在增强鲁棒性上发挥更大作用（无标签数据已经用在了\\(\\mathcal{L}_{\\mathrm{LwF}}^{\\mathrm{R}}\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}}\\right)\\)中）：（此方法称为 TRADES，不是作者提的，引用了另一篇 SOTA 的文章 Theoretically principled trade-off between robustness and accuracy）\\[\\mathcal{L}_{\\mathcal{R} \\mathcal{T C}}\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}}\\right) =\\mathbb{E}_{\\mathbf{x} \\in \\mathcal{U}}\\left[\\max _{\\|\\boldsymbol{\\delta}\\|_{\\infty} \\leq \\epsilon} \\mathcal{K} \\mathcal{L}\\left(\\rho\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}}, \\mathbf{x}+\\boldsymbol{\\delta}\\right), \\rho\\left(\\boldsymbol{\\theta}, \\boldsymbol{\\theta}_{\\mathrm{c}}, \\mathbf{x}\\right)\\right)\\right]\\]道理很简单，无论有无标签，对抗训练都希望扰动之后预测值不变：对有标签数据，不变的是已知的预测标签 \\(y\\)，所以 \\(\\mathcal{L}(\\cdot, \\cdot)\\) 第二个位置填 \\(y\\)；对无标签数据，就填扰动前的输出了。（有标签数据也可以这样填，但是纯粹找麻烦了。）" }, { "title": "组会论文/报告列表（长期更新）", "url": "/posts/paperlist_group-meeting/", "categories": "科研", "tags": "日常管理, 长期更新", "date": "2022-09-19 00:00:00 +0800", "snippet": "这是我组组会上讨论的论文与报告列表，按照时间倒序排序。每篇论文给出以下信息： 论文链接：点击论文题目即可； 出版信息：会议、期刊、预印本等； 作者：一般不详细列举，因为复制一遍这些信息实在没什么意义，只大体写一下主要作者所在的机构。仅对感兴趣的、值得关注的作详细的标注； 组会主讲人：均为本组博士生，以字母代替； 内容简介（空着的是懒得写了…）。我的其他关于论文的博文中出现论文元信息时，也遵从上述原则。对于需要详细讲解的论文，一般不会写内容简介。2022-2023 第二学期2023-05-25DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning 会议：ECCV 2022 作者：东北大学，谷歌 主讲人：H 内容：Prompt-Tuning Few-shot learning 主讲人：L 内容：讲解几篇使用 Prompt-Tuning 的小样本学习工作的思想。2023-03-02Gradient Regularized Contrastive Learning for Continual Domain Adaptation 会议：AAAI 2021 作者：悉尼大学，香港中文大学，商汤 主讲人：W 内容：Learnable istribution Calibration for Few-Shot Class-Incremental Learning 发表：ArXiv 2022 作者：中国科学院大学，华为等 主讲人：Z 内容：小样本持续学习2022-02-23Task-Customized Self-Supervised Pre-training with Scalable Dynamic Routing 会议：AAAI 2022 作者：华为诺亚方舟实验室 主讲人：L 内容：New Insights for the Stability-Plasticity Dilemma in Online Continual Learning 会议：ICLR 2023 作者：首尔国立大学 主讲人：H 内容： 组合了不同的 Normalization 方法，BN 负责稳定性部分，LN、IN（广泛地应用于迁移学习）负责可塑性部分。 为重演样本提出了限制散开程度的损失。 2022-02-16（假期进展汇报）2022-2023 第一学期2022-11-14一个小样本任务微调的框架 主讲人：L 内容：S3C: Self-Supervised Stochastic Classifiers for Few-Shot Class-Incremental Learning 会议：ECCV 2022 作者：印度科学理工学院（班加罗尔） 主讲人：Z 内容：快慢网络式持续学习与任务相似性机制的结合 主讲人：W2022-11-07Cross-Domain Cross-Set Few-Shot Learning via Learning Compact and Aligned Representations 会议：ICLR 2022 主讲人：L 内容：Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning 会议：NIPS 2022 作者：蒙特利尔大学、微软、DeepMind、CIFAR 等 主讲人：Z 内容：On the Effectiveness of Lipschitz-Driven Rehearsal in Continual Learning 发表：NIPS 2022 作者：意大利两所不出名大学 主讲人：H2022-10-31Worst Case Matters for Few-Shot Recognition 会议：ECCV 2022 作者：南京大学，计算机软件新技术国家重点实验室 主讲人：L 内容：Do Deep Networks Transfer Invariance Across Classes? 会议：ICLR 2022 作者：斯坦福大学、宾夕法尼亚大学，Finn 组 主讲人：Z 内容：Compacting, Picking and Growing for Unforgetting Continual Learning 会议：NIPS 2019 作者：（台湾）中央研究院资讯科学研究所 主讲人：W 内容：参数隔离方法，是先训练后剪枝重新训练的 PackNet 的改进：在训练新任务时，选出旧任务参数的一部分在剪枝时也重新训练。选择哪些参数是学习了在旧任务参数上的 mask，旧任务参数是固定的，类似 Piggyback 训 mask 的方式。2022-10-24Curvature-Adaptive Meta-Learning for Fast Adaptation to Manifold Data 会议/期刊：ICCV 2021, TPAMI 2022 作者：北京理工大学计算机学院，贾云得组 主讲人：L 内容：Efficiently Identifying Task Groupings for Multi-Task Learning 会议：NIPS 2021 作者：Google、斯坦福大学，Finn 组 主讲人：W 内容：多任务学习场景的任务分组方法，基于任务相似度为任务作分组，划分模型分组训练小的多任务。任务相似度计算自训练过程的损失变化。其中任务分组、任务相似性的度量可以借鉴到持续学习上。Exemplar-free Class Incremental Learning via Discriminative and Comparable One-class Classifiers 发表：ArXiv 2022 作者：北京交通大学 主讲人：Z 内容：2022-10-17持续学习中区分高频/低频信息的想法 主讲人：H 内容：关于持续学习中区分高频/低频信息的想法Margin-Based Few-Shot Class-Incremental Learning with Class-Level Overfitting Mitigation 会议：NIPS 2022 作者：华中科技大学，北京大学 主讲人：Z 内容：通过实验发现了持续学习在每个任务上不能学得太狠，最好学个大概即可。(主题) 论文： Free Lunch for Few-shot Learning: Distribution Calibration Adaptive Distribution Calibration for Few-Shot Learning with Hierarchical Optimal Transport Powering Finetuning for Few-shot Learning: Domain-Agnostic Bias Reduction with Selected Sampling 会议/期刊： ICLR 2021 Oral, TPAMI 2022 NIPS 2022 AAAI 2022 作者： 悉尼科技大学 香港中文大学，深圳市人工智能与机器人研究院 CMU 主讲人：L 内容：基于 mask 的持续学习 论文： PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights Scalable and Order-robust Continual Learning with Additive Parameter Decomposition Supermasks in Superposition 会议： CVPR 2018 ECCV 2018 ICLR 2020 NIPS 2020 作者： 伊利诺伊大学香槟分校 伊利诺伊大学香槟分校 韩国 KAIST，AITRICS 华盛顿大学等 主讲人：W 内容：整理了持续学习加 Mask 的论文，为这一类方法总结出了一个分类体系（见持续学习笔记 的网络结构法部分）。2022-10-10Cross-Attention Multi-Scale Vision Transformer for Image Classification 会议：ICCV 2021 作者：MIT-IBM Watson AI Lab 主讲人：Z 内容：Cross-ViTTraining data-efficient image transformers &amp;amp; distillation through attention 会议：ICML 2021 作者：Facebook 主讲人：Z 内容：DeiTMeta-Baseline: Exploring Simple Meta-Learning for Few-Shot Learning 会议：ICCV 2021 作者：UC San Diego，UC Berkeley 等 主讲人：L 内容：Overcoming Catastrophic Forgetting with Hard Attention to the Task 会议：ICML 2018 作者：西班牙巴塞罗那的大学 主讲人：W 内容：持续学习模型 HAT，是将 mask 机制加到持续学习的第一篇论文，提出了一个很简单的、每个神经元引入一个任务 mask 的方法，并给出了训练方法，和一个解决模型容量问题的稀疏正则项，让新旧任务 mask 重合。它属于参数隔离方法，之后很多带 mask 机制的持续学习论文以此篇为基础。梯度操控法持续学习 论文： Orthogonal Gradient Descent for Continual Learning Continual Learning of Context-dependent Processing in Neural Networks Gradient Projection Memory for Continual Learning 会议/期刊： AISTATS 2020 Nature Machine Intelligence 2019 ICLR 2021 作者： DeepMind 中科院自动化所，类脑智能研究中心 普渡大学 主讲人：W 内容：三篇基于梯度修正的持续学习论文，是这种方法最早、最经典的工作。第一、三篇把新任务的梯度投影到垂直于旧任务子空间的方向，为了防止覆盖旧任务的知识。二者的区别在第一篇直接拿旧任务用过的梯度张成子空间，第三篇是用旧任务数据（奇异值分解出的向量）构造。第二篇工作直接归结为一个修正梯度的矩阵，对其使用 RLS 算法迭代更新。2022-09-19Meta-attention for ViT-backed Continual Learning 会议：CVPR 2022 作者：浙江大学、阿里 主讲人：Z 内容：DyTox: Transformers for Continual Learning with DYnamic TOken eXpansion 会议：CVPR 2022 作者：法国索邦大学 主讲人：Z 内容：A Multi-head Model for Continual Learning via Out-of-distribution Replay 会议：CoLLAs 2022 作者：伊利诺伊大学芝加哥分校，Bing Liu 组 主讲人：Z 内容：Channel Importance Matters in Few-Shot Image Classification 会议：ICML 2022 作者：电子科技大学，哈尔滨工业大学 主讲人：LVariational Continual Learning 会议：ICLR 2018 作者：剑桥大学 主讲人：W 内容：从贝叶斯学派角度提出了一个持续学习框架——变分持续学习（VCL），提出框架是主要的。同时提出了一个在此框架下简单的防止遗忘的机制——coreset。Improving and Understanding Variational Continual Learning 发表：ArXiv 2019 作者：剑桥大学 主讲人：W 内容：对上一篇文章在训练技巧上做了一点改进，同时讨论了 VCL 特有的现象——剪枝效应。作者认为剪枝效应对持续学习意义是很大的2021-2022 第二学期2022-04-27Modeling Label Space Interactions in Multi-label Classification using Box Embeddings 会议：ICLR 2022 (Poster) 作者：马萨诸塞大学阿默斯特分校 主讲人：L 内容：Model Behavior Preserving for Class-Incremental Learning 会议：IEEE TNNLS 2022 作者：西安交通大学 主讲人：H 内容：2022-04-20数据增强论文整理 主讲人：LContinual Learning with Recursive Gradient Optimization 会议：ICLR 2022 (Spotlight) 作者：清华大学计算机系 主讲人：W 内容：本文可以看成是加正则项的持续学习方法。Leanring a Unified Calssifier Incrementally via Rebalancing 会议：CVPR 2019 作者：中科大、香港中文大学等 主讲人：Z 内容：2022-04-13MAML is a Noisy Contrastive Learner in Classification 会议：ICLR 2022 (Poster) 作者：（台湾）国立交通大学 主讲人：L 内容：The Close Relationship Between Contrastive Learning and Meta-learning 会议：ICLR 2022 (Poster) 作者：马里兰大学 主讲人：L 内容：Representational Continuity for Unsupervised Continual Learning 会议：ICLR 2022 (Oral) 作者： 纽约大学、韩国科学院、清华大学智能产业研究院等 主讲人：W 内容：这是一篇将持续学习用在无监督场景的论文，做的实验、内容还是比较综合的：里面既涉及到比较火的无监督学习模型，也把持续学习的三大类方法中比较新提出的推广到无监督场景中。目前看挺适合入门一下无监督的持续学习。无监督学习是一般是学习表示，让无监督学习持续起来，也就是题目所述的“Representational Continuity”。2022-04-06ConFeSS: A Framework for Single Source Cross-Domain Few-Shot Learning 会议：ICLR 2022 作者：高通 AI 研究院 主讲人：L 内容：2022-03-16(CoPE) Continual Prototype Evolution: Learning Online from Non-Stationary Data Streams 会议：ICLR 2021 作者：比利时鲁汶大学 主讲人：W 内容：2021-2022 第一学期2021-12-17Context-aware Attentional Pooling (CAP) for Fine-grained Visual Classification 会议：AAAI 2021 作者：英国边山大学 主讲人：L 内容：2021-12-10BNS: Building Network Structures Dynamically for Continual Learning 会议：NIPS 2021 作者：北大数据科学中心、胡文鹏（北大数院信息系）、王选计算机研究所、Bing Liu 主讲人：Z 内容：将强化学习用于持续学习中，在每个task中训练一个agent用来决策网络结构和初始化，使其训练后能在验证集上达到最优效果，reward包含当前task和之前task，达到防止遗忘和知识迁移两个目的。训练代价大，每个task之间agent似乎没有联系，没有真正将持续学习和强化学习联系起来。Is Class-Incremental Enough for Continual Learning? 会议：Frontiers in AI 2022 作者：Andrea Cossu*, Gabriele Graffieti, Lorenzo Pellegrini, Davide Maltoni, Davide Bacciu, Antonio Carta, Vincenzo Lomonaco 主讲人：W 内容：Does Continual Learning = Catastrophic Forgetting? 发表：ArXiv 2021 作者：Anh Thai, Stefan Stojanov, Zixuan Huang, Isaac Rehg, James M. Rehg 主讲人：W 内容：2021-12-03Memory Efficient Class-Incremental Learning for Image Classification 会议：IEEE TNNLS 2021 作者：浙江大学计算机学院 主讲人：W 内容：2021-11-26IIRC: Incremental Implicitly-Refined Classification 会议：CVPR 2021 作者：蒙特利尔大学等 主讲人：Z 内容：提出了持续学习中出现不同粒度的类别，且相互关系未知。用多标签分类的指标作为评价标准，在几个经典算法上观察了实验效果，说明了粗细粒度的关系会影响分类效果。HCV: Hierarchy-Consistency Verification for Incremental Implicitly-Refined Classification 会议：BMVC 2021 作者：西班牙巴塞罗那的大学，南开大学 主讲人：Z 内容：针对IIRC问题提出了判断类别关系的方法，根据前面类别的打分确定是否为之前某一类的子类。超类和子类同时输出高分。测试时根据训练得到的层级关系调整矛盾的预测结果。2021-11-19Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning by Finding Flat Minima 会议：NIPS 2021 作者：香港科技大学 主讲人：H 内容：Intriguing Properties of Contrastive Losses 会议：NIPS 2021 作者：Google 主讲人：L 内容：2021-11-12Efficiently Identifying Task Groupings for Multi-Task Learning 会议：NIPS 2021 作者：Google、斯坦福大学，Finn 组 主讲人：Z 内容： 目的：设计高效的多任务分组方法。 方法：提出 inter-task affinity，用a任务的梯度方向观察b任务的损失函数变化情况，以此刻画任务间的相关程度。 主要结论：此算法和SOTA相比在测试准确率不降低的情况下大幅减少了计算时间。 Meta-learning with an Adaptive Task Scheduler 会议：NIPS 2021 作者：斯坦福大学，中科大，腾讯 AI Lab 等，Finn 组 主讲人：H 内容：2022-11-04Can multi-label classification networks know what they don’t know? 会议：NIPS 2021 作者：CMU 等 主讲人：L 内容：启发于基于能量的OOD判别方法，本文针对多标签分类问题基于能量模型提出一种OOD鉴别指标。2022-10-29Few-shot Open-set Recognition by Transformation Consistency 会议：CVPR 2021 作者：韩国 KAIST 主讲人：L 内容：提出一种不需要训练集中包含未知样本的小样本开放集识别的方法。这种方法基于一类通过对类别原型进行变换的小样本识别方法，利用这种变换的一致性，通过取代原型的方法比较取代前后的距离来判断是否是unseen样本。A continual learning survey: Defying forgetting in classification tasks 会议：IEEE TPAMI 2021 作者：比利时鲁汶大学，西班牙巴塞罗那的大学，华为诺亚方舟实验室等 主讲人：W 内容：2021-10-22Co2L：Contrastive Continual Learning 会议：ICCV 2021 作者：韩国 KAIST 主讲人：Z 内容： 目的：将对比学习用于持续学习中。 方法：1.将每个task的交叉熵损失换成监督型对比损失，并提出非对称损失，防止进一步分离之前见过的类（否则会出现存储样本与整体分布的偏差）。2.用instancewise relation distillation防止遗忘。 主要结论：对比损失能提取更适合在任务间迁移的特征。 Exploring Architectural Ingredients of Adversarially Robust Deep Neural Networks 会议：NIPS 2021 作者：墨尔本大学，北大人工智能学院等 主讲人：H 内容：在 WRN32-10 的下框架，探究怎样的结构有助于提高神经网络的对抗鲁棒性，发现当越靠近输出层宽度越小时，网络的对抗鲁棒性越强，当越远离输出层的网络参数越大时，对抗鲁棒性越强。2022-10-08Few-Shot Learning with Part Discovery and Augmentation from Unlabeled Images 会议：IJCAI 2021 作者：中科大，中科院计算所 主讲人：L 内容：本文解决的小样本问题场景为：大量的无标签数据可作为特征提取器的预训练数据集。核心思想就是图像中关键Part的获取。在预训练部分，选择每张图像中信息量最大的Part，利用这一Part参与对比学习从而得到下游任务需要的特征提取器。在下游任务中，先用小样本数据训练一个分类器，再用该分类器对无标签数据进行预分类，选择分类概率较高的样本作为增强数据。增强的方法即通过一个样本的attention block，意在放大与类别相关的特征。最后再用增强的数据及原小样本数据重新训练分类器。Data Augmentation for Meta-Learning 会议：ICML 2021 作者：马里兰大学 主讲人：H 内容：本文探索了在元学习的不同阶段做数据增广，观察在每个阶段做数据增广的不同表现，得到数据增广在元学习不同阶段所能起到的不同作用2022-09-17Continual Learning in the Teacher-Student Setup: Impact of Task Similarity 会议：ICML 2021 作者：帝国理工大学，牛津大学等 主讲人：Z 内容： 目的：实验观察持续学习中前后任务的相似性对结果的影响。仅涉及两层神经网络两个任务的情况。 方法：teacher-student setup，ODE数值模拟。 主要结论：intermediate task similarity leads to greatest forgetting 持续且无遗忘的深度学习方法研究 会议：博士学位论文 作者：胡文鹏（北大数院信息系） 主讲人：W 内容： 目的：通过持续学习克服灾难性遗忘问题，从表面原因（训练样本分布不均衡）与根本原因（特征偏置）下手 本次主要汇报第3部分： 1.参数生成与模型自适应方法（PGMA）：一种持续学习算法，针对表面原因 2.全面学习（HL）：解决单类别分类，用全面正则项（H-reg）实现，解决根本原因 3.全面持续学习框架：一种持续学习算法，结合H-reg，引入参数迁移、后处理机制，解决根本原因 " }, { "title": "编配：《喀秋莎》手风琴独奏", "url": "/posts/accordion_transcribed_Katyusha/", "categories": "音乐", "tags": "手风琴, 乐谱, 音乐编配", "date": "2022-09-12 00:00:00 +0800", "snippet": "乐谱已上传至 MuseScore 和 B站，有实时的声音与乐谱对照。此编配参考了众多网上的版本，主要是 YouTube 博主 amarcordeon 版本 和 B 站 tsekuiwu 版本，融合为一个不需要太花哨的技巧、左手和声简单、旋律有层次感的版本。喀秋莎旋律很简单，只有 4 句，再加每遍合唱重复后两句，共 6 句。我将其重复了 6 遍，每一遍加的东西有所不同，从简单到复杂，为了使气氛体现出越来越高昂的趋势。例如：第 1 遍什么都不加，非常朴素；第 2 遍有了装饰音；第 3,4 遍加入更密集的右手和声；第 5,6 遍升高一调，基本在高八度位置弹。为了区分每遍的后两句合唱，我也在这两句加入了更多右手和声，凸显层次感。开头部分打谱软件难以表现其速度与情感，可以参考我演奏视频里的感觉。（看看开头就好了……我演奏水平很差，视频是随便录的。注意那个视频不是按照此谱拉的，是复现 amarcordeon 的版本，勿将其作为此谱的演奏示范）乐谱PDF请扫码获取或访问：https://disk.pku.edu.cn:443/link/D5F5BEB9667011C9AAD63F6FE3409E2E，密码 3Og9。" }, { "title": "我的电子设备使用与管理方案", "url": "/posts/my_devices_solution/", "categories": "生活", "tags": "日常管理", "date": "2022-08-29 00:00:00 +0800", "snippet": "古话说一屋不扫何以扫天下，作为无产的学生，该管好的屋其实就是自己的电子设备。本文总结一下我是如何管理我电子设备的一亩三分地的，要管的方面包括 App 使用习惯和文件管理习惯。快开学了，这也算是给自己立的规矩吧，写本文的目的也是方便自己查询这些规矩，养成好习惯。我的方法可能只适合于我，不通用，分享给大家仅供参考。首先说明一下是我的配置。我下文讲的有些方案是依赖设备的（主要是 Windows 和 Mac 系统的区别），大家参考时应注意自己设备能不能替代实现。目前我手里的实体设备： MacBook Air (13 寸), 512G iPhone 12, 128G iPad Pro（11 寸）, 256G 联想拯救者 15-ISK（Windows 游戏本，15寸）, 512G（固态） + 1T（机械） 移动硬盘：2T（机械），512G（固态） 若干 U 盘常用的云存储空间： iCloud：50G（6元/月的方案） 百度网盘：理论上无限，以 1T 为单位（每个账号）从大面上说，我目前给各个设备的定位如下，没有什么特殊的： MacBook 主力工作机，完成主要的学习、科研、办公工作，完成大型项目； iPhone 接收消息提醒：主要指微信； 使用便民服务：如购物、银行、订票、打车等； 接受新闻资讯； 快速查询：用浏览器和各种专业 App； iPad 辅助学习机，方便 MacBook：主要读书、看论文、手写演算等； 当作 iPhone 的备用机：主要是接收新闻资讯； Windows 游戏本 专门打游戏用； 当作下载专用机； 当作 MacBook 的备用机，主要是利用其计算资源完成大型项目。 下面以应用场景为专题，介绍我的使用和管理方案。最后总结一下整体的 App 管理和文件管理方案。新闻资讯平时接触的信息源有很多，我对它们管理的原则是： 各平台各司其职，功能不重复。每个平台根据其特点决定怎么用它。例如看新闻就专门一个地方看，不把微博当作主要新闻获取渠道，不在多个平台关注同一个号等； 减少关注的数量，宁缺毋滥。能搜索则搜索，只对特别感兴趣的、确保以后会经常看的号关注。如果担心找不到，可以记在笔记中以供检索，而不是在平台上关注。以下是我 iPhone 上各新闻资讯类 App： 微信公众号：定位是接收官方推送。只关注自己组织内的公众号（例如 “北京大学XXX” 一类）以及一些自己特别感兴趣的、只在微信平台有号的如个人公众号、自媒体等（例如 “全员光滑”），不关注任何新闻类公众号； 知乎：1. 当作搜索引擎；2. 了解社会热点，看乐子。只关注自己特别感兴趣的实体人，不关注任何带有营销性质的号； QQ、Telegram：主要是参加一些群组，都是一些小圈子，例如游戏玩家交流群； 百度贴吧：1. 每个吧都是一些小圈子，例如各类游戏吧、公交吧；2. 了解社会热点，看乐子； Bilibili、YouTube：1. 当作视频内容搜索引擎；2. 刷视频娱乐； 新闻媒体：通过 iOS 系统将网页固定到主屏幕的功能，将所有感兴趣的媒体官网固定成 App 的样式。各家媒体的 App（如人民日报、央视新闻）及新闻聚合类 App（如今日头条）一律不装； 学校论坛：我看的有北大未名 BBS、PKU 树洞、水木社区，同上可以固定到主屏幕； 其他资讯类 App：我不把这些 App 作为每天固定获取的主力信息源，只当作社会热点的搜索引擎。包括微博、Twitter、小红书、Reddit、脉脉、小黑盒等； 查询类 App：这些 App 通常是某个专业领域的数据库，较少提供新闻资讯，我将其归为查询类 App，只在对特定内容查询时点开使用。包括：豆瓣（查书影音）、安居客（查房价）、股市（查股票）、同花顺（查股票）、网易云音乐（作为 Apple Music 的参考补充）、花伴侣（查花草）等。iPad 作为 iPhone 的备用机，除了一些极度不常用的（尤其是查询类 App），上述 App 基本都安装。MacBook 上能用网页端的，就在网页端实现（下面的浏览器书签管理就是为此服务的），电脑由于不方便，不当作获取新闻资讯的主要渠道，偶尔有空可以用电脑刷一刷。详见 App 管理总结。社交软件我的社交环境只允许用微信，QQ、Telegram、Facebook 等几乎不作联系用。本节主要探讨微信的使用与管理（不包括微信公众号）。先吐槽一下微信，众所周知微信优化极差，功能不完善，依靠的是长期垄断国内市场。对我来说，它的主要缺点有： 占用空间大：在 iOS 系统中，微信的所有聊天内容是封装在 App 中的，且聊天记录全部存储在本机上，不在服务器上。微信在本机占用空间动辄几十个 G，文件存储空间效率极低； 跨设备体验差：手机、平板、电脑几个端聊天记录不互通、不同步（登录微信时的 “同步最近的消息” 选项仅限最近，依然无法做到同步）。这个是由其本地存储导致的，当接收消息时，哪台设备开着微信，哪台才会接收。由于手机通常是一直开的，主体的聊天记录通常全部在手机上；长期以来，我电脑、平板上的微信都只当作文件传输器。注：微信在 2022 年 8 月推出了跨设备迁移功能，不同步的问题可通过迁移手动同步。 备份机制残缺：微信只支持：1. 手机的聊天记录备份到电脑，注意不是迁移到电脑端微信，而是封装在电脑上的备份文件中，无法在电脑上查看，只能通恢复到手机上查看；2. 手机的聊天记录迁移到别的手机。首先说使用习惯，目前我对自己只有一条建议：对于大型群聊，尽量只参加官方的、公务的、不得已而加的群（将兴趣群之类一律在 QQ、Telegram 上实现），这样可以显著减少占用空间。其他的随心用就行。然后说重要的备份功能。我喜欢留住旧聊天记录当作回忆，每到换手机时，直接将旧手机记录迁移过来，现手机存了从使用微信到现在的所有聊天记录，已经占用了几十 G。我希望能把记录放在电脑上，减少手机的压力，但微信的电脑备份功能是完全无法满足需要的，因为无法在电脑端查看！为此有多种第三方工具提供解决方案（例：GitHub 上的一个项目 WechatExporter），可以将聊天记录导出为 TXT、HTML、PDF 等通用格式。但是这些工具参差不齐、不稳定、不靠谱、bug 多，开发者也有跑路的风险。不过幸运的是，自从 2022 年 8 月推出跨设备迁移功能后，这个缺陷可以间接地去弥补。有了跨设备迁移，微信 iPad 端和电脑端就不是废物了。请看我的方案： 将所有聊天记录放在电脑端微信（为此请将将手机里的所有聊天记录先迁移到电脑），电脑端微信就是所有聊天记录的库，可翻看回忆； 手机当作新聊天记录的接收器； 定期迁移手机接收的新聊天记录到电脑端； 迁移是复制操作，且不会重复保存，迁移结束后可根据需要留存一些聊天记录或删除；也可以从所有聊天记录库中找出一些临时常用的记录放到手机中，手机完全可以当作小型的查看器。不过注意尽量不要留存太多，否则每次迁移都耗时很长； 定期把电脑上的聊天记录库生成为备份文件（这个 “定期” 一般不如第三条的 “定期” 频繁）。由于微信未开发电脑端微信备份功能，只能将手机端微信聊天记录备份，所以应当把电脑端聊天记录迁移回手机再备份。为了不妨碍手机端微信使用，最好另找一个手机，这时候 iPad 就派上用场了。iPad 在微信的逻辑中被视为大号手机。 iPad 平时只能当作小型的查看器，和原来一样摆烂就好了。邮件我有多个电子邮箱，曾经尝试过多种邮件客户端，包括苹果的邮件 App、Microsoft OutLook、QQ 邮箱、网易邮箱大师，发现都不好用。问题包括延迟收件、发不出去邮件、需要经常重新验证等。我的方案是： 只用网页端，将各邮箱网站固定到书签栏； 设置提醒事项，保持几天一查的频率。浏览器MacBook 端很多软件是只需网页端就能完成的，由于苹果全家桶方便，我只用 Safari 浏览器。本节讨论如何管理 Safari 浏览器。首先是标签页管理。我经常打开一大堆标签页，都能把内存给占满。有时候遇到有用的网页不舍得关，又不明确怎么留存下来。我的方法是用 Safari 浏览器的 “标签页组” 功能。这个东西和 OneTab 一个原理，就是把一些标签页临时地放到一边，不占用后台。可以建几个常用的标签页组：例如经常看课程，可以把最近在播放的课程网站归到一个组里；或者临时的标签页组：例如最近关注某一专题，搜了很多相关资料需要以后处理，可以先堆到一个组里。平时浏览网页还是随性地用，但当打开的标签页太多时及时归归类，丢到标签页组里就好了。标签页组的性质就是临时的，不够的话可以随时再建。第二是很重要的书签管理。我之前的书签分类经常混乱，最后还是沦为看到新的好网站就直接 Cmd + D 算完了，也懒得去分类了，于是越积越多。下面是我新想的一个方案，希望能有所改变：我一共分三个文件夹，它们是并列的。按这种方式分应该就不容易混乱了。 “个人收藏” 文件夹：即 Safari 浏览器默认带星星的 Favorites 文件夹。这里只放最常用的，确保日常使用随手要点开的。这个文件夹的书签可以摆在地址栏下面，起到方便的作用，所以尽量以简写命名这些书签。目前我这里的书签有： 文件夹：每日必看（BBS 树洞新闻网站之类的，视兴趣随时变）、邮箱、VPN（放现用的机场网站）、北京大学（放常用的校内网站）、Apple（Apple ID、iCloud、官网）、科研（放常用的科研网站，arXiv 之类的）、资源网站（放常用的如 Z-library、rutracker）、线上工具（放常用的如画图的 draw.io、格式转换器、LaTeX 识别器）； 书签：依次是此个人主页、Google、百度、必应、维基百科（中文）、维基百科（英文）、YouTube、B 站、GitHub、知乎、Quora、Reddit、CSDN、Telegram Web。前三个是固定的，非常常用，可以用快捷键 Cmd + 1,2,3 来打开。 “收藏的网站” 文件夹：存放平时看到的有意思的网站。已知的很大众的网站不去收藏（如微博官网、腾讯官网之类的），在 “个人收藏” 文件夹里有的也不收藏。平时感到无聊或者有探索欲时，可以来这里逛逛之前收藏的宝藏。这里也最好按分类建几个文件夹，每个人看的网站不一样，就不说了。 “收藏的文章” 文件夹：有时收藏的不是成体系的一个网站，而是寄托在某平台上的一篇文章。这个文件夹专门放收藏的文章，例如见过的一些好的 tutorial，可以收藏起来。我找到过一些挺有意思的网站，可以分享给大家，见另一篇文章：我收藏的资源网站。工作软件我目前的经常做的编辑型工作及使用的工作软件如下： 写论文：Overleaf 目前足够。主要是电脑网页端，极少数情况可用 iPad 网页端； 做幻灯片：主要指组会上讲的学术 slides，Overleaf 目前足够。主要是电脑网页端，极少数情况可用 iPad 网页端。如有其他需求可以小用 PPT、Keynote 等软件。（幻灯片这种东西用不用苹果家问题不大，因为只需要导出 PDF 分享给别人，别人也不必要求原始格式）； 做项目、写代码：VSCode 一站解决。个人只是写写科研的小代码，用这个就足够了，而且它的扩展很方便。VSCode 只能电脑端； 处理学校公务文件，分析数据：这种东西对格式有要求，一般是 doc, xls 之流，苹果家的三件套还是别用了。我在电脑端使用学校正版的 Microsoft Office。极少数情况在 iPad 上用 WPS（学校没给订 Office 365，将就用国产吧），但尽量不依赖，仅作填个小表签个字之类用途使用。另外 PDF 编辑也不太常用，电脑中装了学校正版的 Acrobat，偶尔用一用。 画图：目前使用线上工具 draw.io。 打谱：MuseScore。极少数情况需要剪视频、P 图。不是专业 up 主，电脑里白嫖了学校正版的 Premiere, Photoshop，偶尔用用。我个人没必要花小几千买 Logic Pro、Final Cut Pro 之类的软件。我需要成建制保存的一些大型项目放在一个“项目”文件夹中，这些项目包括： 深度学习科研：我主要的工作，都是大型的深度学习项目； 学习用代码：学习某项技术时使用的示例代码或者实验性质的代码； 视频剪辑项目、PS 项目； 该网站的项目目录。这些项目通常文件过大，应该放在 MacBook 存储空间上，而不是在 iCloud 中同步，因为手机、平板不需要查看这些项目文件。比较正式的代码若需要放在 Windows 游戏本上跑，最好上传到 GitHub 上（未完成的项目设为 private 即可），clone 下来使用 Git 来管理。服务器也可以这么做。图书看书是我日常重要的学习活动。这一节所谓的书不是指论文等材料，是指由出版社出版的书。由于人工智能这种专业性质，几乎所有的专业书都能在网上找到 PDF，平时看的闲书也能找到，除了 PDF 还能找到 EPUB 等格式。真的感谢大名鼎鼎的 Z-library。我读书都是使用图书 App，通常在 iPad 上看，少数情况在 Mac 上看。图书 App 类似照片 App，也是把图书文件（PDF、EPUB 等）封装到软件内部的库中。根据尽量不与软件绑定的原则，我本应直接用文件系统来看书，苹果的文件 App 自带 PDF 预览（iPad 也可以），而且也能作标记。但我还要使用图书 App 的原因是：1. 在这个软件上同步不占用 iCloud 的空间，可以白嫖（我的书有很多，占用空间有十几 G 也不小了）；2. 文件 App 不能打开 EPUB 格式。当然，图书 App 也有很多缺点：文件系统做的不好、在 iPad 不能分屏同时看两本书等等，这些缺点我都忍了。我的书不是全部放到图书 App 里的，也有类似我照片管理的二级存储机制。我把现在经常看的书导入到图书 App 的书库中，此为 “内存”；收集的不经常看的或已经看完的近期不会再翻阅的放在移动硬盘上，此为 “硬盘”。此外还有一些从资源网站获得的原始资源（例如 “xx大包”），这些文件不是一本是一本的，往往是一个大压缩包之类的，也归在 “硬盘” 部分，单独找一个文件夹 “原始资源” 存放（放在百度网盘里）。除了单独的书，还有连载的杂志之类的，例如经济学人，不适合放在图书 App 中。我在 iCloud 中开一个 “杂志” 文件夹，将其当作普通文件存放并阅读。论文论文使用文献管理软件 Zotero 来管理。文献管理软件是封装论文信息的软件，论文的元数据、PDF以及笔记、附件等内容封装在其数据库中，并通过其界面进行查阅或管理，给我的科研带来很多方便。我通过 Zotero 的插件 ZotFile 将数据库中的 PDF 文件移动到一个统一的“所有论文 (ZotFile)” 目录中（数据库中只存放指向该目录的链接），它位于 文稿/科研 目录中，可以与 iCloud 同步。这样做的目的是脱离文献管理软件的束缚（论文的 PDF 文件也属于自己的资料，不希望交给软件封装）。在电脑端可以直接在 Zotero 中访问文件；在移动端只能离开 Zotero 软件，去同步的该目录中现搜索。需要记住的关键工作流程是： 每次导入文献后，记得右键 “Manage Attachments” - “Rename and Move”； 每次删除文献后，记得删除“所有论文 (ZotFile)” 目录中对应的文件（因为软件不会自动删除，这样省下了校对文件的麻烦）；笔记电子笔记这种东西是最容易被软件绑定的。市面上的笔记软件繁多，包括 OneNote、印象笔记、Goodnotes、Notability 等等。它们虽然都能导出 PDF，但原始文件都封装于软件中，有的导不出来，即使导出来也不是通用的格式，只能用人家自己的软件打开，既要下载软件，又要担心开发商跑路，我正是希望电子化的笔记也像纸质的日记本一样，能够安心地长期保存。按照此观点来看，连用 Word 文档记笔记都不是一个好的选择。极端情况是用文本文件（如 TXT）记笔记，这样只要计算机不灭则不灭，但这种格式未免太朴素了，也只能写点日记之类的。我认为最好的方式是在笔记里面加上一些自己看得懂的、不太复杂的语法，表示一些常用的含义。这样既能以通用格式存储笔记，又增加了表达性。这种东西早就被大佬想到了，并开发出大家公认好用的 Markdown 语法。本网站的每篇文章的页面都是由一个 Markdown 文件编译成的，就是一篇笔记。我的笔记部分放在网站目录下给大家分享，不打算发出来的会放在 iCloud 的 “笔记” 文件夹中。Markdown 编辑器，在电脑上用 VSCode 即可，高亮、预览，需要的功能都能用插件实现，iPad 上非常麻烦，还基本是收费的编辑器，还是算了吧，用电脑记就行了。对于纸质笔记，我现在觉得没有必要了，主要是不能检索，其次是过度依赖软件。我曾购买过 Goodnotes，当年写纸质笔记不亦乐乎，现在就暂且弃用放在那里，当作纸制本子的一种实现吧，但实际上几乎不用了。Markdown 语法的笔记主要是一个个 md 格式的文本文件，适合坐在电脑前认真地记系统的笔记。小笔记、临时的笔记、杂事等一律用苹果的备忘录 App 来记，一些手写的随手记的东西也用它，就别上 Goodnotes 了，Goodnotes 就记一些成系统的手写笔记（几乎没有）。备忘录里的笔记很杂，要分几个文件夹： 备忘：存放要记的较长的信息。很短的目标或计划用提醒事项 App 记； 草稿纸：专门放演算纸、胡乱画的； 科研：记录一些琐碎的科研灵感、记录； 归档：放不再使用的、具有纪念意义的笔记。注意，“备忘录” 和 “快速备忘录” 文件夹是默认自带的，不能删除，就当作新笔记的收集箱好了。照片照片管理是一个非常麻烦的事，尤其是照片太多的时候，会出现存储空间不够的矛盾，不能把所有照片堆在一个地方。以下是我的方案，其实本质上类似于计算机的多级存储机制。我设计为两级的，一级是封装在照片 App 里的照片，相当于内存，它就在手边方便，但是空间不是很大；二级是存放在文件系统的照片，相当于硬盘，它空间很大，但是找起来麻烦。照片 App 封装的照片库：苹果设备拍的照片默认扔到照片 App 里（我开了 iCloud 同步，iPhone、iPad、MacBook 的照片 App 的照片库是同一个），这是一个封装的软件，照片数据在打包好的一个巨大的图库文件中（在 ~/图片/Photos Library.photoslibrary）。因为它是封装的，离开软件它就不行，所以照片 App 基本是没什么用的。照片 App 只当作临时存储拍的照片的箱子好了。这里的照片不存在管理问题（例如建所谓的相簿给照片分类），一是照片通常较少没必要管理；二是管理了也没用，因为最终要把这里面的照片并到下述文件系统的照片库。文件系统的照片库：这里存放着庞大数量的照片，我有几百 G 的照片，放在了移动硬盘上。这里就要做照片管理了，先说结论，我个人觉得还是用操作系统的文件夹系统比较靠谱。这种个人资料必须确保通用性，在任何地方、不借助任何软件、在 N 年之后都能访问，我希望的就是只用操作系统的文件管理就能访问。因此，所有需要导入到软件内部封装的照片库的软件统统不要用（包括照片 App、Lightroom、xx照片管理大师之类的）。我的方案就是只用文件夹，划几个大的文件夹（我是初中、高中、大学、研究生、家庭），每个文件夹里面再划一层（如 “高中” 文件夹下设同学与老师、运动会、风景、合影、毕业后等），照片只存放于这一层（父目录如 “家庭” 文件夹中不存照片）。两级就差不多够了，忌分级过多（归类的时候太累），我觉得这样是最简单有效的。（照片里还有元数据这种东西，我觉得太花里胡哨了，照片太多打标签太累，实在没有必要，文件夹就够了）对于文件系统的照片库，单靠操作系统还是不太方便。我推荐一个软件：Adobe 家的 Bridge（也是白嫖学校正版的）。请注意它与其他照片管理软件的最大区别是不用导入到软件内部封装的照片库，就是一个功能强大的文件浏览器（和 VSCode 是一个道理），只不过多了很多对照片友好的检索等操作，UI 也比操作系统的人性化。从照片 App 到文件系统转移照片的时机：我的习惯是视照片 App 的存储空间而定，满了就把一些照片导出到文件系统的照片库中。照片 App 里只留刚拍的比较新的照片。转移照片后理论上应该及时归类到上述的两级文件夹中。但虽然只有两级，照片归类还是很耗时间的事。如果没有时间，我选择再建一个 “未归类” 目录，暂时堆到这里面，有空的时候再整理。另外还有些平时能用到的照片，例如重要截图、收据、二维码、表情包等，为了随时方便查看、发送，但是不放在照片 App 里，里面不能建立文件夹单独管理（所谓相簿只是个标签），会产生混乱。而是放在 iCloud 云盘的 “收藏” 文件夹里，当作普通文件管理。影视这里的视频指影视作品：电影、电视剧等资源，不包括自己拍的视频，它们应该属于照片管理体系。我给自己的建议是为了方便，尽量在线上观看，包括各种美剧网站、电影网站。实在找不到线上资源的，或者特别喜欢的对画质有要求的，再通过资源网站下载（原则还是能不下则不下）。下载此类资源一般是两种途径： 百度网盘； P2P 下载：包括 BT 站、PT 站。然后是下载的视频资源管理问题。如果电脑空间足够，可以先堆在“下载”文件夹里（PT 站也方便做种，按照下载来源分目录）。文件很大，最好是全部存储在移动硬盘里，找一个“影视”文件夹堆在一起即可，这不是什么重要文件，完全不需要管理。资源的观看： 在电脑上，直接看或插着移动硬盘看就行。视频播放软件，Mac 上用 IINA，Windows 用 PotPlayer。 想用 iPad 或 iPhone 看，可以用转接头插着移动硬盘看。视频播放软件用 VLC。音乐我个人比较爱好音乐，听歌的量是很大的，收藏有几千首曲目，需要管理。我自 2019 年弃用了 QQ 音乐、网易云音乐等国产软件，转用 Apple Music。具体原因不再赘述。我的手机上还安装着网易云，偶尔查一下当前流行歌，歌单之类的，只作探索用，不在其中收藏任何歌曲，起到辅助作用。现在流量比较便宜，能线上听则线上听而不去下载，大多数音乐在 Apple Music 上是能找到的。极少数找不到的音乐，如果我特别感兴趣，需要从其他地方下载，导入到 Apple Music 资料库中。具体方法如下： 如果是无损格式，先转为 ALAC 文件（有很多线上转换工具）； 如果是有损格式，先转为 AAC 文件； 拖入 Apple Music 资料库中； 右键编辑专辑信息（相当于批量编辑歌曲信息），必要的包括专辑名称、艺人、年份、风格，为了美观最好把专辑封面传上去，可以从 QQ、网易云上搜出来保存图片； 右键编辑歌曲信息中的音轨数； 等待歌曲状态由虚线的云彩变为实心底的下载符号； 右键歌曲移除下载。本地文件随便处理删掉都行，因为此时文件已经在 Apple Music 资料库的服务器上了，从外部导入的歌曲就和 Apple Music 上有的歌曲没什么区别了。最重要的是，它和图书 App 一样，导入的东西可以同步且不占用 iCloud 的空间，又可以白嫖了。APP 管理总结大体总结一下我各个设备中的 App。我的原则是能不下 App 就不下，能用网页端就用网页端。做事尽量不被 App 绑架，能用也尽量用苹果系统原生的 App。iPhone 新闻资讯类占大头，微信、知乎、QQ、Telegram、百度贴吧、Bilibili、YouTube，放在外面；新闻媒体、其他资讯类、查询类、学校论坛这些一类的 App 各自给一个文件夹； 便民服务：带有社会服务性质的 App 全部放到一个文件夹里，包括：银行、铁路12306、携程、大众点评、滴滴等； 购物软件专门放在一个文件夹中； 不常用的系统自带工具放到一个文件夹，如翻译、计算器、语音备忘录等，第三方工具放到一个文件夹，如腾讯会议、调音软件、跑步软件等； 留一个文件夹专门放临时下的或被迫下的，例如拼多多，及时卸载； 游戏专门放到一个文件夹中。iPad和 iPhone 基本一样，按照 iPhone 的组织逻辑来即可。说一下区别。 便民服务、临时下的、购物 App 都不必在 iPad 上下载了，平时用不着； 有些探索性的 App 如 Google Map、Google 地球、星空，感兴趣就下。只在 iPad 上下载就好了，别在手机上下载；MacBook App 在启动台中分成几个屏（其实打开 App 不需要从启动台里找，用 Spotlight 打首字母即可，这里只是展示一下组织逻辑）： 第一屏：放苹果系统 App 、基本软件和小工具。包括微信、QQ、百度网盘、平时不会点开图标的，单独扔到一个 “工具” 文件夹里，如 IINA、解压软件、Cheatsheet 等；安装软件时产生的无用的图标（尤其是 Adobe）放到一个 “无用” 文件夹里； 第二屏：放办公软件，包括 Office、Adobe 系列软件等； 第三屏：放与写代码相关的软件，如 VSCode、自带的 Terminal 等； 第四屏：游戏和兴趣爱好，如 Garage Band、打谱软件等。 新闻资讯类的平台全部在浏览器书签里，在网页端看。文件管理总结我的主要存储设备为 MacBook、移动硬盘、百度网盘；iPad, iPhone, Windows 游戏本只用于安装软件或游戏，自己的存储空间目前基本不用或作临时用途。iPad, iPhone, Windows 游戏本通过 iCloud 同步 MacBook 里的东西。iCloud 中除了照片、图书、音乐、备忘录等同步外，正式文件都在 iCloud 云盘中。（原则上除了上述方案提到的苹果自带软件，本着不被软件绑架数据的原则，都不要在其他软件内置的存储里存任何文件，例如 WPS。）iCloud 云盘中包括： 文稿：主要文件的存放地。 个人：私人文件的存储位置，包括个人档案、证书、收据、简历、微信备份等； 科研：存放科研相关文件，目前只放论文，科研项目都在 “项目” 文件夹中，自己做的 slides、写的论文都放在 Overleaf 云端； 学校事务：存放学校事务中的各种通知、填表等； 课程：存放目前正在上的课的所有资料（不包括在网上自己学的，这些归到 “收藏/其他课程” 文件夹中。每个课给一个文件夹； 收藏：各种感兴趣的资料，如地图、乐谱等； 桌面：即 MacBook 的桌面，放 MacBook 正在临时处理的没经过归类整理的文件； 共享：当作中转站使用的共享文件夹全部存放于此； 待观看资源：想同步到 iPad、iPhone 上看的影视资源复制到这里，看完删除。不在 iCloud 云盘中共享的 MacBook 本地文件存放在用户目录 ~ 中，包括： 项目：上文已提及； 下载：MacBook 下载的东西（从浏览器、Motrix 等）临时放于此，下载的东西可能比较大，不要放在桌面上，桌面会通过 iCloud 同步。移动硬盘上我分了两个区： 主存储区：主要的存储区，以 Shawn 命名，文件系统用 APFS； 数据传输区：由于 APFS 无法在 Windows 系统上识别，我又不想买那些 NTFS 软件（不想被绑架），因此上面两区只能连 MacBook 访问。我为 Windows 系统专门预留了一个区，文件系统用通用的 ExFAT，以便向 Windows 系统传输文件。由于 ExFAT 不稳定，不建议在此区长期存储文件，所以只预留了 100 G，仅够文件传输用，当作大号 U 盘即可。此区以 ExFAT 命名。主存储区中存放归档的、不常用的文件或各种资源： 文件：不常用的个人、学校事务、课程、收藏； 照片：即 “照片” 一章提到的二级存储的 “硬盘” 部分； 图书：即 “图书” 一章提到的二级存储的 “硬盘” 部分； 影视：下载的影视资源； 音乐：下载的音乐资源； 项目：已完成的、不再常用的项目。百度网盘分四个文件夹： 主存储区：主要的存储区，这里是真正的自己的文件，里面的文件夹体系与移动硬盘的主存储区一样，包括文件、照片、图书等，存放更不常用的文件或资源； 外部文件：存放从别人链接里拉过来的文件，但又不想变为自己文件的； 备份：见下一节 “备份”； 共享：要通过百度网盘分享给别人的文件一律复制到在此，在此文件夹中共享。一些注意事项： 音乐书籍由于一般是乐谱，将其存储在 iCloud云盘/收藏/乐谱 中，不放在图书 App 了； 壁纸存储在 iCloud云盘/收藏/壁纸 中，其中建立一个 “正在使用的壁纸” 目录，MacBook 的壁纸选择此目录循环播放； 这套体系外的文件请自行忽视，不要去管。举例：MacBook 用户目录中的图片、音乐、影片之类的系统文件夹，iCloud 云盘、文稿文件夹里由其他软件创建的目录，移动硬盘的隐藏文件夹，百度网盘中 “我的应用数据” 等。备份备份是一个重要的事情，为防止存储设备出现意外，尤其是移动硬盘。App 的数据（除微信外）都不需要备份，大部分 App 都是账号，存放在服务器上；用 iCloud 同步的文件也无需备份。我主要讨论的是其他本地的重要文件的备份： MacBook 本地只有个项目文件夹重要，这里面的项目重要的上传到 GitHub 即可，即实现了备份（因此无需使用 Mac 系统的时间机器来备份）； 移动硬盘中的文件、照片、图书、项目文件夹比较重要，应当备份；其他资源性的如影视、音乐文件夹都不重要。对移动硬盘需要备份的文件，我没有第二块硬盘或者 NAS 服务器之类的，目前的解决方案是上传到百度网盘专门找个地方放着（“备份”文件夹），标明备份的来源，不要与其他文件混淆。我没有买百度网盘的同步服务，只能手动备份。每次备份视情况而定：如果只是新加入了一个完整的文件夹，可以手动上传此文件夹到相同位置；如果变动地方比较大（如对移动硬盘文件作了一些整理），还是乖乖地把原备份整个删除，重新备份一遍。因此，备份频率不宜过高。最好在移动硬盘的这些文件发生大的、重要变更时备份一次。" }, { "title": "爬虫项目通用架构", "url": "/posts/web-crawler-pipeline/", "categories": "有趣的事情", "tags": "技术", "date": "2022-08-14 00:00:00 +0800", "snippet": "多年之前读本科的时候买了崔庆才的《Python3 网络爬虫开发实战》，会了一点点爬虫的技术，当时就是随性写写写几十行的小代码，能随便爬点维基百科之类的。这些东西学的很零散，简直是拾起来就忘。最近又有爬虫需要了，这里有意识地关注方法论，搜了一下“爬虫架构”，不出意外，发现爬虫项目都是遵循统一的流程（或称架构），例如菜鸟教程里讲的。本文想系统地整理一下爬虫的固定流程，每个阶段负责做什么，有什么工具可供使用，以便以后爬虫时能立刻理顺逻辑；另外也提供一个通用的爬虫项目模版，以便以后可以抄抄作业。爬虫的固定流程网络爬虫遵循的固定流程可以如下叙述： 访问待爬网站，下载网页的 HTML 代码； 将需要的内容从 HTML 文本中解析出来； 将解析内容处理好后，输出或写入文件； 根据规则（如网站 URL 的命名规律）得到下一个待爬网站，重复步骤 1,2,3，直到没有待爬的网站。按照面向对象封装的思想，各部分可以由以下几个功能模块负责： URL 管理器：负责管理所有待爬的 URL，通常存放于集合中（因为不想重复爬一个网站），生成下一个待爬网站 URL 的规则在逻辑上包含在这里； HTML 下载器：将待爬 URL 的 HTML 下载下来； HTML 解析器：将需要的内容从 HTML 文本中解析出来； 数据管理器：将解析内容处理好后，存储到变量、输出或写入文件； 调度器：总指挥，规定了以上功能模块如何调度（按照什么顺序执行，数据从哪里到哪里）以实现完整的爬虫（依据上面所述的流程），有点像主函数的作用。通用爬虫模版该模版参考此文：https://cloud.tencent.com/developer/article/1423611。我将此文给出的模版作了一些简化和抽象。from urllib import responseimport requestsimport sysfrom config import *class URLManager: &#39;&#39;&#39; URL 管理器 &#39;&#39;&#39; def __init__(self): self.urls = set() # 存储已经爬过的 URL，用集合实现 def root2first(self, root_url): &#39;&#39;&#39; 从首页找到第一个待爬的 URL，找不到则返回空 &#39;&#39;&#39; if notfound: return None return first_url def next(self, url): &#39;&#39;&#39; 寻找下一个待爬的 URL，找不到则返回空 &#39;&#39;&#39; if notfound: return None self.urls.add(next_url) return next_urlclass HTMLDownload: &#39;&#39;&#39; HTML 下载器 &#39;&#39;&#39; def download(self, url): &#39;&#39;&#39; 从 URL 中下载 HTML 文本 &#39;&#39;&#39; if url is None: print(&#39;HTML下载器：URL为空，HTML未下载&#39;) return None response = requests.get(url) if response.status_code != &#39;200&#39;: print(f&#39;HTML下载器：URL&amp;lt;{url}&amp;gt;不存在，HTML未下载&#39;) return None response.encoding = &#39;utf-8&#39; html = response.text return htmlclass HTMLParser: &#39;&#39;&#39; HTML 解析器 &#39;&#39;&#39; def parse(self, html, url): &#39;&#39;&#39; 从 HTML 文本中解析出想要的内容 &#39;&#39;&#39; return dataclass DataManager: &#39;&#39;&#39; 数据管理器 &#39;&#39;&#39; def __init__(self): self.dataset = [] # 存储爬到的数据 def cleaning(self, data): &#39;&#39;&#39; 清洗爬到的数据 &#39;&#39;&#39; return data def store(self, data): &#39;&#39;&#39; 存储爬到的数据 &#39;&#39;&#39; self.dataset.append(data) def output(self, data): &#39;&#39;&#39; 输出爬到的数据 &#39;&#39;&#39; def process(self, data): data = self.cleaning(data) self.store(data) # （可选） self.output(data) # （可选）class Spider: &#39;&#39;&#39; 调度器 &#39;&#39;&#39; def __init__(self): &#39;&#39;&#39; 调度器包含以上四个功能模块：URL 管理器、HTML 下载器、解析器、数据管理器 &#39;&#39;&#39; self.manager = URLManager() self.downloader = HTMLDownload() self.parser = HTMLParser() self.datamanager = DataManager() def crawl(self, root_url): &#39;&#39;&#39; 调度完成爬虫 &#39;&#39;&#39; url = self.manager.root2first(root_url) while url: html = self.downloader.download(url) data = self.parser.parse(html, url) self.datamanager.process(data) if __name__ == &#39;__main__&#39;: spider = Spider() spider.crawl(ROOT_URL)" }, { "title": "论文笔记：Variational Continual Learning 系列", "url": "/posts/papernotes_Variational-Continual-Learning/", "categories": "科研", "tags": "论文笔记, 机器学习, 持续学习", "date": "2022-08-12 00:00:00 +0800", "snippet": "论文信息Variational Continual Learning 会议：ICLR 2018 作者：剑桥大学Improving and Understanding Variational Continual Learning 发表：ArXiv 2019 作者：剑桥大学 贝叶斯观点下的监督学习贝叶斯学派将模型参数 \\(\\theta\\) 当作随机变量。普通的贝叶斯监督学习只需要求一次后验分布（即一次推断），而对在线学习/持续学习场景，数据是分批来的，需要根据如下迭代公式多次求后验分布：\\[\\begin{align}p(\\theta\\mid D_{1:t}) &amp;amp;= p(\\theta \\mid D_{1:t-1},D_t) \\\\&amp;amp;\\propto p(\\theta \\mid D_{1:t-1})p(D_t\\mid D_{1:t-1},\\theta)\\\\&amp;amp;= p(\\theta \\mid D_{1:t-1})p(D_t\\mid \\theta)\\end{align}\\]\\[p(\\theta\\mid D_1) = p(\\theta)p(D_1\\mid \\theta)\\]\\(p(\\theta)\\) 为先验分布。其中最后一个等号是假设了 \\(D_t\\) 与 \\(D_{t-1}\\) 独立。求出后验分布后，测试阶段用推断算法作预测：\\[p\\left(y^*\\mid \\boldsymbol{x}^*, D_{1: t}\\right)=\\int q_t(\\theta) p\\left(y^* \\mid \\theta, \\boldsymbol{x}^*\\right) \\mathrm{d} \\theta\\]近似算法用迭代公式 \\(p(\\theta\\mid D_{1:t})=p(\\theta \\mid D_{1:t-1})p(D_t\\mid \\theta)\\) 直接计算后验分布是很难的，需要近似算法来计算。这里近似算法的通用框架是：引入一个 \\(q_t(\\theta)\\) 作为后验分布的近似 \\(p(\\theta\\mid D_{1:t})\\)，初始化与其相同，但迭代公式改为近似公式：\\(p(\\theta\\mid D_{1:t})=proj(p(\\theta \\mid D_{1:t-1})p(D_t\\mid \\theta))\\)，\\(proj(p)\\) 代表近似计算 \\(p\\)。不同的 \\(proj\\) 代表了不同的近似算法。作者列举了四个： Laplace 近似：用一个正态分布来近似。只需求出均值、方差两参数即可； 变分法近似：从一个（概率）函数族 \\(Q\\)中找一个最接近的作为近似。科普一下，变分问题是指泛函（函数的函数）的极值问题。 Moment Matching：是一种最优传输算法。最优传输目的是将一个普通的分布映射到另一个分布，使其传输代价最小。Moment Matching 想让分布映射到指数族分布的组合； 重要性采样：一种采样方式在在线学习中，每一步迭代都是用同一分布的一部分数据更新。以上四种对应的迭代更新算法，都有相应的工作： Laplace Propagation Online VI / Streaming Variational Bayes Assumed Density Filtering Sequential Monte Carlo本文选用的是变分法近似，变分法常用 KL 散度作为分布间相似程度的度量：\\[q_t(\\theta) = \\arg\\min_{q(\\theta)\\in Q} KL(q(\\theta)\\|\\frac1{Z_t} q_{t-1}(\\theta)p(D_t\\mid \\theta)), t = 2,\\cdots, T\\]\\(1/Z\\) 是归一化常数。这等价于训练时在最大化似然的损失函数中加入 KL 项：\\[\\mathcal{L}_t\\left(q_t(\\theta)\\right)=\\sum_{n=1}^{N_t} \\mathbb{E}_{\\theta \\sim q_t(\\theta)}\\left[\\log p\\left(y_t^{(n)} \\mid \\theta, \\mathbf{x}_t^{(n)}\\right)\\right]-KL\\left(q_t(\\theta) \\| q_{t-1}(\\theta)\\right)\\]本文中取 \\(Q\\) 为简单的正态分布的乘积族（称为 Gaussian mean-field Approximation）：\\(q_t(\\theta)=\\prod_{d=1}^D \\mathcal{N}\\left(\\theta_{t, d} ; \\mu_{t, d}, \\sigma_{t, d}^2\\right)\\)（对应地 \\(q_0(\\theta)\\) 应初始化为正态分布）。这样，泛函优化转化为对正态分布参数 \\(\\mu_{t, d}, \\sigma_{t, d}^2\\) 的优化。注意，模型参数 \\(\\theta\\) 不是优化的目标，贝叶斯方法从来不是更新 \\(\\theta\\) 的确定值，它只是分布的自变量。训练时，使用了 Monte Carlo（类似随机梯度下降）处理似然项 \\(\\sum_{n=1}^{N_t}\\) 太大的情况，也用了再参数化（reparameterization）技巧减少参数量。有空我开一篇笔记总结一下训练这种损失函数对技巧。防止遗忘的手段：Coreset持续学习与在线学习的区别是不同任务之间的数据不服从同分布假设，必须要采取防止遗忘的手段。在非贝叶斯框架下，防止遗忘的手段有重演、正则化、网络结构三种方法；在贝叶斯框架下，也需要发展出类似的手段。本文提出了一个简单的防止遗忘的方法 —— coreset，直译为核心数据集，是对数据做操作的，类似于重演数据的方法。每个任务有数据 \\(D_t\\)，也有一个 coreset \\(C_t\\)。\\(C_t\\) 需要迭代地构造出来，作者给了几种简单的方法： 随机取 \\(D_t\\) 中 K 个点加到 \\(C_{t-1}\\)； K-center 算法，确保 K 个点平摊在 \\(D_t\\) 中，面面俱到； 其他启发式算法……这里迭代求后验 \\(p(\\theta\\mid D_{1:t}/C_t)\\) 的近似，而不是 \\(p(\\theta\\mid D_{1:t})\\)。求出了 \\(p(\\theta\\mid D_{1:t}/C_t)\\) 后，可以继续算出 \\(p(\\theta\\mid D_{1:t})\\)，这才是我们要用的。\\(p(\\theta\\mid D_{1:t}/C_t)\\) 的迭代公式推导：\\[\\begin{align} p(\\theta|D_{1:t}/C_t) &amp;amp;= p(\\theta|D_{1:t-1}\\cup D_t/C_t\\cup C_{t-1}/C_{t-1})\\\\&amp;amp;=p(\\theta|D_{1:t-1}/C_{t-1} ,D_t\\cup C_{t-1}/C_t)\\\\&amp;amp;\\propto p(\\theta|D_{1:t-1} /C_{t-1})p( D_t \\cup C_{t-1}/C_t|\\theta)\\\\\\end{align}\\]以 \\(\\tilde{q}(\\theta)\\) 表示 \\(p(\\theta\\mid D_{1:t}/C_t)\\) 的近似，使用变分法近似：\\[\\tilde{q}_t(\\theta) = \\arg\\min_{q(\\theta)\\in Q} KL(q(\\theta)\\|\\frac1{Z_t} \\tilde{q}_{t-1}(\\theta)p(D_t \\cup C_{t-1}/C_t\\|\\theta)), t = 2,\\cdots, T\\]在测试时，才求出 \\(p(\\theta\\mid D_{1:t})\\)：\\[p(\\theta\\mid D_{1:t})= p(\\theta\\mid D_{1:t}/C_t\\cup C_t)=p(\\theta\\mid D_{1:t}/C_t,C_t)\\propto p(\\theta\\mid D_{1:t}/C_t)p(C_t\\mid \\theta)\\]剪枝效应论文的实验考虑了两个数据集：Split MNIST 和 Permuted MNIST，分别对应持续学习的类别增量和任务增量场景。实验将 VCL、VCL+Coreset 与其他持续学习方法对比平均准确率等指标，也对比了 Coreset 不同的大小的影响。在该团队的另一篇论文 Improving and Understanding Variational Continual Learning 中，提到了一个很有趣的事情：剪枝效应（pruning effect）——每个任务训练时会只用极少部分的神经元，剩下的神经元看起来被 prune 掉了。被 prune 掉的神经元表现出两方面： 前面连接的权重的（边缘）分布在更新时几乎不动； 后面连接的权重的（边缘）分布几乎为 0 的单点分布（密度为 delta 函数），使得它对最后结果的影响几乎为 0。具体来说，在 Split MNIST 实验（一次来两个新类）中： 选用了包含一个 200 神经元隐藏层的网络； 发现每来一个新任务，只用一个神经元； 有无 coreset 不影响剪枝效应。在 Permuted MNIST 实验（一次 10 个类都有）中： 选用了包含两个 200 神经元隐藏层的网络； 发现每来一个新任务，下层隐藏层神经元一次只用一部分神经元，而上层只用到 11 个神经元，且每个任务都用这 11 个神经元。见下图。 有无 coreset 不影响剪枝效应。对该现象的解释，作者认为这个效应是 VCL 特有的，是 “变分” 导致的，即那个 KL + 似然的损失函数导致的。作者从这个函数给出了直观的解释，没有严格推导，但我觉得很玄学，就不再说了。这个剪枝效应对持续学习是好是坏？作者倾向于认为是好，原因有二： 每次只用一小部分神经元，自动地为后面的任务预留了空间，解决了持续学习模型 “容量” 不够的问题； 天然地完成了 forward / backward transfer。作者解释这个主要在 Split MNIST 体现：假设任务 1 只用了第 1 个隐藏层神经元，任务 2 只用了第 2 个。第 1 个神经元更新输出到 2 的权重会帮助任务 2 的分类；第 2 个神经元更新输出到 1 的权重会帮助任务 1 的分类。" }, { "title": "持续学习基础知识", "url": "/posts/continual_learning/", "categories": "科研", "tags": "学习笔记, 机器学习, 持续学习", "date": "2022-07-27 00:00:00 +0800", "snippet": "目前我的研究方向是持续学习。本文汇总了持续学习的基础知识体系，可以看作一篇综述吧，希望这篇笔记能带你进入我的研究领域。本文涉及的方法都是我觉得有代表性的，只介绍思想，不会非常详细地讲细节。目录 一、相关概念 二、持续学习关心的问题 灾难性遗忘 后向迁移与前向迁移 模型容量分配问题 三、任务：分类问题 形式化定义 Baseline：多头模型 微调算法 固定特征提取器 数据集 四、持续学习的指标 总体指标 曲线图 训练过程中的学习曲线 其他 五、防遗忘机制概论 重演数据法 iCaRL 非参数分类器 重演数据 正则化法 LwF EWC 梯度操控法 网络结构法 模型扩张法：Progressive NN Mask 机制 Mask 的实现方式 Mask 如何规定训练过程与测试过程 如何构造 mask 优缺点讨论 六、前沿方向 参考资料一、相关概念持续学习（Continual Learning, CL）是多个任务的机器学习的一种学习范式。持续学习又称终身学习（Lifelong Learning）、序列学习（Sequential Learning），终身学习的概念很早就提出了，进入深度学习时代后研究者逐渐改叫持续学习；还有人把持续学习叫做增量学习（Incremental Learning）。持续学习从大面来说定义为：多个任务数据依次交付给持续学习算法学习（每次只有当前任务数据），使得最终学到的模型能够胜任所有任务。“依次”和“所有”是持续学习的核心，缺一者就与只有单任务的机器学习无差别了： 缺“依次”：每次算法能用之前所有任务数据学习，则最后一个任务时就能学习所有任务，就不“持续”了。在实际场景中，旧数据主要是由于存储限制或隐私保护等原因而不可用的。 缺“所有”：若不要求在所有任务上表现都好（例：只要求当前任务），则就是当前任务的单任务的机器学习。另外，持续学习也不允许每个任务都单独学习一个独立的模型，这样相当于多个单任务的机器学习。这种称为独立式学习（isloated learning）。（参考下面讲解的模型容量分配问题）持续学习与其他学习范式的主要区别如下， 在线学习：数据都是同一个任务来的，一定是独立同分布的；持续学习划出多个任务，数据不一定（不是一定不，但通常不是）是独立同分布的。且它的研究重点是“在线”与“离线”的区别； 迁移学习：重点关注的是“迁移”——如何利用旧任务的学习成果帮助新任务的学习； 多任务学习：数据是一次给完的，强调同时学习多个任务； 元学习：“学会学习”的角度更高，不只关注如何解决当前所有任务，还试图提取学习经验，泛化到新的任务。二、持续学习关心的问题以下先从较抽象的角度介绍持续学习关心的问题，之后再给出形式化定义和具体的例子。灾难性遗忘上面已经说过，在持续学习中，使用新任务数据训练模型使其在该新任务上效果好，是很容易做到的，只需应用成熟的单任务机器学习的算法即可；相反，很多时候学习新任务后，模型在旧任务上的效果会变差，这就是灾难性遗忘（Catastrophic Forgetting, CF），也是持续学习关心的核心问题。灾难性遗忘是神经网络模型固有的特性，实际上是不能完全解决掉的，只要用的神经网络。可以这样的简单地理解：它们的信息是记在网络参数中的，学习新任务后，就会覆盖之前的参数。但我们仍然可以缓解这一问题，方法就是引入防遗忘机制，使模型保持旧任务上的效果。设计各种防遗忘机制也是持续学习的主要目标。从“哲学”上来说，假设模型的学习能力是固定的，模型在新任务上效果好，则在旧任务上效果会变差；反之，模型保持了旧任务上的效果，则在新任务上效果就不会好。前者是模型学习新知识的能力，称为可塑性（plasticity）；后者是旧知识的记忆能力，称为稳定性（stability）。可塑性与稳定性是内在相互矛盾的，术语叫可塑性-稳定性困境（Stability-Plasticity Dilemma），这是机器学习的一个天然的哲学约束，类似于 “没有免费午餐定理”。持续学习的目标是在所有任务上表现都好，即同时追求可塑性和稳定性；但这个困境说明了实现这一目标没有捷径，持续学习场景不是伪命题，并不是无脑加防遗忘机制、加强防遗忘的力度（例如调大防遗忘正则项超参数）就可以了，必须切实地提高模型的真本领。后向迁移与前向迁移除了灾难性遗忘作为核心问题，持续学习还关心算法是否具备： 后向迁移（backward transfer）能力：学习后面的任务时，能否帮助到前面的任务； 前向迁移（forward transfer）能力：学习前面的任务时，能否帮助到后面的任务。 请注意用词：在后向迁移和前向迁移术语中，“前”是指旧任务方向，“后”是指新任务方向。而我平时习惯说“后”是指新任务。拥有后向迁移能力是比克服灾难性遗忘还要厉害的事情。克服灾难性遗忘仅仅是学习后面的任务不会给前面的任务帮倒忙，而后向迁移还要求能帮正忙。按照我的理解，后向迁移能力与灾难性遗忘是同一种能力的两种程度。拥有前向迁移能力意味着，在还没有见到要学习的任务时，就已经在积累该任务的知识，并且在训练该任务时用到。由于没有该任务的信息，这种能力也是最难拥有的，目前持续学习的研究基本无法触碰这个话题。模型容量分配问题持续学习的一大特点是学习任务的类型和数量没有预定义。在学习每个任务的期间，永远不知道未来有多少个任务、它们是什么样子的。对于之前所说的“独立式学习”（每个任务学习一个独立的模型，其模型大小随任务量线性地增加），模型尝试学习、记下每个任务所有的知识，对应的算法也是与普通机器学习没有差别，是持续学习不允许的。我们不希望模型大小无序地膨胀，而是固定模型容量（capacity），让算法在固定容量的模型下完成持续学习（偶尔也会允许少量的膨胀）。（这里所说的模型容量更多的是一个抽象概念，指模型的表示能力。当然，对于深度学习，模型的表示能力也与参数量成正相关。）很显然，固定容量的模型，随着任务越来越多，模型也不能容纳所有的知识，会出现模型容量饱和（capacity saturation）问题。知识必须有所舍弃，各任务上的效果也会打折扣，遗忘也就越严重。由于深度学习基于的是参数化的神经网络模型，这个问题是不可能解决的，因为参数是会被覆写的，模型的表示能力是有限的。目前持续学习的研究致力于缓解这个问题，而不是彻底解决它。一个好的持续学习算法能让模型尽量记住任务重要的知识，在需要舍弃知识时舍弃不重要的，减少模型表示上的重叠或冗余，从而减缓遗忘的速度。在持续学习中，每个任务会占据模型的一部分容量，任务之间也会共用部分容量（根据任务相似的程度）。但是如果不加限制，每个任务学习后就会很自然地占满所有模型容量，这样不仅容易导致任务的过拟合（因为通常适合持续学习多个任务的模型要比适合某个任务的模型要大很多），也让后面的（与该任务不太相似的）任务无处占据模型容量，导致后面的任务效果都变差。所以，需要在算法中加入一些稀疏化（sparsity）机制来解决模型容量不够的问题。我们在下面的参数隔离方法中可以显式地看到这方面的研究。三、任务：分类问题持续学习也分监督学习、无监督学习等，也有判别模型、生成模型。目前大家研究最多的是监督学习，且更多地关心分类问题。本文只讨论分类问题。在分类问题中，持续学习场景按照如下两个分类维度划分： 任务类别是否相同：有所有任务完全相同的、完全不相同的（每个任务来的都是新类），也有既有相同也有不相同的； 数据是否包含任务 ID：任务 ID 是指数据来自第几个任务的“几”。分训练数据与测试数据讨论： 训练数据与测试数据都有任务 ID 信息：在测试时知道数据来自第几个任务； 训练数据有任务 ID 信息，测试数据没有：在测试时不知道数据来自第几个任务，这样需要算法能够自行显式或隐式地判断任务 ID，更加困难； 训练数据与测试数据都没有任务 ID 信息：没有任务 ID 信息就是没有明确的任务边界（task boundary），数据随任务是流式地进入算法的。 根据此分类，某些常见场景有习惯性叫法： 类别增量学习（Class Incremental Learning, CIL）：每个任务包含若干不重复的类别，训练数据有而测试数据没有任务 ID 信息； 任务增量学习（Task Incremental Learning, TIL）：训练数据与测试数据都有任务 ID 信息，对任务类别是否相同无要求； 领域增量学习（Domain Incremental Learning, DIL）：从形式上看，只有“每个任务包含的类别相同”一个要求。但它强调的是每个任务的具体数据集中数据输入 \\(\\mathbf{x}\\) 领域的不同。形式化定义下面给出几个场景的形式化定义。设有任务 \\(t=1,2,\\cdots\\)，每个任务的数据集为 \\(\\mathcal{D}^{(t)}\\)，其中 \\(\\mathcal{D}^{(t)} =\\{(\\mathbf{x}_i,y_i)\\}_{i=1}^{N_t} \\in (\\mathcal{X}^{(t)},\\mathcal{Y}^{(t)})\\)。算法在每个时刻 \\(t\\) 利用 \\(\\mathcal{D}^{(t)}\\) 将 \\(f^{(t-1)}\\) 更新 \\(f^{(t)}\\)，希望 \\(f^{(t)}\\) 能完成目前涉及到的所有分类任务，即输入 \\(\\mathbf{x} \\in \\mathcal{X}^{(1)}\\cup\\cdots\\cup\\mathcal{X}^{(t)}\\)，输出所有涉及到的类别 \\(\\hat{y} \\in \\mathcal{Y}^{(1)}\\cup \\cdots \\cup \\mathcal{Y}^{(t)}\\)。 类别增量学习：\\(\\mathcal{Y}^{(t)}\\) 之间互不相交。可以记 \\(\\mathcal{Y}_1 = \\{C_1,\\cdots,C_{k_1}\\}, \\mathcal{Y}_2 = \\{C_{k_1 + 1}, \\cdots, C_{k_2}\\}, \\cdots\\)； 任务增量学习：知道了输入的任务 ID \\(t_{\\mathbf{x}}\\)，即目标简化为输入 \\(\\mathbf{x}\\in \\mathcal{X}^{(t_\\mathbf{x})}\\)，输出 \\(\\hat{y} \\in \\mathcal{Y}^{(t_\\mathbf{x})}\\)； 领域增量学习：\\(\\mathcal{Y}^{(1)}=\\cdots=\\mathcal{Y}^{(t)}\\)，但强调 \\(\\mathcal{X}^{(1)},\\cdots,\\mathcal{X}^{(t)}\\) 的不同。一定要强调上面加粗的“所有”二字，这对理解 TIL 与 CIL 的区别非常关键。对于 CIL，很多人的误区是以为任务 \\(t\\) 训练时只在 \\(\\mathcal{Y}^{(t)}\\) 中分类就可以了，而事实是在 \\(\\mathcal{Y}^{(1)}\\cup\\cdots\\cup\\mathcal{Y}^{(t)}\\) 中分类（就是下图多头模型有无灰色箭头的区别），也就是说训练任务越来越难，成包含关系。所以 CIL 场景是比 TIL 场景要困难不少的。注意点： 之前说过，持续学习过程中永远不知道之后有多少个任务。但在实际实验中，持续学习数据集是固定的，总任务数是固定的 \\(T\\) 个（也为了计算指标方便），以上 \\(t=1,\\cdots,T\\)，但是持续学习算法在任何时刻都禁止使用 \\(T\\) 这个信息。 CIL 第一个任务至少要包含 2 个类，之后的任务没有限制。与其他学习范式在具体定义上的区别可以参考下图1，不再详述：Baseline：多头模型对于非每个任务类别相同的场景，类是越来越多的。而且系统不知道未来有哪些类，无法在一开始就把所有类包括进来，构造出输出头固定的分类器；只能每当出现新类，临时加入该类的输出头。所谓的多头模型是指模型的主要部分（特征提取器 \\(\\varphi\\)）由各任务共用，但输出端不固定，随着新类别的引入，随时会引入新的输出头。因此模型参数会包含共享参数和类别独有的参数两部分，后者的比例应该是非常小的，所以即使它的数量线性增长问题也不大，是允许的。下面介绍以多头模型为基础的持续学习最简单的算法，算是所有持续学习算法的 baseline。微调算法最简单的学习方式，并不是每个新任务都从头开始训练，而是用上一个任务的训练结果作为下一个任务的初始化。具体来说，上图模型参数分为网络共享权重 \\(\\mathbf{w}_0\\) 和每个类别独有的权重 \\(\\mathbf{w}_1,\\mathbf{w}_2,\\cdots\\)。每遇到新类别都会引入新的 \\(\\mathbf{w}_i\\)，都作（随机）初始化。\\(\\mathbf{w}_0\\) 在算法的最开始（随机）初始化，且在每个时刻 \\(\\mathbf{w}^{(t)}_0\\) 都会用 \\(\\mathbf{w}^{(t-1)}_0\\) 初始化。这个算法在持续学习论文里习惯叫做微调（fine-tuning），因为直接拿上一个任务初始化的方式有微调上一个任务的意思。直观上看这种方式直接覆盖了上个任务的成果，重新把所有的模型容量让新任务占满，很容易灾难性遗忘。（尽管有类别独有的参数能防止遗忘，但它们占的比例太小，起的作用是远远不够的。）因此这个算法可以认为没有任何防遗忘机制，是一个白板算法，大家研究的持续学习算法都是在其基础上引入自己的防遗忘机制的。下图2描述了持续学习算法与微调白板模型参数更新路径的对比，图中展示的是参数空间，上面的点是参数，两个圈分别代表任务 1、任务 2 的分类损失函数（等高线）。训练任务 1 后参数位于右上方点，在训练任务 2 时，采用微调白板算法参数会直接更新到左上方点，而采用有防遗忘机制的持续学习算法（图中为 OWM）会更新到下方点。 需要注意的是，对于 CIL 这样的新任务中没有旧类的场景，由于没有旧类数据作为起对照作用的负样本，微调算法学到的模型一定会将所有输入预测为最后一个任务的类，导致旧任务不仅全部忘掉，还会使其准确率降为0（此时的遗忘，真的称得上是“灾难性”了）。所以这个微调算法只对不会出现新类的 TIL 场景有意义，对 CIL 是没有任何效果的。事实上，即使独立式学习也无法解决 CIL 场景。对 CIL，必须加入一定的防遗忘机制。固定特征提取器固定特征提取器走了另一个极端，在第一个任务训练结束后，特征提取器再也不动，遇到新任务时就只更新对应类别的输出头。这样固然不会忘记第一个任务，但是会导致后面的任务知识无法被学到（因为输出头的表示能力是不够的），最终导致第一个任务准确率很高，之后的任务很低，实际上和微调模型效果是类似的。数据集持续学习分类问题的常用数据集是通过机器学习的标准数据集划分、构造出来的。标准数据集例如常用的 MNIST、CIFAR、ImageNet 等。划分方式主要有两种： 分割（split）：按照类别划分数据集为任务，用于 CIL； 置换（permute）：对原数据集所有数据做一次相同的变换，得到一个任务，可以用于 TIL、DIL 等每个任务类别相同的场景。以 MNIST 为例，可以构造 Split MNIST、Permuted MNIST 两种数据集。Split MNIST 按类别划分成（以 5 个任务为例）0v1, 2v3, 4v5, 6v7, 8v9；Permuted MNIST 每构造一个任务时就按相同方式打乱各图片像素的顺序。四、持续学习的指标总体指标持续学习的主要目标是让模型在所有任务上表现都好，因此持续学习的指标首先是各任务平均指标，其次关注其他关心问题上的表现，如后向迁移能力、前向迁移能力。这些指标都是持续学习过程训练的各模型在各任务上的单个指标计算出来的1：记 \\(R_{\\tau,t}\\) 表示时刻 \\(\\tau\\) 训出的模型在第 \\(t\\) 个任务上的指标（例，对分类问题是准确率），注意每个任务都有自己的测试集 \\(\\mathcal{D}^{(t)}_{test}\\)，\\(R_{\\tau,t}\\) 是用 \\(\\mathcal{D}^{(t)}_{test}\\) 做测试的。可以得到以下总体指标： 各任务平均指标 \\(ACC = \\frac1T \\sum_{t=1}^T R_{T,t}\\)，即最后得到的模型在所有任务上的平均表现； 平均后向迁移 \\(BWT = \\frac1{T-1} \\sum_{t=1}^{T-1} (R_{T,t}- R_{t,t})\\)，即任务刚开始学（\\(t\\) 时刻）与学到最后（\\(T\\) 时刻）效果之差，对所有非最后一个任务取平均。这个指标只衡量了最后一个任务的后向迁移情况。 平均后向迁移指标可正可负。若为负，则代表没有后向迁移能力。 平均前向迁移 \\(FWT = \\frac1{T-1} \\sum_{t=1}^{T-1} (R_{t-1,t} - \\bar{b}_t)\\)。\\(\\bar{b}_t\\) 是随机初始化并用 \\(\\mathcal{D}_t\\) 训练的模型效果（多次实验取平均），有点 \\(R_{0,t}\\) 的意思，但不太一样。指标表示到在任务刚开始学但还没有学（\\(t-1\\) 时刻）期间累积的知识（比较的对象是不使用 \\(\\mathcal{D}_1,\\cdots, \\mathcal{D}_{t-1}\\) 前向迁移的知识、而只使用 \\(\\mathcal{D}_t\\) 自己知识的结果 \\(\\bar{b}_t\\)），对所有非第一个任务取平均。 平均后前向迁移指标可正可负。若为负，则代表没有前向迁移能力。 各任务平均指标有上界：将所有任务一起学习的指标，或各任务独立式学习的指标。 第一个指标的定义方式是公认的，后两者可能还有待探索。举个例子，FWT 中的 \\(R_{t-1,t}\\) 在 CIL 场景下是无法计算的，因为在 \\(t-1\\) 时刻压根就没有 \\(t\\) 时刻新出现的类别，FWT 需要另外定义。 以上指标都是以任务为单位作算数平均计算出来的，通常要求任务划分得比较均衡（事实上多数数据集是这样的，例如 CIL 每个任务的类数量相等），否则最好根据任务规模/难易加权平均，例如有些文章使用调和平均（harmonic mean）。还有的指标转而以类别为单位算平均。曲线图在持续学习实验中，最常见的观察对象是 \\(R_{T,t}\\)（横坐标为 \\(t \\in 1:T\\)），即最后学到的模型在各任务上的指标折线图。这个图相当于总体指标 ACC 的展开，更加具体：曲线越高，效果越好。还有一种是 \\(R_{t,t0}\\)（横坐标为 \\(t \\in t_0:T\\)，\\(t_0\\) 一般为第一个任务），即某个任务学习后，继续学习其他的任务对它的影响，也能反映遗忘的程度：下降得越慢，说明防遗忘效果越好。还有人会观察总体指标随 \\(T\\) 的变化曲线。也就是说，测试过程通常是每训练完一个新任务就对已涉及的所有任务测试一遍。训练过程中的学习曲线在训练过程中需要监视学习曲线，持续学习有多个任务，就有多个独立的学习曲线（下图对角线上的图）。但持续学习的目标不只是让当前任务学好，更关注是否会灾难性遗忘，所以还需要监视模型在旧任务上的表现，于是得到更多的学习曲线（下图对角线上方的图）。下图是一个例子，来自持续学习算法 EWC 论文[^footnote EWC]，我暂且称为 “三角图”： 注意这些图画的是模型于各训练阶段在整个测试集上的表现。为了画这张图，需要每隔一段时间（横坐标单位）就做一次完整的测试，其实很耗时间，但这时间不是算在训练时间内的，无所谓。一个好的持续学习算法应当在任务切换后（并不是瞬间）在旧任务上效果不变差太快，例如图中对任务 A，在训练结束切换至 B 时，准确率曲线 EWC 几乎不下降，而不加防遗忘机制的 SGD 就会迅速下降，说明 EWC 防遗忘性能比较优秀。其他分类问题常常讨论混淆矩阵，它实际上是准确率的展开，反映了哪些类分对、分错。对于所有任务类别相同的 TIL 场景，可以画每个任务的混淆矩阵；但多用于 CIL 场景，因为可以把所有任务画在一个矩阵里。以下是一个例子，来自持续学习算法 iCaRL 论文：五、防遗忘机制概论目前防遗忘机制可以概括为几类：重演数据法、正则化法、梯度操控法、网络结构法。（注意它们并不是互斥关系，持续学习算法可以结合多种防遗忘机制。）下面我将展开讨论每种机制的原理并介绍代表算法。在开始之前，首先说一下防遗忘机制有的共同元素。持续学习算法防止遗忘不是凭空的，总是要记住一些旧任务的知识或信息。持续学习算法一般都有一个对象在随新任务的到来不断地积累知识或信息，这个对象统称为记忆（memory），记为 \\(\\mathcal{M}\\)。这个记忆当然也是有限制的： 当然不能记下旧任务的所有训练数据，否则就变成了多任务学习，这种情况是坚决不允许的； 但记下旧任务训练的整个模型，如果它们不是直接用于测试，在目前持续学习的研究中，也是被允许的，因为从逻辑上讲这不构成独立式学习。持续学习算法的记忆存的信息是多样的，例如在重演数据法中存的是旧任务的部分数据，在其他方法中存的有梯度、mask 等非数据信息。相比于存整个数据，这种信息往往可以忽略不计，即使随任务数线性增加也没有关系。即使是存一个任务的整个模型，也比存整个数据小得多。来新任务时： 如何在新任务上使用旧任务的记忆； 如何迭代地积累新任务的知识；是大部分防遗忘机制的设计要素，下面介绍各算法时也主要说清楚这两点就可以了。在防遗忘机制中，通常有一个支配防遗忘程度的超参数可供人工调节，例如正则化法中的正则项系数。介绍算法是也会额外指出这个算法中调节防遗忘程度的超参数是什么。重演数据法防止遗忘最直接的方式是在记忆中旧任务的训练数据，称为重演（replay 或 experience replay）数据。此时这块记忆通常称为情节记忆（episodic memory，模仿心理学术语）。由于存的是数据本身，占用的空间是巨大的，记忆容量需要有更严格的限制，因此此方法一定要涉及记忆空间的管理，通常需要考虑如何选择有代表性的重演样本、空间不够时如何舍弃等问题。受记忆空间的限制，重演数据与真实训练数据相比太少，是重演数据法面临的主要问题。对应地，重演数据法的两个设计要素是： 如何在新任务上使用重演数据； 如何选择或构造重演数据，重演数据空间如何管理。iCaRLiCaRL1 是最早提出的重演数据法，也是 CIL 场景的最经典的算法。它只适用于 CIL 场景。非参数分类器上文说过，微调模型不适用于 CIL 场景，因为缺少旧任务的负样本。这主要影响的其实不是特征提取器，而是分类器。微调模型的分类器是带参数的一层网络，缺少负样本会让正样本对应的权重尽可能大，负样本权重尽可能小。因此，改成非参数的分类器，是解决问题的一种方法。在介绍之前，首先提一个关于神经网络的事实——最后一层作为线性层的分类器学习到的权重向量 \\(\\mathbf{w}_k\\)（与特征维度一致）可以作为特征空间上该类 \\(k\\) 的代表点，或者叫原型（prototype）。道理比较简单（但是没见过的话不一定会想到），因为对于训练数据 \\((\\mathbf{x}, y=k)\\)，交叉熵损失函数是让 \\(\\phi(\\mathbf{x})\\) 与 \\(\\mathbf{w}_k\\) 的内积尽量为 1，也就是说让 \\(\\mathbf{w}_k\\) 尽量与见过的训练数据 \\(\\mathbf{x} \\in \\mathcal{D}_{train}\\) 相等。因此，普通的线性层分类器测试时就是在比较 \\(\\phi(\\mathbf{x})\\) 与哪个原型 \\(\\mathbf{w}_k\\) 内积更大（更接近），因此可以看成一种原型最近邻分类器，只不过这些原型是可学习的参数。iCaRL 使用的是非参数的原型最近邻分类器，原型不是通过反向传播算法学习出来的，而是在 t 任务时直接使用该任务类的训练样本的均值作为原型。这种非参数分类器的坏处是，特征提取器无法与它一起训练（它没有参数，梯度流受阻），因此 iCaRL 采取的策略是训练时用参数分类器（它只是为了训练特征提取器），测试时改用非参数分类器。重演数据上面的设计只是间接地规避了没有负样本给训练带来的“逻辑”错误，不能看作一种防遗忘机制。缺少旧任务的负样本仍然是事实，是导致灾难性遗忘的原因。重演数据就是用于填补缺少的负样本。 重演数据的使用方法 可以简单地将其看做普通的训练数据，混到新任务数据中一起训练。iCaRL 采取了另一种方式：对重演数据使用的不是真实标签，而是通过上一个任务模型预测的蒸馏标签（准确来说，是得分）。在新任务训练开始前，先计算出蒸馏得分，再将它与新任务得分（全 0）拼起来作为重演数据交叉熵损失的参考对象。 用蒸馏得分的原因是，它比标签更容易保存低频信息。 重演数据的选择与空间管理 重演数据选择的逻辑是，离原型（均值）越近的越有代表性，因此选择的是（在特征空间上）离原型最近的前若干个点（注：原文有些许差别，但这里为了方便不计较了）。本文也要求重演数据的总量固定为 K，每个旧任务平均分配这个空间：一开始第一个任务占据了所有 K 个，之后第二个任务占据 K/2 个，第一个任务则舍弃掉 K/2 个（依据的也是离原型的远近），不需要重新选择；以此类推。记忆容量 K 是此算法唯一调节防遗忘程度的超参数。正则化法正则化法是对损失函数下手，对任务 \\(t\\) 的分类损失函数加防遗忘正则项，引导训练过程考虑防遗忘。防遗忘正则项不是普通的深度学习正则项，例如 L2 正则项等，因为这种正则项不包含旧任务的任何信息，只是漫无目的地防遗忘，而不是防止特定旧任务的遗忘。（EWC 论文中讨论过它们的效果差异，可以参考）正则项不能直接是旧任务的损失函数（因为无法获取旧任务的数据），而是某种代理损失（proxy loss）。引导的方向不同，就导致了不同的正则化法。\\[\\min_{\\theta} L^{(t)}(\\theta) = L_{FINETUNE}^{(t)}(\\theta) + \\lambda L_{REVIEW}^{(t)}(\\theta)\\]其中 \\(\\theta\\) 指代持续学习的所有参数；\\(L_{FINETUNE}^{(t)}(\\theta) = \\sum_{(\\mathbf{x},y)\\in \\mathcal{D}_{train}^{(t)}} L(f(\\mathbf{x};\\theta),y)\\) 即任务 \\(t\\) 正常的分类损失；\\(L^{(t)}_{REVIEW}(\\theta)\\) 是需要设计的防遗忘正则项。这里调节防遗忘程度的超参数是正则化系数 \\(\\lambda\\)。防遗忘正则项的构造方式有很多，可以是来自旧任务的各种信息，此时记忆存储的就是这些用于构造防遗忘正则项的信息。记忆这些信息的空间代价往往比重演数据要小很多。 正则项的构造甚至可以用重演数据构造，但一般把使用重演数据的归为重演数据法。例如 iCaRL 中的重演数据用于蒸馏损失，其实本质上就是正则项。LwFLwF（Learning without Forgetting）1是一个非常简单的防遗忘机制：在任务 t 训练开始前，先让任务 t 的数据 \\(\\mathcal{D}_{train}^{(t)}\\) 过一遍旧模型，得到旧模型分类的结果；在正式训练时，引导分类结果与旧模型分类结果靠近，模仿旧模型，达成防遗忘的作用。即加正则项：\\[L_{REVIEW}^{(t)}(\\theta) = \\sum_{\\mathbf{x}\\in \\mathcal{D}_{train}^{(t)}} L(f(\\mathbf{x};\\theta),f(\\mathbf{x};\\theta^{(t-1)}))\\]注意，在这个过程中没有用过重演的旧数据，用的是任务 \\(t\\) 训练之初自然继承下来的旧模型。整个算法唯一要记忆的是上一个任务学习的模型参数。这种简单的方法缺陷是致命的：旧模型的信息全部浓缩到了分类结果这个小小的标签中，信息量太少——因为达成一个分类结果的方式有很多，这样最终可能使得模型与旧模型只有 “形似” 而没有 “神似”。另外一个角度，模型会对同一个数据参考两个标签（旧模型的分类标签、真实标签），若二者不同，这种冲突不太合理；若相同，就没有引入正则项的必要了。EWCEWC（Elastic Weight Consolidation）1是正则化法中第一个产生重要影响的算法。它限制每个参数的值与旧任务尽量接近，并根据参数（对旧任务）的重要程度决定限制的程度（加上的每个参数的重要程度是 EWC 与 L2 正则化的本质区别）：\\[L_{REVIEW}^{(t)}(\\theta) = \\sum_{\\tau=1}^{t-1} \\sum_p^{#params} F_{\\tau,p} (\\theta_p - \\theta_{\\tau\\star,p})^2\\]\\(\\theta_{\\tau\\star,p}\\) 是学习了 \\(\\tau\\) 任务后的模型参数，\\(F_{\\tau,p}\\) 是 Fisher 信息量，它度量的是 \\(\\tau\\) 任务的样本对第 p 个模型参数所能提供的信息量，因此可以作为参数 \\(\\theta_{\\tau\\star,p}\\) 的重要程度。Fisher 信息量定义为似然函数对参数导数平方的期望，可以估计为用 \\(\\tau\\) 任务各样本计算的损失函数对参数导数平方的均值，它就是 \\(\\tau\\) 任务训练时梯度平方的累加，可以随着 \\(\\theta_{\\tau\\star,p}\\) 一起计算得到。记忆存放的信息是之前所有任务的模型参数 \\(\\{\\theta^{(\\tau)}_{\\star}\\}_{\\tau=1}^{t-1}\\) 以及计算得到的 \\(\\{F_\\tau\\}_{\\tau=1}^{t-1}\\)。要注意存的旧模型是不允许用于测试的，只允许辅助训练，这是它不属于独立式学习的原因。梯度操控法正则化法通过修改损失函数影响反向传播，间接地改变了参数更新过程。我们也直接规定、操控训练的更新过程。最常用的做法是直接操纵梯度，修改梯度的计算、梯度下降公式等，这里我称为 “梯度操控法”。一般流程为根据某种防遗忘的需要推导出修改梯度原梯度 \\(\\mathbf{g}\\) 的算法，再用修改后的梯度 \\(\\tilda{\\mathbf{g}}\\) 套用梯度下降公式跟新：\\[\\theta = \\theta - \\alpha \\tilda{\\mathbf{g}}\\]我将在这篇笔记中讨论梯度操控法的经典论文。网络结构法网络结构法从网络结构下手，将网络划分成各部分并按某种机制分配给各任务，构成某种子网络，因此又称参数隔离法（Parameter Isolation）。它显式地体现了模型容量分配问题，将模型容量这一概念显化到模型各部分参数了。这也是一种直接规定、操控更新过程的方法。注意， 对网络划分的意思并不是为每个任务使用单独的模型，各模型之间完全独立的独立式学习。各部分之间一定有着某种联系； 划分不是数学概念上的划分（split），可以重叠也可以不重叠； 一般是对网络的特征提取器部分进行划分，输出头部分总是共用的。这类持续学习算法只适用于 TIL 场景。在测试时，需要根据测试数据的任务 ID \\(t\\) 的信息，选取对应的网络划分部分作预测。对于这类方法，如果各部分能做到不相互重叠，那真的是可以显式地达到各任务互不干扰，它防遗忘方法中的最强者。某些论文里甚至声称能做到零遗忘（zero-forgetting）。模型扩张法：Progressive NN论文链接：Progressive Neural Networks, arXiv 2016该方法不固定模型大小，即模型大小可以随任务线性增加。它是每来一个新任务，就将网络（指特征提取器部分的网络，不算输出头）扩充一列（一列包括每层若干个神经元），如图：该方法主要的问题是模型大小线性增长，它以此为代价来解决模型容量问题，其实是很像独立式学习了。但它与独立式学习的区别是多了图中斜向右上箭头代表的权重，使新的一列网络与旧网络建立了联系——旧知识（旧网络的输出）可以通过该权重迁移过来。训练时，固定旧网络部分的参数（虚线）不动，训练新加入的参数（实线）。图中 \\(a\\) 为论文额外加入的非线性函数，可以更加强调斜向右上箭头权重的特殊性。测试时，使用测试数据任务 ID \\(t\\) 对应的部分网络作预测。Mask 机制如果要固定模型大小，参数隔离方法就是对固定数量的网络参数按任务进行划分，可以归结为引入加在模型上的遮罩（mask），它用来规定对模型某部分的选择，遮盖其他部分。Mask 用于不同的任务上，每个任务都有自己的 mask。持续学习中有大量的工作是基于 mask 机制的，例如 PackNet、HAT 等等，我不打算对着几篇代表性论文讲解，而是根据 mask 的实现方式、构造方式与在训练过程中的指导作用，对这些工作分分类。我将在这篇笔记中详细讨论一些工作应用 mask 机制的细节。Mask 的实现方式Mask 是遮在模型上的指示性变量，有两种形式： 加在参数上的（weight mask）：直接在每个参数上规定，对第 l 层与第 l+1 层之间的参数，\\(\\mathbf{M}^l\\) 为矩阵； 加在神经元上的（feature mask）：在每个神经元上规定，对第 l 层神经元，\\(\\mathbf{m}^l\\) 为向量，间接地影响与神经元相连接的参数。这种 mask 数量会大大减少。各任务的 mask 需要存储在记忆 \\(\\mathbf{M}\\) 中。对于 weight mask，每个任务占用空间是参数量级的；对于 feature mask，每个任务占用空间与神经元数目一致（相比 weight mask 成平方级地减少）。虽然记忆是随任务数线性增长的，但不用担心，它们比重演数据小多了。另外再次强调，mask 覆盖的参数不包括输出头的参数，因为不能直接影响输出头的输出结果（例如可能会让输出值变为 0）。Mask 变量的取值，可以是： 二元的 0,1：称为 hard mask，只有 2 种状态：被遮住与不被遮住； 实数值：称为 soft mask。以下不特别说明，都是指二元 mask。Mask 如何规定训练过程与测试过程Mask 如何影响新任务的训练过程（包括前向传播和反向传播）与最终的测试过程（前向传播），把这两件事定义清楚，mask 的作用就定义清楚了。以下是通常的选择： Mask 可以选择是否作用新任务训练过程的前向传播，若选择作用： 对于 weight mask：将 \\(\\mathbf{W}_l\\) 乘以 mask，即 \\(\\mathbf{W}_l \\odot \\mathbf{M}_l\\)； 对于 feature mask：将神经元输出（激活前后都一样）\\(\\mathbf{o}_l\\) 乘以 mask，即 \\(\\mathbf{o}_l \\odot \\mathbf{m}_l\\)； Mask 可以选择是否作用新任务训练过程的反向传播，若选择作用，指的是修改反向传播公式，在梯度流中阻塞： 对于 weight mask：将计算的 \\(\\frac{\\partial L}{\\partial \\mathbf{W}_l}\\) 乘以 mask，即被遮住的参数不更新； 对于 feature mask：将神经元输出（指激活后）\\(\\mathbf{o}_{l-1}\\) 乘以 mask，根据反向传播公式 \\(\\frac{\\partial L}{\\partial w_{i j}^{l}}=\\delta_{j}^{l} o_{i}^{l-1}=g^{\\prime}\\left(a_{j}^{l}\\right) o_{i}^{l-1} \\sum_{k=1}^{r^{l+1}} w_{j k}^{l+1} \\delta_{k}^{l+1}\\)，被遮住的神经元发射的参数都不更新； 对于测试过程的前向传播，mask 通常都是作用的（一般没有例外），将 \\(\\mathbf{W}_l\\) 乘以 mask，即 \\(\\mathbf{W}_l \\odot \\mathbf{M}_l\\)，即对于任务 ID 为 \\(t\\) 的测试输入，使用任务 \\(t\\) 的 mask 选择的子网络作预测。 如果 mask 对训练过程的前向传播和反向传播有独立的作用，那么一个二元 mask 是不够表示的，需要两个二元 mask，表示四个状态。但大多数论文只会涉及一个，所以只用一个二元 mask。如何构造 mask训练任务 \\(t\\) 时，不仅要将旧任务的 mask 作用到（如果选择作用的话）网络上，还要构造出新任务 \\(t\\) 的 mask。构造 mask 可使用固定算法（例如按照某种指标选择一定比例的重要程度较高的参数），也可将 mask 看成可学习的参数融进训练过程学习，即将 mask 代入到模型中当作参数随模型参数正常训练，也就是对如下计算图（以一层为例）作反向传播： 对于 weight mask: \\(f(\\mathbf{o}_l;\\mathbf{W}_l,\\mathbf{M}_l) = (\\mathbf{W}_l \\odot \\mathbf{M}_l)\\cdot \\mathbf{o}_l\\) 对于 feature mask: \\(f(\\mathbf{o}_l;\\mathbf{W}_l,\\mathbf{m}_l) = \\mathbf{W}_l \\cdot (\\mathbf{o}_l \\odot \\mathbf{m}_l)\\)注意： 如果 mask 融进训练过程来学习，每个任务的训练过程不仅要考虑训练新 mask，还要将旧任务 mask 的作用考虑进来，最终训练的反向传播过程形式更麻烦（并不只是上式）； 根据不同的构造方式，各任务的 mask 可以设计为允许重叠，也可以不重叠。如上所述，mask 不重叠（参数硬隔离）可以实现零遗忘，但也要面临模型容量问题； 二元 mask 是无法直接融进训练过程的（因为不连续不可导），需要采取一些训练技巧处理不可导。对于融进训练过程与模型参数一起训练的 mask，一个重要的事情是使其稀疏化，即尽量让 mask 少占用剩余可用的位置，防止模型容量过快用完。稀疏化也是必须的，因为如果不加任何限制，每个任务总会倾向于学到一个全 1 的 mask（因为为了在当前任务上效果好，学习算法会试图占据一切可能利用的模型容量资源的），导致从第一个任务开始，模型容量就快用完了。这是模型容量分配问题在 mask 机制上实际的表现，每篇使用可学习 mask 的论文都应设计恰当的稀疏化机制。稀疏化的方法通常是对 mask 加约束，如正则项，论文 HAT 中的正则项就是一个典型。 Mask 机制实际上就是一种注意力机制，注意力机制对输入（或者中间层）的每个元素考虑赋予不同的注意力得分，让模型更加关注输入中得分高的部分，得分低的则看作无用信息被抑制。重要的是，这个得分可以通过梯度下降自动地学习出来，而不用手动规定。 记住，注意力机制是一种思想的统称，可以与各种模型混合，不单单用在 RNN 中的 Encoder-Decoder 结构。这里的 mask 机制可能是比较简单的一种用法。优缺点讨论 重演数据法最大的缺点是重演数据量不够； 直接操控更新过程的方法（包括梯度操控法、参数隔离法）有更好的可解释性，但也会带来僵硬的问题。（二者有冲突）六、前沿方向这里列举一些持续学习的其他前沿方向，不作详细介绍。 小样本持续学习： 持续异常检测；（换问题） 带粗细粒度的持续学习；（造新场景） 持续学习 + Transformer（ViT）：DyTox。 …参考资料以下列举一些持续学习相关的参考资料与学习资源。 课程 Continual Learning（比萨大学）：https://course.continualai.org 持续学习社区 Continual AI：https://www.continualai.orgGradient Episodic Memory for Continual Learning M. De Lange et al., “A Continual Learning Survey: Defying Forgetting in Classification Tasks,” IEEE TPAMI 2021, vol. 44, no. 7, pp. 3366–3385, Jul. 2022, doi: 10.1109/TPAMI.2021.3057446. &amp;#8617; &amp;#8617;2 &amp;#8617;3 &amp;#8617;4 &amp;#8617;5 G. Zeng, Y. Chen, B. Cui, and S. Yu, “Continual learning of context-dependent processing in neural networks,” Nat Mach Intell, vol. 1, no. 8, Art. no. 8, Aug. 2019, doi: 10.1038/s42256-019-0080-x. &amp;#8617; " }, { "title": "论文笔记：Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks", "url": "/posts/papernotes_Continual-Learning-of-a-Mixed-Sequence-of-Similar-and-Dissimilar-Tasks/", "categories": "科研", "tags": "论文笔记, 机器学习, 持续学习", "date": "2022-07-06 00:00:00 +0800", "snippet": "论文信息Continual Learning of a Mixed Sequence of Similar and Dissimilar Tasks 会议：NIPS 2020 作者： Zixuan Ke：伊利诺伊大学芝加哥分校，博士生，后者的学生。 Bing Liu：伊利诺伊大学芝加哥分校，教授。他是《终身机器学习》的作者，我有系列读书笔记。 Xingchang Huang：苏黎世联邦理工大学，博士生。 一、场景本文的场景就是普通的持续学习，但与其他文章不同的是，其他文章通常只关注如何解决灾难性遗忘，本文认为除了解决灾难性遗忘，还有很多其他的事情要做。假设已学完任务 \\(1, \\cdots, t-1\\)，准备学 \\(t\\)。以作者的理解，对旧任务 \\(t_{old}\\) 的处理是有区别的： 如果 \\(t_{old}\\) 与 \\(t\\) 相似，则二者的知识应相互迁移： 前向迁移（Forward Transfer）：用 \\(t_{old}\\) 学到的知识帮助新任务 \\(t\\)； 后向迁移（Backward Transfer）：\\(t\\) 学到的知识反过来更新旧任务； 如果 \\(t_{old}\\) 与 \\(t\\) 不相似，则二者蕴含的知识也是不交叉的，所以都应该记住，即在学习新任务 \\(t\\) 时避免遗忘 \\(t_{old}\\) 的知识。而其他文章将其一视同仁，都化为第二种情况，干脆都别忘了。迁移学习要求两个领域有相似性（见《终身机器学习》第二章笔记），作者如此分成相似与不相似的任务，前者应用迁移学习，所以本文是一篇典型的持续学习与迁移学习结合的文章。如果任务 \\(1,\\cdots, t-1\\) 都属于作者描述的不相似的任务，那么作者的方法就相当于其他文章了。所以本文也可以看作普通持续学习场景的推广：普通场景任务各不相似，而本文允许 “Mixed Sequence of Similar and Dissimilar Tasks”。注意本文的场景必须是任务增量（文中称 Task Continual Learning, TCL），任务标识 \\(t\\) 随数据一并给出。类别增量一般每次来的是不同的类，因此很少会出现相似的情况。二、模型本文模型称为 CAT（Continual learning with forgetting Avoidance and knowledge Transfer）。它由三部分组成： 知识库（KB）：是一个网络，权重中存储知识，是模型的主要部分。输入即原始输入 \\(x\\)，输出为 \\(x\\) 的一个 Embedding，其后可接分类器分类。 记号：\\(k_l\\) 为第 \\(l\\) 层神经元个数，\\(w_l\\) 表示第 \\(l\\) 层到第 \\(l+1\\) 层的连接权重；\\(g_l\\) 表示损失函数在权重 \\(w_l\\) 上的梯度，二者都是 \\(k_l \\times k_{l+1}\\) 矩阵；\\(h_l\\) 表示一个输入 \\(x\\) 在第 \\(l\\) 层的输出，是 \\(k_l\\) 维向量。 任务 Mask（TM）：为 KB 网络每一层神经元（不是权重）提供 mask，标识了对当前任务的重要程度，它是二值的 0,1。 记号：\\(m_l^{(t)}\\) 表示任务 \\(t\\) 第 \\(l\\) 层的 mask，是 \\(k_l\\) 维向量。它起到了选择子网络的作用，注意有时我们想要整个网络，则不允许它起作用（或全设为1）。具体的选择作用表现在两个方面： 选择每一层的输出：一个输入 \\(x\\) 在通过网络得到 \\(h_l\\) 后，还要调整 \\(h_l \\otimes = m_l\\)（效果是把一些输出值置为0），才能继续通过下一层；（最终，加了 mask 后最后一层的输出可看成针对任务 \\(t\\) 的特征） 选择更新的权重：选择了输出后，未被选中的神经元后面连接的权重就不必更新了。做法是先将 mask 向量扩展（与 broadcast 类似）为形状同 \\(w_l\\) 的 \\(k_l \\times k_{l+1}\\) 矩阵，再调整梯度 \\(g_l \\otimes = (1-m_l)\\)（效果是把置0的神经元后面权重的梯度置为0）。 知识迁移 Attention（KTA）：是一个使用了简单的 Soft Attention 机制的网络，负责知识迁移（参考此文章），其目的是把相似任务的结果融合并迁移到新任务上。输入为 \\(x\\) 过任务 \\(i_{sim}\\) mask 的一系列特征 \\(h_{mask}^{(i_{sim})}\\)，输出它们的某个线性组合，其后可接分类器分类：\\[h_{K T A}^{(t)}=\\sum_{i} a^{\\left(i_{s i m}\\right)}\\left(\\left\\{h_{m a s k}^{\\left(i_{s i m}\\right)}\\right\\} \\theta_{v}\\right)\\]系数即 Attention 得分：\\[a^{\\left(i_{s i m}\\right)}=\\operatorname{softmax}\\left(\\frac{\\left(e_{K T A}^{(t)} \\theta_{q}\\right)\\left(\\left\\{h_{\\operatorname{mask}}^{\\left(i_{s i m}\\right)}\\right\\} \\theta_{k}\\right)^{\\top}}{\\sqrt{d_{k}}}\\right)\\]参数有公共的 \\(\\theta_k,\\theta_q,\\theta_v\\) 和 task-specific 的 \\(e_{KTA}^{(t)}\\)。Attention 机制有参数少的特点，这些参数存储了更多的知识迁移的经验。模型如图所示，主网络为 KB + KTA，TM 是嵌入在 KB 里的一个挂件。该网络在完全不需要知识迁移的时候使用 KB 的 Embedding 后接分类器分类，而需要迁移的时候将 KB 的 Embedding 再通过 KTA 得到进一步的 Embedding 后接分类器分类。 由于是任务增量学习，分类问题是几分类是已经知道的，所以分类头的形状是提前固定的。Task Embedding每个任务标识 \\(t\\) 对应一些 Embedding，它不是输入 \\(x\\) 的 Embedding，称为 Task Embedding。这些 Embedding 提供了“第几个任务”这种信息，是学习出来的，属于 task-specific 的网络参数，在本文中有两类： \\(e_l^{(t)}\\)（合称 \\(e^{(t)}\\)）：用于生成 KB 第 \\(l\\) 层的任务 mask \\(m_l^{(t)}\\)。二者对应关系为 \\(m_{l}^{(t)}=\\sigma\\left(s e_{l}^{(t)}\\right)\\)（\\(s\\) 为超参数），大于 0.5 取 1，反之取 0。 \\(e_{KTA}^{(t)}\\)：用于输入给 KTA，辅助计算 Attention 得分，其完成知识迁移。模型输入不只有 \\(x\\)，还有任务 ID \\(t\\)，这正是任务增量学习的设定。如上图右下角所示，任务标识\\(t\\)先生成两种 Task Embedding，箭头分别指向 KB 和 KTA。三、测试过程经过 \\(T+1\\) 个任务的训练，训练过程已训练好如下参数或了解到如下信息： \\(\\theta^{(T)}\\)，它包含所有与任务无关的网络参数，如 KB 的权重，KTA 的 \\(\\theta_k,\\theta_q,\\theta_v\\) 以及两个分类头的权重。在训练时，它们不断更新，只用最后任务 \\(T\\) 的结果； 每个任务与之前任务的相似性判断结果：每个任务都有两个集合 \\(\\tau_{sim}, \\tau_{dis}\\) 分别表示相似任务与不相似任务； 每个任务 \\(t\\) 的 Task Embedding \\(e^{(t)}\\) 和 \\(e_{KTA}^{(t)}\\)（后者有的可能没有，也不需要，见下节tip），前面说过它们是 task-specific 的参数。测试过程：新来一个测试数据 \\(x\\) 及其任务ID \\(t\\)： 如果有前面的任务与之相似（\\(\\tau_{sim}\\neq \\varnothing\\)），则 \\(x\\) 通过 KB 在 \\(\\tau_{sim}\\) 任务上的 mask 得到一系列特征 \\(h_{mask}^{(i_{sim})}\\)，让它们过 KTA 和后面的分类头，得到分类结果； 如果没有一个相似，则 \\(x\\) 通过 \\(e^{(t)}\\) 生成的该任务的 mask，得到最后一层特征 \\(h_{mask}^{(t)}\\)，再通过 KB 后面的分类头，得到分类结果。四、训练过程任务 0：直接训练 \\(f_{mask}\\)，即 KB + TM + 后面的分类头： 数据：\\(D_{train}^{(0)}\\) 损失函数：\\(\\frac{1}{N_{0}} \\sum_{i=1}^{N_{0}} \\mathcal{L}\\left(f_{m a s k}\\left(x_{i}^{(0)} ; \\theta_{m a s k}\\right), y_{i}^{(0)}\\right)\\) 训练方式：因为是第一个任务，随机初始化 要用的结果：训练得到的 KB + 分类头权重 \\(\\theta^{(0)}\\)，任务 0 的 Task Embedding \\(e^{(0)}\\)（对应的任务 mask \\(m^{(0)}\\) 如图右上）任务 1：首先判断它与任务 0 是否相似。做法是比较两个模型 \\(f_{\\varnothing}, f_{0\\rightarrow 1}\\) 的效果：先训练参考模型（Reference Model） \\(f_{\\varnothing}\\)，即对任务 1 从头开始训练的 KB + 后面的分类头： 数据：\\(D_{train}^{(1)}\\) 损失函数：\\(\\frac{1}{N_{1}} \\sum_{i=1}^{N_{1}} \\mathcal{L}\\left(f_{\\varnothing}\\left(x_{i}^{(1)} ; \\theta_{\\varnothing}\\right), y_{i}^{(1)}\\right)\\) 训练方式：单独复制出来一份 KB（之前的 KB 是主要的东西，不能覆盖掉），随机初始化 要用的结果：用验证集 \\(D_{val}^{(1)}\\) 测试效果再训练迁移模型（Transfer Model） \\(f_{0\\rightarrow 1}\\)，即参考模型中的 KB 部分不用从头训，直接用针对任务 0 的特征——将训练数据过任务 0 mask 最后一层的输出，只训练后面的分类头。这件事可以等价地看成固定之前训练的任务 0 的 KB + mask： 数据：\\(D_{train}^{(1)}\\) 损失函数：\\(\\frac{1}{N_{1}} \\sum_{i=1}^{N_{1}} \\mathcal{L}\\left(f_{0\\rightarrow 1}\\left(x_{i}^{(1)} ; \\theta_{0\\rightarrow 1}\\right), y_{i}^{(1)}\\right)\\) 训练方式：固定 KB + mask 部分的权重，分类头随机初始化 要用的结果：同上如果迁移模型比参考模型效果好，任务 1 用 0 的知识都比它自己努力学习知识要好，那么有充分的理由说 0 里包含了与 1 相似的知识。相似与否，决定了任务 1 该如何训练：若任务 1 与任务 0 不相似，则应该走防止遗忘的路线。先用任务 0 的 mask 屏蔽掉对任务 0 重要的权重，训练任务 1： 数据：\\(D_{train}^{(1)}\\) 损失函数：\\(\\frac{1}{N_{1}} \\sum_{i=1}^{N_{1}} \\mathcal{L}\\left(f_{\\varnothing}\\left(x_{i}^{(1)} ; \\theta_{\\varnothing}\\right), y_{i}^{(1)}\\right)\\) 训练方式：固定 KB 属于任务 0 mask 对应的权重，训练其他部分 要用的结果：\\(\\theta^{(1)}\\)，\\(e^{(1)}\\)（此时任务 mask 如图右中，注意 \\(m^{(1)}\\) 与 \\(m^{(0)}\\) 不可能重合）若任务 1 与任务 0 相似，则应该走知识迁移的路线。与迁移模型道理一样，也是提取任务 0 mask 最后一层输出，但这时要训练不是简单的分类头，而是作者提出的专门负责知识迁移的 KTA + 分类头。另一个不同的是，作者没有固定前面的 KTA + TM，它允许被梯度回传训练，一是为了得到 \\(e^{(1)}\\)（这一步必须有，否则后面任务无法进行）；二是也更新 KB，被认为是 Backward Transfer。 数据：\\(D_{train}^{(1)}\\) 损失函数：两部分 \\(\\frac{1}{N_{1}} \\sum_{j=1}^{N_{1}} \\mathcal{L}\\left(f_{m a s k}\\left(x_{j}^{(1)} ; \\theta_{m a s k}\\right), y_{j}^{(1)}\\right)+\\frac{1}{N_{1}} \\sum_{j=1}^{N_{1}} \\mathcal{L}\\left(f_{K T A}\\left(x_{j}^{(1)} ; \\theta_{K T A}\\right), y_{j}^{(1)}\\right)\\) 训练方式：不要屏蔽任务 0。在 $$正常训练即可 要用的结果：\\(\\theta^{(1)}\\)，\\(\\theta^，\\)e^{(1)}\\(（此时\\)m^{(1)}\\(与\\)m^{(0)}\\(可以有重合，因为任务 0,1 是相似的），\\)e_{KTA}^{(1)}$$任务 t：相当于把任务 1 的两种情况推广到一次面对多个旧任务的情形。首先是判断任务相似。此时要比较任务 \\(t\\) 与 \\(0,1,\\cdots,t-1\\) 共 \\(t\\) 个任务的相似性。参考模型 \\(f_{\\varnothing}\\)只需要一个，迁移模型则要 \\(t\\) 个：\\(f_{0\\rightarrow t}, \\cdots, f_{t-1\\rightarrow t}\\)。得到与 \\(t\\) 相似与不相似的任务集 \\(\\tau_{sim}, \\tau_{dis}\\)。对 \\(\\tau_{dis}\\) 中的任务，要防止遗忘，它们重要的权重要统统屏蔽掉，只需要将它们的 mask 并起来即可\\[m_{l}^{\\left(t_{a c}\\right)}=\\text { ElementMax }\\left(\\left\\{m_{l}^{\\left(i_{d i s}\\right)}\\right\\}\\right)\\]对 \\(\\tau_{sim}\\) 中的任务，要知识迁移，提取在这些任务 mask 最后一层输出，这可能涉及到多个。所幸这个 KTA 能接受多个输入，也能训练出多个 \\(e^{KTA}\\)。所以，任务 t 要训练的是： 数据：\\(D_{train}^{(t)}\\) 损失函数：两部分 \\(\\frac{1}{N_{t}} \\sum_{j=1}^{N_{t}} \\mathcal{L}\\left(f_{m a s k}\\left(x_{j}^{(t)} ; \\theta_{m a s k}\\right), y_{j}^{(t)}\\right)+\\frac{1}{N_{t}} \\sum_{j=1}^{N_{t}} \\mathcal{L}\\left(f_{K T A}\\left(x_{j}^{(t)} ; \\theta_{K T A}\\right), y_{j}^{(t)}\\right)\\) 训练方式：屏蔽任务 \\(\\tau_{dis}\\)，其他正常训练即可 要用的结果：\\(\\theta^{(t)}\\)，\\(e^{(t)}\\)，\\(e_{KTA}^{(t)}\\) 请注意，\\(e_{KTA}^{(t)}\\) 并不是所有任务都需要。如果永远没有与 \\(t\\) 相似的任务，那么也就无需训练它了，因为测试阶段不可能用到它。上图最下面的两图，表示任务 2，\\(\\tau_{sim} = \\{1\\}, \\tau_{dis} = \\{0\\}\\)。请自行体会。总结一下上面，任务 0 相当于不与前面任何相似。任务 1 由于前面只有一个，要么空集，要么全集；中间情况只有任务 2 才可能开始有。任务 0 和 1 的过程都可以统一到任务 t 的流程内。" }, { "title": "一个新手写的做饭基础知识指南", "url": "/posts/cooking_basics/", "categories": "生活", "tags": "做饭", "date": "2022-07-05 00:00:00 +0800", "snippet": "我说过会在这个网站更新生活的东西，没错！就是做饭，哈哈！我其实不是很会做饭，小时候就是给我爸打杂的，现在也只能搞定一些简单的菜，炒个肉，下个面条，做大菜容易翻车哈哈，这也是我要开这些做饭笔记的原因——提高自己的厨艺！这不又回家了，这次在家的时间不出意外有几个月，是一个系统锻炼厨艺的好机会。科研累了能靠做饭换换脑子，好事！既然要系统起来，这篇文章就整理一些通用的知识，不搞具体的菜谱。做饭并不是完全形式化的流程，不是严格按照菜谱就能做好的（我认为 GitHub 上很火的项目“程序员做饭指南”想法很好，但没有那个必要），需要掌握一些基本原则和经验，否则菜谱上的流程理解不了，也记不住。本文就是对这些基本原则与经验的总结。我暂时只涉及中餐（西餐是另一套体系），按照准备一顿完整的饭的逻辑，包括： 准备做饭的工具：各种厨具的科普； 第一步：选购食材，介绍各种食材的特点，以及保存方法； 第二步：处理食材，介绍食材从买回来到正式下锅需要做的处理工作，包括清洗、切等； Before 第三步：介绍各种调料的种类和作用； 第三步：正式上锅，开始烹饪。介绍烹饪方式，包括煮、蒸、炒、炖、煎、炸等。 最后介绍饭后的清洁工作，包括剩菜的处理，刷碗、刷锅的方法等。整理这些，对我来说算是重新学习了；对大家来说，如果你是纯厨房小白，可以当作入门指南看哦。其实包括我在内，刚进厨房时很多常识都不懂，会被爸妈笑话一通，这种感觉会让下厨房有心情障碍，劝退了很多同学！所以我在这里将其组织成更有逻辑的、符合学生思维习惯的材料。这里面科普性的知识整理自百度、Google、B 站的厨师大号，经验性的常识基本都是我从爸妈、姥姥那里听来的！看完了这篇指南，再看一看我整理的做过的菜谱列表，就可以动手了！另外，本文不涉及食物的营养成分以及如何减肥等话题。本人还算很健康，所以就只管好吃啦。做饭是一门怎样的技能？做饭也是一门技能，想学习也是需要心态和方法。以下是我总结的一些感悟，可以在正式内容前看一看，仅供参考哦（我也是小白！）。 首先要找到在灶台前不发怵、从容自在的感觉。我在第一次炒菜时很怕油（见过家里油锅起火），生怕某一步发生危险，心情是很紧张的。这种紧张会让动作变得生硬，大脑也容易短路忘掉关键的步骤。从容自在才会减少做饭的失误，看上去也会有模有样； 必须多加练习，在了解了必要的常识后，练习是解决发怵问题的唯一方法，也是做饭最重要的事情；有很多实操的内容也是图片文字描述不清楚的，需要参考一些视频，多动手找感觉； 需要多思考多琢磨，理解大神菜谱中每一步起的作用。能做到这一点，基础知识是必不可少的，但很多时候需要自己动手试错才能明白。在理解了原理之后，就容易举一反三不看菜谱做饭了。我会在本文尽量关注这种本质的东西，并在其他菜谱类的文章中附上简短的实战笔记； 做饭虽然像写代码，但它的一大难点是有时间限制（尤其是下锅后），需要卡好关键时间点。拿游戏来比喻，写代码是回合制游戏（如下棋），而做饭是 RTS，很多人能在深思熟虑后走好棋子的每一步，但却玩不动需要高度集中注意力的 RTS 游戏； 家庭做饭还要考虑效率问题，做饭不能花太久的时间，否则增加的机会成本也很劝退。在无法压缩步骤时，需要统筹好做菜各个工序，插空利用时间，同时做多道菜更是如此。虽然这件事本质是 AOE 图关键路径问题，但是人脑不是计算机，没有装备算法！只能靠长期做饭找时间管理经验。所以不必怕做饭慢，做时间久了就能压缩掉很多做饭间隙的很多空余时间。一、厨具准备好工具是下厨房的前置工作，如有缺失应按需购买。本章介绍一切与厨具有关的事情，包括种类、用途等。只涉及重要、必要的工具。热源一般居民楼家庭的主要热源是燃气灶，通常通过楼房内置的输气管道输送天然气作为燃料（计费方式类似水电），极少数仍然使用煤气罐的煤气。农村仍然使用烧煤、烧柴起的明火。详细用法在“烹饪方式”一节穿插讲述。除此之外，家庭中有很多花样的厨用电器可提供热源，它们虽然不提供明火，但起到加热的效果。不同电器的常见用途也各不相同： 电磁炉：燃气灶的取代品。它是一种暗火，原理是利用交流电产生不断变化的磁场，磁感线垂直于炉面，在上放置锅具（导体）后截面磁通量也不断变化，根据法拉第电磁感应定律，从而形成绕磁场的环状电流（涡流）。因此最好使用导磁性好（磁导率高）的铁锅； 烤箱：能直接提供高温环境，是暗火。原理是传统的电阻丝发热（焦耳定律）。起到类似效果的有空气炸锅，可以理解为进风的烤箱，可以当作烤箱用； 微波炉：仅用于加热食物，难以使食物由生变熟。原理是将微波（一种频率高的电磁波）辐射到食物中，加速分子运转，使其相互“摩擦”； 电饭煲：就是自带加热功能的柱形锅，最常用来煮米饭，也可以煮粥、熬汤等； 电饼铛：两面发热的铁板，常用来烙面食（与烙煎饼的鏊子是一回事），也可以用来煎。 使用安全： 烤箱不可放入不耐热的容器，包括玻璃制（会炸裂）、木制（会起火）、瓷器（会绷瓷）、塑料制（会融化、有毒）。最常用的烤箱容器是金属制品； 微波炉不可放入金属容器，会出现电火花并放出微波，这会损伤微波炉；也不能加热塑料、封闭容器。 食材处理工具刀和菜板是处理食材的主要工具。刀仅考虑中式菜刀，即横面为长方形的。它主要分成： 片刀：比较细腻，适合普通切菜； 剁刀：适合切肉、剁骨等重力气的场景；剁刀切菜会感觉很钝，而片刀剁骨会崩裂刀口（很危险！），所以家里应备两种菜刀各一把。菜板一般为木制，也有塑料制，注意有些菜板很娇贵剁不了肉。 注意食品安全：生熟分开，刚切完生食的刀和菜板不能直接切熟食。还有一些其他工具。能提高处理特定食材的效率，一般可以用刀来替代： 削皮器：用来削土豆、胡萝卜等蔬菜的皮。正确使用方法：反手拿（吃劲省力）、向离开自己的方向削（不容易伤自己）； 蒜臼：捣蒜用。淘宝上还有类似的捣蒜神器； 刨丝器：刨萝卜丝、土豆丝等。正确使用方法：固定刨丝器面板，让萝卜沿着一个方向蹭刀片。注意在一根萝卜快刨完时不要伤到手； 搅蛋器：混合蛋清和蛋黄，形成蛋液。可以用筷子取代。正确使用方法：沿着一个方向搅动； 大剪刀：常用来处理鱼，也可以处理包装袋； 硬刷子：用来刷掉蛤蜊、海蛎等贝类的沙。容器这里的容器指非炊具，用于临时盛放处理前后的食材。建议购买足够数量的大小不等的盆，最好金属制。虽然也可以使用碗和盘，或者切完直接放在菜板上，但碗和盘是瓷的易碎，有盆会更加方便，而且能完成冲洗、腌制的工作。另外最好也备一个沥水盆，方便沥水。量具包括量勺、量杯、电子秤等，主要用于量取调料的用量，平时做饭可有可无，可以买来起参考作用。详见“调料”一章。炊具炊具即用来煮食的器皿，主要介绍锅（厨用电器也算炊具）。锅按用途分为： 圆底炒锅：中餐最常用的炊具，有手柄便于翻动，圆底形状适合炒菜（炒菜需要起锅烧油煸炒辅料等操作，见“炒”一节）； 平底锅：与圆底炒锅一样也是炒菜用，但平底形状更适合煎（见“煎”一节）； 汤锅：锅壁很高，有双耳朵便于端握，不能用来炒，一般用来煮汤或煮粥； 蒸锅：一口大锅，中间以带孔的蒸片隔开，用于蒸（见“蒸”一节）； 高压锅：给锅内高气压使液体沸点升高，食物更容易熟。通常用于炖、煮不容易烂的食物； 高压锅爆炸非常危险，新手慎用！根据需要选购，应至少备有一个炒锅或平底锅。选购时注意材质的区别： 铁锅：最传统的材质，容易生锈； 不锈钢锅：轻快，但导热不如铁锅好； 不粘锅：带有不粘涂层，比铁锅等更容易洗； 砂锅：陶瓷制，只适用于汤锅。 不粘锅不能用铁铲，也不能温度过高，会把涂层刮坏或融化，释放有毒物质。应该用木铲或硅胶铲。在烹饪时需要搅拌翻炒食物，此类工具有： 铲子：适合炒菜； 大勺：更适合带汤的菜； 漏铲：带长条形漏孔的铲子，适合煎（因为不想把液体的油也捞起来），还可用于压土豆泥； 笊篱：用于从水中捞、搅食物，场景如焯水后捞出，捞饺子（注：北方饺子不带汤）。根据需要选购，应至少备有一个铲子和大勺。二、食材本章介绍各种食材、保存方法、处理方法。食材种类繁多，差异很大，建议走进农贸市场或超市亲自看看。主食及其做法主食包括水稻、面食、玉米等。面食我将在另一篇米面食专题系统介绍，玉米、南瓜、红薯等的做法一般是蒸（见“蒸”一节）。本节只讲水稻，因为其他主食如馒头等能买到现成的，米饭和粥只能自己做。水稻即大米，分类有： 粳米（圆粒米）：粘性高，做米饭更粘香，适合做粥、寿司等。市面上东北大米等属于此类米； 籼米（长粒米）：粘性低，粒粒分明，做米饭更干松，适合炒饭。市面上南方大米、“xx香米”等属于此类米； 糯米：非常粘，一般用于制作甜点（见米面食专题），不用于做米饭。米的两种吃法是米饭和粥。做熟米饭的原理是，让大米吸水，并受热变熟。为了达到此目的，通常遵照以下几步： 淘米：洗掉米中的杂质，让大米吸收水分； 浸泡（可选）：让大米进一步吸收水分； 加水：提供下一步消耗的水。加水的量是有经验的，不能太多，也不能太少，通常加到大米表面上面一个指节； 加热：吸水、受热的主要步骤。通常用电饭煲（是全自动的，按下按钮等待即可），也可用蒸锅蒸； 焖制（可选）：停止热源，不开盖，让米饭在热气中浸一会。粥这里指仅有大米的白粥，其他种类的粥见米面食专题。粥本质就是米饭多放了很多水，所以只需在上面的“加水”步骤中多加一些水。应当注意： 粥只能煮，不能蒸；不需要“焖制”步骤； 水的量也是不能太多或太少的，太多会很稀，太少会粘稠甚至容易糊锅。食材容易熟的程度做菜的一个原则是，不容易熟的食材先下，容易熟的后下，这样才能做到出锅时各种食材同时熟（不同时熟的话，需要继续把没熟的食材烧熟，但由于已经熟的食材混在了里面，继续则会让它们变老）。本节总结一下哪些食材容易熟，哪些不容易熟： 原则上，蔬菜类比肉类都容易熟得多； 蔬菜熟又称断生； 鸡蛋很容易熟，只需要不到一分钟，所以打蛋花的步骤要在临出锅前。食材保存方法买回来的食材需要保存。其实最好的办法是现买现做现吃，但很多时候时间成本（加上现在的防疫因素）不允许这样做，就需要妥善保存，防止变质。保存食物的重要工具是冰箱。冰箱分冷藏、冷冻两个位置，前者是冰点以上个位数的温度；后者在冰点以下，一般低于零下10度。需要放冰箱的食物（见下）如果不放冰箱，大多不到 1 天就会变质，放冷藏保质期一般 1-3 天，放冷冻保质期能延长数月。使用冰箱应注意： 有些食材如蔬菜、水果，不可放冷冻，会冻坏，冻坏的蔬菜会变味，吃了容易生病（仍记得在学校食堂吃了没完全解冻的白菜水饺，拉了半个月..qwq）； 注意分类放置，防止串味或交叉感染细菌。各种食物的保质期、保存方法都有所区别，这里仅列举一些大的原则，具体食材的保存方法建议百度。 肉类、海鲜类：切小块（分出每次的使用量），挤出空气包上保鲜膜（或塑料袋），时间短（1-3天）放冷藏，时间长放冷冻；肉类不要直接放进冰箱，容易滋生细菌； 菜类 块根类：如地瓜、土豆、胡萝卜、洋葱、西红柿等，放在阴凉干燥处。家里一般找个筐或网袋装，放在架子靠近地面的位置。不要放入冰箱； 绿叶菜：最怕蔫、萎，重点是保持水分，应像搭理家里的花草一样，放在冷藏，定期喷水； 水果类：没有熟的放外面自行催熟，熟的则像绿叶菜一样放在冰箱。注意与蔬菜分开，否则也会催熟蔬菜。处理方法大部分食材的处理遵循一下几个步骤。这三步在极少数情况下会调换顺序。第一步是去掉脏东西、清洗。例如蔬菜需要择菜，肉类需要去血水，鱼需要刮鱼鳞、去内脏等。去掉脏东西后用水冲洗或浸泡干净，就可以进行下一步了。如果是半成品精包装，此步可以省略。不同食材具体的处理方法穿插在我的菜谱类文章中。第二步是拆解整块的食物，有时这一步会放在第三步后面。“拆解”就是切成或剁成各种形状，需要刀工。下面介绍刀的用法：首先是拿刀姿势，不是直接抓握刀柄就可以了，那是砍人的姿势！正确的应该是大拇指和食指抓紧刀身，其他三指自然扣在刀柄上，这样能方便控制刀的位置，不会切偏发生危险。然后要手腕用力，而不是手臂。接下来是刀工，列举如下表： 刀工 姿势 适用食材 直刀切 垂直下落切 清脆的，如土豆、芹菜等蔬菜 推/拉刀切 稍斜下落切，多一个推或拉的动作，最多推+拉（不可反复） 不脆的有韧性的东西，主要是肉类 压刀切 先卡好位置，用另一只手心用力压刀背 硬的、难切的、需要保持形状的，如熟鸡蛋、鸭头 锯刀 来回地推拉，像锯东西一样 软的、需要保持形状的，如馒头片、红烧肉的肉块 摇摆切，轧（gá） 刀的前后端交替下落切 容易崩出来的，如蒜末，花生碎 片 平行于桌面横着切 竖着切不好切的，软的，如烤鸭 拍 用刀板横面拍砸 能拍烂的食材，如黄瓜、蒜 剁 刀刃离开食材，后摇砍砸 正常切不可能切开的，往往是带骨头的，如鸡块，排骨 马蹄刀 用两把刀来回剁，发出马蹄声节奏：XXX XXX 需要弄成泥的，肉馅 注意安全，不要切到手。手应当尽量向后收，不要平放在食材上，更不能垫在食材下面，最好的方式是左手大拇指垂直按住食材，这样切也只会划过指甲，而不是割伤皮肤。 不同刀法使用刀刃的位置有所不同，记住口诀：前切后剁中间片。这是最省劲也最不容易切到手的位置。 学习方法：每次切东西时首先纠正拿刀姿势，再有意识地考虑该用什么刀法，在上表对号入座，最后多加练习。多次运用以上刀法可以将食材切成不同的形状。但这也是讲求效率的，例如切丝不能真的一丝一丝地切。列举如下： 滚刀块：切的时候滚动原料切成的有棱角的块； 丝：先切成片，再沿着同一个方向切丝； 丁：先垂直横截面切成网格状（有条件的话不要切断，方便固定），再横切成丁； 菱形片（斜刀片） 块状食材（如胡萝卜、黄瓜）：先切出斜截面，再纵切片； 空心食材（如青椒）：先垂直横截面切十字，再斜切片； 花刀：在食材身上划一些纹理，使其能卷成麦穗状、松果状等好看的形状。很考验技术，家庭做菜一般不用。 蔬菜做熟后一般会缩水变小。切蔬菜的时候注意把块稍微切大一些。第三步是焯水或腌制。有的食材不需要这一步。焯水就是下锅用清水煮一会（见“煮”一节），有时会加一点点去腥的辅料。焯水的目的是也是去掉直接洗洗不掉的脏东西，例如肉里的血，蔬菜里的有毒物质等。以下是不同食材的焯水方法： 肉类：必须冷水下锅，开小火和水一起煮，中途会冒出血沫，瓢出来到差不多没有了，捞出。总体原则就是肉类不能忽冷忽热，要点： 冷水下锅，突然的热水会堵死小血管，导致放不出血沫； 不能开大火，道理同1； 焯水后不要放冷水，否则肉会缩水变柴。 蔬菜：开水下锅（先把水煮开），放入蔬菜后水就不开了（因为菜是凉的）。焯水时间根据具体蔬菜而定，有的只需烫一下即捞出，如绿叶菜；有的需要等水再次开，如芹菜等坚脆的蔬菜。腌制是将肉、鱼等用调料涂抹、长时间浸泡。腌制的目的，一是提前使食材入味（因为肉在短时间的烹饪中难以入味），二是去腥。腌料（腌制使用的调料）也需要服从这两个目的，例如加盐是入味，加葱姜是去腥；还有一些调料能起到特殊效果，如淀粉、蛋清使肉滑嫩，腌肉用油封口保持水分防止变柴；做不同的菜一般用到不同的腌料。详见“调料”一章。三、调料饭菜的味道取决于调料，所以调料是很重要的，这里专门拿出一章介绍调料的种类与作用。调料的用量比食材小很多（克数量级），需要更精确地控制。很多菜谱提供了精确的调料用量，可以用量具做到精确；但平时炒菜时，由于各种环境、作者个人口味、食物本身的因素，菜谱上写的定量仅供参考，一般无需严格按数值做饭。个人觉得最好是将量具当作辅助品，平时不靠量具凭感觉找量，而是需要的时候注意一下量具显示的数值，久而久之心中就有概念了，看到数值就能大体知道是多少了。盐盐就是氯化钠（NaCl)，平时超市买到的细的精制盐，根据生产来源有海盐（海水）、湖盐（咸水湖的水）、井盐（地下水）、矿盐（盐矿）等几种，区别都不大。值得注意的是一些特殊品种： 低钠盐：掺入氯化钾代替一部分氯化钠，减少钠的摄入，预防高血压等疾病。但是低钠盐有苦味，不好吃； 加碘盐：为预防大脖子病在食盐中加碘，海边人应该吃无碘盐。盐的用量决定饭菜的底味，一定要重点控制好。以下是大体的原则： 一般来说，盐的重量比例在 0.8%-1.2%。以我家为例，正常炒一盘菜是接近一斤重，需要盐约 5g，家里用 2g 的限盐勺，所以平时做菜就是加 2 到 3 勺盐； 有些调料本身带盐，如生抽，它们的盐量应该算进去； 有些预制的食材如卤货、腊肉、咸肉等本身是咸的，它们的盐量应该算进去，甚至有时可以不额外放盐。油类食用油是用植物或动物的有油的地方榨出来的，比如种子、肥肉。分植物油和动物油两类。在常温下，植物油通常为液态，动物油为固态。油是做大部分菜时需要放入的，大部分的油是生的，需要加热烧熟，起锅烧油是做菜的第一步。用来不同的油做出来的菜味道不一样，需要了解一下，下面列举一些常用的： 大豆油：来自黄豆，没有很特殊的味道，适合炖煮。是基本上最常见的油； 花生油：来自花生瓣，个人感觉焦香味重一些，适合炒菜； 菜籽油：来自油菜籽，适合炒菜； 玉米油：来自玉米胚芽（玉米粒根部小小的硬豆豆），适合炖煮； 猪油：来自猪肥肉，特点是非常香，但不健康。超市一般没有售卖，需要自制，只需将猪肥肉扔到锅里加热融化即可（剩下的固体是猪油渣，也可以吃）； 橄榄油：来自油橄榄，味道特殊，西餐常用，价格也贵。还有两类经过额外加工： 调和油：按一定比例混合起来再加工的油，和普通油用法一样； 色拉油：是一大类，指经过额外精炼（脱酸、脱杂、脱色、脱臭）的油，例如大豆色拉油、玉米色拉油，比普通油贵。特点是本身是熟的，不需要烧热，常用于凉拌、烧烤刷油（当然也可以用于普通做菜，油烟味会轻一些）。其他的油类不能作底油，应当做类似于酱油、醋的普通调料，如： 香油：即芝麻油，来自芝麻，在做菜最后滴上几滴； 红油：即辣椒油，是油加入辣椒烧熟后得到的熟油。可以自制，超市也有成品卖； 花椒油：是油加入花椒烧熟后得到的熟油。可以自制，超市也有成品卖。注意不是花椒籽做的油。酱油类酱油是用黄豆或其他豆类发酵而成（有的地方叫豉油），分为两种： 生抽：颜色浅（接近红棕色），咸味重，作用是提鲜； 老抽：颜色深（接近黑色），咸味淡、甜味重，作用是上色。 生抽和老抽各有自己的作用，不可互相替代。酱油的其他变种，它们与酱油作用基本相同： 蚝油：生蚝（牡蛎）为主要原料熬制而成的调味品； 味极鲜：可以理解为生抽里放了味精； 蒸鱼豉油：加了些糖，适合蒸鱼； 鱼露：用鱼虾发酵而成，北方较少使用。酱可以看做酱油的固体状态，作用与酱油相同。 黄豆酱：用黄豆发酵而成，最常见的酱。做咸酱香味的菜用； 豆瓣酱：用豆类加辣椒发酵而成，上面一层有红油。做辣的、口味重的菜时用；（豆瓣酱在北方不常用，这个叫法容易与黄豆酱混淆，一般叫郫县豆瓣酱） 甜面酱：面粉制曲发酵而成，有甜味。做甜酱香菜的菜（例如很多北京菜）用。 酱油和酱都有咸味，做菜时应当把里面的盐算进去，尤其是生抽、味极鲜。醋、酒类醋和酒是类似的物质，这里放在一起。醋主要酿造自谷物，作用是提供酸味，因此一般做菜用的不多，除了需要有酸味的。醋的种类： 米醋：原料为大米； 陈醋：原料为高梁； 老醋：即老陈醋，酿造时间比陈醋要长，酸度比陈醋要高； 香醋：原料为糯米，适合凉拌； 白醋：成分只有醋酸和水，用蒸馏酒发酵化学制成。价格一般很便宜，中餐一般不用。白醋没有颜色，还可以用于清洁、消毒； 果醋：原料为水果，最常见的是苹果醋。酒不仅可以直接喝，在做饭中也非常有用，主要用于去腥，可用于腌制，也可以在正式烹饪中使用。做饭用的酒的种类： 黄酒（料酒）：原料为大米。注意料酒只是一种称呼，指烹饪用酒，中国所谓的料酒一般都是黄酒加了一些其他调料混合而成。 啤酒：原料为麦芽，去腥作用更强，常用于麻辣的菜。注意，一般不能替代料酒； 葡萄酒：原料为葡萄，中餐不用，西餐用。 酒中的酒精会与食物的腥味物质发生反应，剩下的酒精在烹饪时会挥发掉，所以做饭加酒并不会有酒味。味精糖糖主要成分是蔗糖，除了提供甜味，还有很多重要的作用，所以糖也是重要的调料： 适量的糖可以提鲜； 盐、辣椒、醋放太多时，糖可以中和咸味、辣味或酸味； 炒糖色用于上色，代替老抽。老抽颜色发黑，糖色更红亮通透。食糖的原料为甘蔗或甜菜，常见的形态有： 白糖：有白砂糖和绵白糖两种，最常使用； 冰糖：是白砂糖煎炼而成的大块结晶，可以当白糖用。炒糖色一般用冰糖； 红糖：红糖使用较少。 炒糖色的步骤： 冷油下锅冰糖，小火融化； 等出现大泡泡又出现小泡泡、棕红色时，迅速倒入热水，混合均匀； 调大火几秒钟再关火，倒出冷却。炒糖色的关键是倒入热水这一步，过早炒不出来糖色，过晚会有糊味、苦味。另外千万不能用冷水，容易炸锅。 淀粉做菜用的淀粉的成分就是淀粉（碳水化合物），来自粮食，有的地方叫生粉、太白粉。淀粉可以进一步加工成粉条、粉丝、凉粉等作为食材，这里讨论的是作为调料的作用，有： 勾芡：在菜出锅前倒入水淀粉，使汤汁变浓稠，可以使汤汁粘裹在菜肴上，防止盘底水汪汪； 上浆：在食材上粘裹一层很薄的水淀粉，形成保护层，可以让食材保持软嫩。在腌制时使用； 挂糊：在食材上粘裹一层很厚的淀粉浆，成为油炸时的淀粉糊，和面粉作用一样（面粉糊比淀粉糊更硬）。 以上水淀粉或淀粉浆都是指淀粉掺水搅拌得到的悬浮液。一些注意事项： 勾芡时淀粉和水的比例一般是 1:3-1:4，用不同的比例可以勾成薄芡、流芡、糊芡、浓芡； 无论怎么用，都要与让水淀粉尽量与菜或汤混合均匀，尤其是勾芡。混合均匀的技巧是缓慢倒入，边倒边搅拌。 淀粉根据原料有以下种类： 玉米淀粉：提取自玉米胚乳，适合上浆； 土豆淀粉：提取自土豆，适合勾芡； 红薯淀粉：提取自红薯，适合挂糊，不是和勾芡或上浆；辅料中餐常用的辅料是葱、姜、蒜、香菜，还有辣椒。葱有大葱和小葱两种，大葱辛辣味冲，可以切成葱段、葱花，用于炝锅；小葱葱香味重，一般横截切成小葱花，用于点缀或凉拌。姜可以切成姜末、姜片，用于炝锅。姜的味道比较怪，慎用。蒜可以切成蒜末、蒜片，用于炝锅；还可以整瓣放入用于炖煮。香菜一般用于点缀或凉拌。 不是做所有菜都要放所有辅料的，需要具体情况分析。辅料种类放太多会让口味过于复杂，或者影响口感。辣椒有很多品种，有的可以当做食材用，如菜椒、圆椒、尖椒，一般不辣；这里讨论可作为辅料的较辣的辣椒： 小米辣：朝天椒的一种，比较小的红色细长辣椒，辣味比较清爽，是最辣的。通常横截切成辣椒圈； 线椒：比较长的绿色细长辣椒，常用于做干辣椒，陕西菜常见； 杭椒：长相和线椒很像，不如线椒辣，常用于炒菜； 螺丝椒：比较长的皱皮辣椒，有辣的有不辣的，常用于炒菜； 二荆条：四川典型的辣椒，川菜常用，长相和线椒很像，常用于炒菜或做豆瓣酱。 子弹头：朝天椒的一种，长得像子弹头； 野山椒：朝天椒的一种，颜色淡黄，常用于做泡椒。 注意辣椒辣手，尽量不要用手过多接触辣椒，切完不要揉眼睛！辣椒可以加工成很多辣椒制品，可以在做菜时充当辣椒的作用： 干辣椒：鲜辣椒晒干脱水得到，保存时间长； 辣椒面：干辣椒磨成粉，再加一些其他的香料； 泡椒：野山椒或其他辣椒泡在白酒中发酵腌制而成； 辣椒酱：鲜辣椒剁碎与其他香料或食材末混合的酱，需要在锅中烧熟； 剁椒：朝天椒剁碎用盐、酒、蒜末发酵腌制而成； 油泼辣子：陕西特色，热油泼在辣椒面上（加一些花生碎、芝麻、香油等），辣椒用线椒 红油：即辣椒油，见”油类”一节。香料香料是有特殊香味的辅料，在做菜时放一丁点就够了，不能加太多。各种香料的香味都很特殊，是切记不能乱放的，有些香料只有在特殊用途才用，平时一般不放。中餐用的香料很多是中草药，以下列举家庭做饭常用的： 胡椒：分为白胡椒、黑胡椒，不是不同的品种，而是胡椒果不同的成熟阶段。一般是磨成胡椒粉用，黑胡椒的胡椒味冲，白胡椒的辛辣味冲。黑胡椒粉有颜色。中餐用白胡椒用得多； 花椒：生的需要像葱姜蒜一样在油中爆香。也可以用花椒油； 八角（大料）：一种卤料； 桂皮（肉桂）：一种卤料； 香叶：一种卤料； 孜然：烧烤风味、清真风味时用； 小茴香：小茴香的颗粒要比孜然的大很多，颜色偏绿色；其他的不太常用，例如、白芷、丁香、草果、陈皮等等，不再介绍。复合调料： 五香粉：八角、花椒、桂皮、丁香、小茴香磨成的粉； 十三香：五香粉的升级版，在五香的基础上多了干姜、陈皮、山柰、肉豆蔻、砂仁、草果、木香、高良姜； 火锅底料：这里说的是超市卖的固形物的火锅底料，成分是油、辅料和各种香料，可以当调料炒菜用； 咖喱粉：姜黄为主料，加入其他多种香料：桂皮、白胡椒、小茴香、八角、孜然等等。四、烹饪方式一道做好的菜有两个要素： 食材与调料混合； 加热让食材变熟。一切能完成这两个目标的方式理论上都是允许的，至少是能吃的。但是人们探索出来一些固定的套路——如何加热，何时混合何种调料，这些套路这里称为烹饪方式。 如果食物已经满足上面 2 要素，即已是熟的且有调料（或者无调料，吃的时候有蘸料），如预制菜、剩菜、剩下的主食，只需要简单加热即可。最快速的方法是微波炉，也可以蒸（北方方言叫“馏”）。以下讨论需要用火加热的烹饪方式，不讨论生吃、凉拌等。每一种方式的学习逻辑都是，先知道原理，怎么做出来一道菜（即满足上面的两要素），讲清楚这些后，再考虑让饭菜好吃的注意点或技巧。讨论之前首先需要掌握燃气灶的用法。转动灶台上的旋钮可以让电磁打火器点火。打开后，火的大小也可以用灶台旋钮控制。掌握火候是做菜成功的关键，火候可以粗略分为小火、中火、大火，不同的目的要选用不同的火候，将在下面分烹饪方式讲解。另外，做菜时应当灵活控制火候，不要忘记随时可以调节火候的旋钮。以下是一些场景： 烧开水时速度太慢可以把火开大一些； 炒菜时锅里水太少快要糊了，来不及添水，先把火调小点或者端起锅离火。锅盖的作用是保持温度，避免，但是要注意两个问题： 应保持气压正常，可以时不时打开锅盖，否则会锅里的东西容易溢出（俗称“窜锅”），甚至有爆炸的危险。有的锅盖设计了出气口用于平衡气压； 有些食材盖了锅盖会被水蒸气捂蔫，变得不好吃。煮煮是最简单的烹饪方式，一句话总结就是把食材放在水中加热变熟。水的作用是导热剂，使食材均匀受热，防止变糊。煮可以做的菜： 简单的（没有油、清淡的）汤：食材下水加调料如盐、味精、胡椒粉； 简单的（没有油、清淡的）粥：谷物（大米、小米等）下水，不用加调料； 主食：面条、米粉、河粉；饺子、馄饨；等； 煮鸡蛋。煮需要注意的问题： 食材是开水还是冷水下锅，它们的区别只是时间，取决于食材本身是否容易熟；（煮的其他叫法有汆、烫，汆是开水下锅，烫是用开水停掉热源） 涉及多个食材时，放食材的顺序是以是否容易熟为标准，见食材一节； 调料的顺序随意，只要能均匀地分散到水中即可； 火候：在保证安全的情况下能大则大，为了快速让水烧开； 煮东西要注意搅拌，防止食材粘锅烧糊，尤其是主食；但不要过度搅拌，浪费力气还容易把食物弄碎；煮东西要盖锅盖，为了快速让水烧开，但要防止窜锅。蒸蒸就是用很烫的蒸汽将食物捂热。特点是可以不破坏食物的结构和营养，锁住食材原来的味道（煮会流到汤里）。蒸可以做的菜： 主食：米饭、馒头、包子等； 蒸鱼； 蒸海鲜； 水蒸蛋。蒸要使用蒸锅，蒸锅是一口大锅，中间以带孔的蒸片（俗称篦子）隔开，下面加水，上面放食材，最上面用锅盖盖住。加热时，下层的水蒸发为蒸汽透过蒸片的孔到达上层。（蒸锅一般有出气口来平衡气压）蒸需要注意的问题： 最好先把水烧开再放东西，蒸的时间也是从冒热气开始算时间； 一定要注意下层的水量，应该在开蒸之前就预判好水量，不能出现干烧的情况； 可以把液体调料（醋、酱油等）放在碗中置于上层，这样它们也会变为蒸汽使食材入味。最常见的是蒸鱼时放蒸鱼豉油；炒炒是中餐的核心，也是需要重点掌握的。炒是不用水的，要用油，一句话就是把食材放在油中加热变熟。油的作用就是烹饪方式“煮”的水，作用是导热剂，防止食材变糊。虽然炒还是一句话的事，但用油比用水要困难很多，因为油的量比水小很多，油导热很快温度不好控制，而且油比水更危险，所以“炒”比“煮”难很多，要掌握很多东西。在“煮”中提到了 5 条问题，很多可以对应到“炒”，需要详细讲解。 从“炒”开始要涉及到油，请先穿好围裙，打开油烟机，非开放式厨房关好厨房门，防止油点溅到身上，以及减少油烟味。 食材是冷油还是热油。一般来说是热油（冷油的单独说），所以炒菜的第一步都是烧油。油和水不同，还要额外考虑温度，即油温。食用油的燃点（着火点）大约是 300 度，炒菜用五到六成的油温（大约 150-180 度），油温太低有生油味，菜的香味出不来；油温太高菜熟太快容易糊。有肉眼判断方法是刚开始冒小烟，这个时候就可以开始放东西了，下东西会有比较密集的气泡。 油遇到水容易起火，烧油之前一定要把锅中的水弄干！一般的方法是在倒油之前，倒出锅中的水流，再加热空锅，把锅中的水烤干。 此外，盛油的容器里不能有任何水分，掺了水的油就不要用了，防止起火！ 油锅起火应该先切断火源（把灶台旋钮关掉），再用锅盖盖灭（隔绝氧气），千万不能用水灭火！ 千万不要超过 8 成的油温！判断方法是冒出比 7 成油温还要多的青烟。此时很容易着火，只适合饭店师傅大火爆炒（锅里都着火的那种），对技术要求极高，家庭很难控制火候。 烧油时不要开太大的火，要能稳定地控制油温为准。 涉及多个食材时，放食材的顺序也是以是否容易熟为标准，见食材一节； 放调料的顺序有讲究： 辅料和香料需要在油热后就先放入煸炒出香味，包括葱姜蒜、花椒、各种酱， 在这一步之后放食材； 食材炒的差不多熟了开始调味，即放盐、酱油等液体或粉状的调料。酱油等液体调料应从锅边淋入，也是为了激发香味，否则也有生味； 出锅前放不耐热的调料，如香油、花椒油、水淀粉勾芡； 最后放点缀性的辅料，如葱花、香菜等。 这种顺序的原因是： 辅料和香料需要煸炒出香味才能起作用，不像其他调料可以直接使用；如果后放是没有香味的，是生的味道； 盐提前放会让锅中液体渗透压变大，加速食材水分流失，肉会变柴，菜会变蔫； 酱油等液体本质是水，提前放容易着火。 火候：用中火或大火，目的是减少食物熟的时间，防止变老或水分流失；不能用小火；（用大火是爆炒，熟得很快，一定要算好时间，并注意安全） 在烧油时可以适当转锅，使油浸湿更大的锅面，称为滑锅，目的是增大食物与油的接触面积； 要注意不断翻炒，一是防止食材粘锅烧糊，二是让食物均匀受热或沾上调料（没有水的情况下，均匀要困难得多）；但不要过度翻炒，浪费力气还容易把食物弄碎；没有必要盖锅盖，因为炒的时间是很短的，翻炒动作的频率也比煮的搅拌高。 翻炒时也有一些类似于刀工的基本功，例如颠勺、转勺、翻勺等，能提高翻炒的效率，但对家里做饭不是必要的，不再讲了。 炒的过程中太干（冒烟）可能会糊锅（尤其是不含任何水分的食材），一般是火太大、时间太长引起的，补救方法是加少许液体炝锅，可以是水或液体类调料酱油、醋等（如果还没加的话）； 油的量要控制好，油少了不好吃，油多了吃起来很油腻。油多了或少了在炒菜中是不容易补救的。烧 / 炖烧/炖是在炒之后加水再煮一会，使得菜中有汤汁。加水很多得到的是汤，加水较少得到的是炖菜（也叫烩菜），再少的、最后需要收汁的是烧菜。特别地，酱油比较多的（老抽上色）烧菜是红烧。烧/炖与煮的区别是多了一步炒的过程，炒会激发食物的香味，而且有油，味道会比煮更重更复杂。烧/炖的基础是炒和煮，以下是额外的注意事项： 火候：加水之后，应该大火把水烧开。水烧开后应该转小火，让食材缓慢入味。此时不适合大火，会加速水的蒸发； 对于烧菜这种需要精确控制最后的汤汁量的，更需要提前算好加水的量，控制好上述“小火”的火候。水量过多收不起来汁，水量过少需要二次加水，也会影响食材的口感。煎 / 炸煎/炸是让食材持续与热油接触变得焦香金黄，煎用的油较少，只需在食材和锅之间有一层油即可；炸用的油很多（水的量），食材完全浸泡在油里面。家庭舍不得用油一般用煎的方法。 炸东西的油量特别大，是可以重复使用的，例如炸别的东西或用于炒菜。不过要注意： 盛放油的碗一定要擦干，碗尽量密封保存，防止溅入或吸收水分； 多次炸的油对身体有害，不建议多次反复使用。 煎/炸可以做的菜： 煎鱼、炸鱼； 主食：煎饺、煎包，炸丸子，茄盒/藕盒，薯条等； 炸鸡； 煎鸡蛋。煎一般用平底锅，煎的东西也一般比较平整；炸没有这个要求。煎/炸后的食物有的直接吃，有的交给下一步，比如炒、烧、炖菜等。煎/炸需要注意的问题： 也需要把油烧热，一般是六到七成的油温（比炒菜稍高一些）； 有些食材不易煎成型，需要挂糊，可以用面粉、淀粉、蛋液、面包糠等粘裹； 放入食材时要从锅边轻轻放进去，防止油溅出来引起危险。倒油的时候也是； 煎/炸前一定要把食材表面的水分擦干！一定不能带水！ 煎/炸的时候一般不放调料，最多放一些盐或胡椒粉。更复杂的调味交给后续炒、烧、炖的步骤； 火候：一定要小火，火太大会煎糊或炸糊，也容易油温过高发生危险； 煎东西需要定型，等食材的一面形成金黄焦层变硬后才能动，不能经常翻面，这样煎不透食物，煎不成个。要时刻注意翻面的时机，太晚也会煎糊； 炸东西捞出时请擦干工具（如筷子、笊篱）的水分。其他的技巧总结如下： 姜片滑油，可以防止粘锅。烤烤是让食物处于高温的环境中烘烤变熟。是离热源一定的距离，不直接接触，通过空气中的热度加热的。热源可以是明火，也可以是暗火。烤的容器不能用锅（会糊），家庭一般用电器：烤箱、空气炸锅。烤可以做的菜： 烧烤里的菜品：烤肉类：羊排、肉串等，烤韭菜、茄子等； 烤鸭，烤鸡，烤鸡翅； 主食：红薯、披萨、面包蛋糕等烘焙；烤需要注意的问题： 烤的温度由电器的旋钮直接控制，不同食材使用的温度不同：肉类180-230度，蔬菜类、烘焙150-180度，低于150度是低温烘烤，常用于需要保持鲜嫩的食物如鸡腿肉等。 烤箱一般需要预热，为了让食物从一开始就均匀受热。方法：设定好你的目标温度，先不放入食材，空烤五分钟。预热时间不算在烤的时间内； 烤的食材一般会抹上或腌入调料。刷油可以让食物更香。五、清洗与整理工作做完饭，酒足饭饱之后，还有重要的清洗与整理工作。 剩菜的保存方法：应放在干燥低温的环境下，例如放入冰箱冷藏，尽量不要处于室温。冰箱放不开的话，尽量放在阴凉处例如厨房的台子上。最好不要剩菜，剩菜应尽快吃掉； 洗菜刀、菜板、勺子、铲子、筷子：冲洗一下即可； 刷碗：用刷碗的布擦洗。油特别大的话用洗洁精或食用碱； 刷铁锅：用竹锅刷扫洗。尽量不要用洗洁精、钢丝球。油特别大的话用食用碱； 油烟机、灶台：有专门的洗涤剂，喷洒后用抹布擦拭。 洗洁精对身体有害，一定要冲洗干净，不留残留。结束语：关于做饭，除了学一项基本生存技能之外，我纯粹是对做饭比较感兴趣，为什么呢？一是做饭能按照一定的菜谱来，像写代码一样，也有很多熟能生巧习得的经验供你不断提高，有成就感；二是由于个人经历：有段时间经常出学校吃饭，对北京（可能别的大城市也这样）越来越大资本化的餐饮业感到失望，放眼望去满大街都是火锅、烤肉、中央厨房配送的炒菜，整个商场全是熟悉的连锁店名字，大多贵且难吃，于是暗下决心以后也要在家做出好吃的饭菜。做饭是享受生活的一种形式，有一份不加班的工作，能给亲人和自己做好吃的菜，也是一大心愿吧！" }, { "title": "《Kards》原创卡组分享（长期更新）", "url": "/posts/Kards_decks/", "categories": "有趣的事情", "tags": "游戏, 长期更新", "date": "2022-06-11 00:00:00 +0800", "snippet": "《Kards》是冰岛公司 “1939” 开发的二战题材的卡牌游戏，与《炉石传说》非常相似，本人是老玩家，自 2020 年 4 月 15 日公测起开始已有 900 小时。游戏可在 Steam、Epic 等平台免费游玩。我在本帖分享我的原创卡组，都是我亲手组的，经过大量测试胜率较高的，我一直使用的。请注意这些大多不是主流上分卡组，几乎没见有人用过，娱乐性质较多。文章会出现各种牌或游戏机制的外号，请自行搜索百度 kards 吧了解。美意跳费英日联邦导入代码：%%23|190m0Bgv1J0w1xje0z;790o0Y0deL;1VihhVjz;6208思路： 通过大量叠血手段攒到 30 血，通过英联邦斩杀 核心牌：甜心、印度旅、护航队、防御工事、重新部署 重新部署是主要叠血手段，并可以快速找英联邦 其他补刀手段（对面 20 血以上）：主要 10 费兰开斯特打 5 血，司事火炮、金丘都有机会 AOE：小单位靠虎虎虎/炮击，大单位铺场靠季风+虎虎虎/炮击 季风+虎虎虎是最强拖延手段，相当于 8 费铀工程 2 张牌同时到手要求较高，并且亏牌。补足手段是护航队过牌 + 重新部署找牌 不需要带地毯，与此体系有冲突 针对大哥：仙台、海军力量 增加厚度：格兰特坦克，补足英联邦不能斩杀的情况 由于重新部署的存在，以及亏牌的原因，不可以带观察团使用贴士： 优势对局：日本快攻/剃头，美德前线 劣势对局：断水，德意，苏意，小飞机 最优牌序：1 费印度旅，2 费甜心，3 费护航队，4 费重新部署 注意掌握使用英联邦的时机。12费打出去，如果对面没死，相当于空过一回合，容易给自己造成大劣势。另外对面若有回血手段会趁机回血，跳出斩杀线。英法联邦导入代码：%%26|0m0h0Bgv1C1d0w1xje0G0z;0Y1meLgJ2l06;jA1V08gQ;0o卡组相较于英日联邦更加传统，厚度和回血速度上也不如，并且为了找英联邦，有过牌太快的致命缺点。思路： 通过大量叠血手段攒到 30 血，通过英联邦斩杀 核心牌：甜心、印度旅、护航队、假战、海军力量、解放 假战是主要叠血手段，也是过牌手段 其他补刀手段（对面 20 血以上）：主要 10 费兰开斯特打 5 血，也可以 12 费开光辉号航母 + 不列颠尼亚打 5 血，金丘也有机会。极少数情况可以使用假战让对面疲劳斩杀 AOE：传统的地毯、炮击 针对大哥：纵深防御、海军力量 增加厚度：铺场（印度旅、甜心、十字军、黑卫、澳闪等） + 不列颠尼亚，补足英联邦不能斩杀的情况。非常不稳定。 带观察团可以提前启动英联邦、不列颠尼亚等 卡组一费指令比较多，不带 104使用贴士： 优势对局：日本快攻/剃头 劣势对局：小飞机，德意，苏意 最优牌序：1 费印度旅，2 费甜心，3 费观察团并倾泻低费海军力量等牌，4费假战 非常非常怕魔免大哥，再贴海可以直接投了。能撤退的金丘、澳闪可以救一救前线跳费加州人导入代码：%%56|dkbij8bRbmk4j2fGcYfLj4cpbwbEbTbCjvcZ;j3cWj1gJ;k7czbP;jx 2022 年 4 月份加州人被二次削弱，成为历史卡组，不适合现在版本。思路： 前期通过加州人、美国太君、灵车下的蛋赖在前线，给对方压力并顺便跳费。这样比单纯跳费不下单位要好很多。后期享用跳费的成果，下潘兴、开核弹、开大规模部署都可以 特色辅助牌：捕风捉影 前期争取一回合站前线的先机 配合霞飞复制 配合潘兴，10 费可以逃离斩杀 AOE 几乎没有，定位是站住前线，用星条旗、战略轰炸打底线 增加厚度：由于步兵较多，且核心牌加州人也是步兵，可带CS:GO。跳岛触发在加州人身上有奇效使用贴士： 优势对局：各种中后期卡组 劣势对局：各种极端快攻加州保卫战导入代码：%%25|bn1a0n1d0dhD0Bgv0mgs0v0w0h0F1A1D;j406eL;1V1W0x;08jx 2021 年 11 月份加州人被削弱，成为历史卡组，不适合现在版本。思路：通过给加州人贴空投的手段死赖着护脸，伺机扳回胜局。当时版本加州人能扛住指令伤害，为防止加州人被其他手段解掉，特带了规避动作、秘密特工、Ultra等反制牌各一张。辅助手段是给剧毒兵贴空投。此卡组通过大量当时环境下的实验变成此版本，当时非常好用，但随着加州人的削弱也进入历史的垃圾堆了。" }, { "title": "Vim 学习笔记", "url": "/posts/studynotes_Vim/", "categories": "科研", "tags": "学习笔记, 技术", "date": "2022-05-31 00:00:00 +0800", "snippet": "最近和大师兄聊天，他非常推荐我们平时写代码使用 Vim。Vim 也是久仰大名了，但之前一直没实际用过，觉得就是换个文本编辑器，没什么必要。但这几天我决定入坑，学起来用起来。我的参考书籍是《Practical Vim (2nd Edition)》，中文名《Vim 实用手册》。不再单独开读书笔记了。为什么决定用 Vim？Vim 就是一个文本编辑器，但它是以命令行的形式操作的，这是与其他编辑器的直观的不同点。它的设计哲学是只使用键盘、不使用鼠标，目的是提高编辑文本的效率。Vim 的核心思想是引入了多个模式，把原来为防止键位冲突而不得不引入 Ctrl、Shift 的组合快捷键，全部划到单独一个非输入的模式下，变成单个按键的快捷键。可输入文本的称为插入模式，非输入模式称为普通模式。普通模式就是为大量快捷键腾出地方的。另外，Vim 通过一套“语法”将一些简单元素组合为复杂操作，记这种语义的组合比一个硬性规定的快捷键更直观。Vim 还有一些非常强大的命令和功能，可以用非常方便的方式实现。通过一段时间对 Vim 的学习，我发现 Vim 确实很适合代码这种文本，能很好地解决我曾经写代码时烦过、也思考过、但懒于研究的一些重复性劳动，这是我决定学用它的主要原因。 虽然 Vim 是通用的文本编辑器，但主要还是写代码文本方便，写其他类型的文本可能就鸡肋了。以我博客的 Markdown 文件为例，Vim 的不足之处有： 光标移动问题：Vim 和普通的文本编辑器在行的概念上有区别，见“移动光标”一节的 tips。这个逻辑符合代码习惯，却不符合一段段的 Markdown 笔记； 中文输入法问题：中文输入法下是打不出 Vim 命令的，来回切换输入法反而让 Vim 不方便了；（虽然有插件 SmartIM可以解决这个问题，但 VSCode 配置 Vim 插件也很麻烦。） 冗余功能：Vim 中很多提高效率的操作是普通笔记用不上的，例如各种重复操作、复制操作等。 因此在博客等非代码文本中，我会在 VSCode 中禁用 Vim 插件（仅限博客文件夹的工作区），方法是去扩展中右击 Vim，点 Disable (Workspace)。配置 Vim + VSCodeVim 是类 Unix 系统自带的软件，所以我的苹果电脑直接就能用：在终端敲 vim 就能打开了。虽然方便，但它最大的问题是单纯的文本编辑器，没有 IDE 所拥有的运行、调试代码功能，这样写代码和运行调试代码在两个地方，切换很不方便。这个问题可以通过安装插件解决，但想想就很麻烦，而且装了这些东西 Vim 就不纯粹了，我只是想用 Vim 在文本编辑的方便功能而已！我平时最常用的是 VSCode，它的各种插件使其具备了 IDE 的功能，我也早已习惯了这个方便的图形界面环境。所以想到一个好点子：Vim 能不能看成一个插件，仅是对 VSCode 文本编辑功能的扩展呢？这样既能在编辑文本时用 Vim 提高效率，也能在不编辑文本时继续享用 VSCode 直观的图形界面。事实上大部分 IDE 都有 Vim 插件，VSCode 也有！直接去扩展中搜 Vim 安装即可，启用插件后， VSCode 打开的任何文本文件就都开了 Vim。以下开始介绍 Vim 的用法，都是 VSCode 环境中必要的功能，不会涉及不必在 VSCode 中用的 Vim 命令，如保存文件、切换窗口等。模式Vim 主要有 4 个模式，VSCode 左下角会提示当前的模式： --INSERT--：插入模式，和平时输入文本无差别，所以不想用 Vim 的话就一直开着这个模式好了； --NORMAL--：普通模式，Vim 的主要功能； :：命令模式，在普通模式下输入冒号后跟上相应的命令； --VISUAL--：可视模式，类似于 Word 打开了只读文件的效果。经常选则文本使用。 另外，这些模式的光标是不一样的，插入模式是细光标，其他是粗光标（会盖住一整个字符）。如何切换模式可以一图以蔽之：普通模式是 Vim 的主要功能，因此所有模式的“家”，正如其名字一样，打开 Vim 后默认也是普通模式。其他模式都可通过 Esc “一键回城”。其他模式间的切换现在去死记是没有意义的，我会在下文穿插在用到的地方讲。入门级操作：移动光标后进入插入模式这是基本操作，无论什么文本编辑器，最自然的输入方式就是移动光标后插入文本。Vim 下无论是什么模式，移动光标都可以通过上下左右或鼠标点击完成。但一定要养成在普通模式下移动光标的习惯，先移动再进插入模式。有句话说的好，插入模式，最大的职责就是进行字符的插入。普通模式下，Vim 还有另一套按键映射：h = 左，j = 下，k = 上，l = 右。由于其位置比方向键更方便，最好是练习使用这一套键位。 上下左右键位在 hjkl 其实是历史原因造成的，早期的电脑键盘没有方向键。其方便性可能只是巧合。上面是基本的，当然还有大量的移动光标的快捷方式： 按行：0,^,$ 分别为移到行开头、行不算空白符的开头、行结尾； 按单词：w,b,e,ge 用一张图可解释： 行内查找字符：f{char},t{char} 为向后查找本行第一个为 char 的字符，并移动光标到该位置，区别是前者移动到该字符的下一个位置，后者移动到该字符。F{char},T{char} 为向前查找； 跨行查找字符串：/{string}&amp;lt;CR&amp;gt; 为向后查找为 string 的字符串，以回车键 &amp;lt;CR&amp;gt; 标识字符串结束； 寻找括号的另一半：在光标移到括号的一半时，按 % 可以跳转到它的另一半； `系列 ``：跳转到上一次光标的位置 `.：上次修改的位置 `^：上次插入的位置 `{字母}：跳转到标记的位置，标记的方法是在标记位置打 m{字母}，可以标记多个位置（字母作为标识符） 关于行：Vim 中的行默认是真实的行（以行号标示），而普通文本编辑器是实际显示的行。这种不同点会影响 j,k,0,^,$ 等涉及行概念的行为。在这些按键前加 g 会按实际显示的行来。 此外，Vim 的左右无法跨行。移动好光标后，就可以进入插入模式编辑文本了。应注意普通模式进插入模式有很多方法，有的甚至不需要精确移动到确切的插入位置： a：在光标后面进入； i：在光标前面进入； A：在行末进入； I：在行开头进入； o,O：在下 / 上另起一行进入；另外，修改操作 c 也是会由普通模式进入插入模式的，下一节再讲。 不理解光标哪是前哪是后？注意普通模式的粗光标是盖住一整个字符的。快捷修改操作：普通模式下的 “操作符 + 动作”Vim 当然不只为了移动光标方便，这样的话它还多一个模式切换问题，还不如普通的编辑器呢。Vim 有大量快捷的编辑操作可在普通模式下按几个键实现。有些小操作（一般是字符级别的）只需一个按键即可完成，列举如下： x：删除光标处的字符； p：在光标处粘贴；大部分操作分为 “操作符 + 动作” 两部分，这里动作（motion）应理解为操作的范围。这个逻辑类似计算机体系结构讲的指令系统，只是 Vim 对命令做了极大的简化。用法：先敲操作符，会发现光标变成下划线，等待输入动作。再输入动作，输完后操作会自动执行，不需敲回车等。以复制当前单词 yiw 为例，y是操作符，iw 是动作，直接在普通模式下打 yiw 就能立即执行。 因为不需要敲回车，所以后面只能跟一个动作！操作符有： c：修改（change），即删除后进入插入模式； d：删除（delete）； y：复制（yank）； gU,gu：变成大写 / 小写； &amp;gt;,&amp;lt;,=：左缩进、右缩进、自动缩进。动作有两类： 一类是选择的一端固定在敲了操作符的位置，另一端通过移动光标的方式确定。上节介绍的移动光标的按键都可用； 另一类是从光标处向左右匹配某种 pattern 的，叫文本对象。总是由 i,a 和一个标识符组成。以括号为例，i),a) 表示从光标处左右选择被小括号括起来的内容，区别在于前者不包含括号，后者包含括号。标识符有： 各种成堆匹配的符号，取右半边：)（=b）,],}（=B）,&amp;gt;，&#39;,&quot;, ` ` ,t`（表示 HTML 的标签）； w,s,p,e：取一个单词、句子、段落、整篇文章。 由于对一行的操作很常用，Vim 设计敲 2 次操作符表示动作为当前行，如 dd 表示删除当前行。替代方案：在可视模式选中后操作上面方案的问题是，需要快速确定操作，精准定位范围。有时候我想先漫无目的地随便选点东西，浏览一下，再做修改，”操作符 + 动作“ 就不适用了。平时的编辑思维是选中后再做操作，和上面是反过来的。在某些场景下，这个习惯会更方便。前面说到可视模式通常是为选择文本设计的，要进入可视模式选择文本。用法：先按 v 进入可视模式，这个时候就将选择的起点定在此处，再移动光标（包括 hjkl 和各种快捷方式）到终点，最后敲操作符。还有 2 种选择单位更大的可视模式，方便更宏观的操作： V：以行为单位选择的可视模式； Ctrl + V：以块为单位选择的可视模式。可以看到，用可视模式在选择上更加自由，能处理更精准的范围；相比之下，“操作符 + 动作” 只能有一个动作，但是其更快捷。选择哪种方式应视具体场景而定。自动化工具. 可以在当前光标位置重复上一步操作。操作是指上一节的一套 “操作符 + 动作” 或进入可视模式后的操作。移动光标是不算操作。和 . 起类似作用的有（见“移动光标”一节）： ;,,：重复行内查找字符，; 为向前，, 为向后； n,N：重复跨行查找字符串，n 为向前，N 为向后。数字也可以用于指定操作的次数，可以夹在很多地方，特别灵活。用法例如 3j 为向下移动 3 行，d2w 或 2dw 为向右删除 2 个单词；等等。更强大的是键盘宏（Macro），即录制一遍按下的按键，在其他地方重复执行。宏功能本身并不稀奇，Word 等文本编辑器里也有此功能，主要是 Vim 的键盘宏非常方便、简捷。用法：按 q{字母} 开始录制，字母是宏的名字；录完之后按 q 结束，将光标移动到合适的位置后，按 @{字母} 执行宏。其他暂时用不上的功能到现在还没有介绍四个模式中的命令模式。命令模式的命令有点像微软家的 VBA 编程，甚至能写成脚本，这与普通模式的操作相比完全是另一套东西。普通模式下的操作它都能做，但可以实现更系统的、规模更大的操作，因此也带来的打指令字符更多的麻烦。同样的操作在这里有单独的语法，而且命令比单个按键更难记，需要付出更大的学习成本。因此我决定直接舍弃这一部分。其他用不上的列举如下。注意很多是利用了 VSCode 的便利，在命令行的 Vim 中还是不可避免要是使用的。 普通模式下 u 是撤销，但其实没有什么特殊的。我还是愿意按 Cmd + Z 实现； 保存是用命令 :w ，还是习惯 Cmd + S； 命令行中的 Vim 可以开多窗口同时编辑多个文件，直接用 VSCode 的标签页就行了。" }, { "title": "我使用的网站推荐", "url": "/posts/my_bookmarks/", "categories": "生活", "tags": "日常管理, 长期更新", "date": "2022-05-22 00:00:00 +0800", "snippet": "看了其他博主的博客，发现很多写过博文分享过自己平时使用的网站，那我也来写一个吧。我只想分享一些自己体验过的、感觉好的，可能不被人熟知的，就不列那些软文里凑数的了。搜书网站：Library Genesis，Z-library链接：https://libgen.is, https://z-lib.org号称是世界上最大的电子书网站，后者是前者的镜像网站，主要用前者就行。基本什么书都能搜到，英文资源居多，一般来说我想搜一些较有名的书的电子版都会来这里先看看。中文搜书网站：鸠摩搜书链接：https://www.jiumodiary.comLibGen、Z-lib找不到的中文书，可以来这里搜。一般教材或者年代久远的可能在这里找到。这可能是国内最全的搜书网站了。资源下载站：RuTracker链接：https://rutracker.org俄罗斯最大的BT站，性质和国内各高校的PT站如蒲公英、北邮人、极速之星类似，里面有大量软件、影音、书籍等资源。之前搜电影什么的用北理工的极速之星就解决了，后来极速不行了就用这个了，电影都是能搜到的，外挂上中文字幕就可以了。注意这些资源是盗版的，慎用，存在版权问题。使用方法：注册一个账号（必须），搜索资源（搜英文就能搜到），下载种子，用种子下载器下载资源。下载器我用的是 Motrix，比较简洁直接，没有广告。俄语看不懂用网页翻译。信息聚合网站：今日热榜链接：https://tophub.today国内公司开发的信息聚合门户网站，它可以整合呈现很多网站的热榜等信息，而且可以客制化。我已将其设为首页，打开浏览器就能在这个网站看到自己定制的微博热搜、知乎热榜、未名BBS十大、B站热榜、GitHub Trending 等，很方便。地图网站：OpenStreetMap链接：https://www.openstreetmap.org和 Google、百度地图类似的在线地图网站，但是像维基百科一样完全开源的。它的特点在于图例非常严格清楚，有实际严格测绘的出来的质感，没有商业地图软件那些花里胡哨的东西。但个人感觉仅限我这类地图爱好者探索、娱乐用，很不适合平时用作导航。画图网站：Diagrams.net链接：https://app.diagrams.net可以在线画各种各样的流程图，可以导出成各种格式，非常方便。论文里画网络、流程图什么的完全可以用它。美国国会图书馆：Library of Congress链接：https://www.loc.gov相当于美国的国家图书馆，能搜到很多历史性的资料（主要是 Public Domain），包括影像、报纸、杂志、新闻、录像、网页等等，对爱好历史的人是福音！重点是不光能搜到美国的东西，由于它包含了联合国教科文组织的 World Digital Library，所以中国的东西也有很多！我甚至在这上面搜到过一两百年前的原始曲谱，被电子化为 PDF 了！这个网站最大的用处就是查一些 Google 上难以直接搜到的历史资料。举个例子，历史区的 up 主们做视频会用到大量素材，去这里搜就对了。当然，它的内容简直太广泛了，只要心中有这个网站，每次搜都会有惊喜的。互联网档案馆：Internet Archive链接：https://archive.org美国的非营利性质的数字图书馆，和 Library of Congress 差不多（主要是 Public Domain），也可以搜到各种各样的历史性资源。里面最有意思的功能是 Web Archive，它能按照时间线看到很多网页之前的样子。搜谱网站：Ultimate Guitar链接：https://www.ultimate-guitar.com外网应该是最常用的吉他搜谱网站，大部分英文流行歌曲都能在这里找到吉他谱，包括和弦谱、完整六线谱、能用打谱软件（Guitar Pro）编辑的格式化吉他谱。不过中文资源很少。在我主听摇滚、乡村的一段时间里，总是能在这个网站找到能让我在吉他上实现的东西。搜谱网站：国际乐谱库链接：https://imslp.org专门分享 Public Domain 的音乐，一个类似于维基百科的系统。能涵盖几乎所有古典音乐，在这里能找到录音和各种版本的曲谱，每个作品都是按作曲家规整好的，不会存在重复。搜古典音乐的谱子就去这个网站。" }, { "title": "编配：《一步之遥》手风琴四重奏", "url": "/posts/accordion_transcribed_Por-una-Cabeza/", "categories": "音乐", "tags": "手风琴, 乐谱, 音乐编配", "date": "2022-05-14 00:00:00 +0800", "snippet": "乐谱已上传至 MuseScore 和 B站，有实时的声音与乐谱对照。此编配主要参考 Jian Shi 的小提琴、手风琴、钢琴的三重奏版本，左手和弦使用了仲凯老师的版本。我将小提琴和钢琴部分分配给适合的手风琴各声部；也通过听原曲，加入了一些原曲的细节。我没有听过实际效果，如果大家在排练过程中发现不科学的地方，欢迎和我说！乐谱PDF请扫码获取或访问：https://disk.pku.edu.cn:443/link/D5F5BEB9667011C9AAD63F6FE3409E2E，密码 3Og9。" }, { "title": "读书笔记：《终身机器学习》第 1 章：终身学习简介", "url": "/posts/readingnotes_Lifelong-Machine-Learning_%D0%A1hap1/", "categories": "科研", "tags": "读书笔记, 《终身机器学习》, 机器学习, 持续学习", "date": "2022-05-10 00:00:00 +0800", "snippet": "开新坑了。我的研究方向是持续学习，这就是一本少见的系统讲持续学习的书，作为稀缺的资源我也把它涉猎一下吧。此书不够前沿，我不打算特别细致地读，这些笔记也只是总结书的逻辑和一些亮点。纵观全书，感觉就是一篇写的很长的综述，甚至和综述论文结构都是一样的。至于是不是在水，等我看完之后评价。后面几章格外关注 NLP 和强化学习领域，暂时不打算看，我计划是看到第 5 章。书籍信息Lifelong Machine Learning (Second Edition) 出版年月：2018 年 8 月 作者： Zhiyuan Chen： Google 研究员，可能是后者的学生 Bing Liu：伊利诺伊大学芝加哥分校，教授 到底是持续学习？还是终身学习？有没有感觉人工智能这个领域，好多词语很混乱诶，分不清！就我所知持续学习还有另一个名字——终身学习，而且目前论文也会经常出现这个词！本书给了一个统一的逻辑区分它们，本章只讨论一下终身学习和持续学习，其他的词汇在第 2 章专门讨论。持续学习和终身学习的思想是一样的，区别仅在于名词出现的早晚：终身学习在 1996 年就由 Thrun 提出来了，之后的文章也用终身学习这个词；直到近些年深度学习社区才开始使用持续学习这个词，并且把关注点转移到了灾难性遗忘问题上。目前只看这两个词，其他的相关术语在本书第二章专门介绍。所以终身学习的涵盖范围（在方法上，而不是概念上）比持续学习广很多，本书的标题就是终身学习，持续学习只是作为其中的一个章节。 除基于深度学习的持续学习以外，终身学习还涉及很多类的方法与领域，散布在本书的各章： 第 3 章：终身的监督学习（Lifelong Supervised Learning） 第 4 章：持续学习（Continual Learning） 第 5 章：开放世界学习（Open-World Learning） 第 9 章：终身强化学习（Lifelong Reinforcement Learning）在 1.3 节给出了整个终身学习各个领域的简要历史，可以参考。所以，看看这本书不仅能把 AI 这类思想来龙去脉搞清楚，兴许也能从老终身学习方法中借鉴到一些灵感呢！另外，以作者做研究的角度，终身学习出于很多原因在之前一直不温不火，而且更多地用在 NLP 上（不像现在持续学习多数用图像数据集），见第 8-9 页。又是被啰嗦过一万遍的背景！为什么要终身学习？这种问题每看一篇持续学习的论文都要七嘴八舌地搬出一套逻辑，都看烦了诶！但我还是想说一下作者的逻辑！终身学习的反义词，作者选用的词语是独立式学习（isolated learning）。纵观本书开头的长篇大论，就是从三个方面阐述了持续学习的重要性： 世界：世界上万事万物都是有联系的； 人类：人类的学习从来不是独立地从头学习，而会记住之前的知识；终身学习是实现人类级别智能的必经之路； 实际应用：独立式学习消耗大量数据，如果能吸收在之前其他学习过程学到的知识，就再好不过了。作者举了几个应用例子。第一个是 NLP 中的语义分析，例如分析大量用户评论中对商品的评价。互联网公司常需要大量处理这种任务，随着任务的增多，新东西其实越来越少，比如商品的属性就那么多，用户出现的评价词也那么多，所以如果能把之前见过的东西利用起来，总是比从头学习要好的。第二个例子是自动驾驶，想想就知道，车肯定是平时在路上不断地碰见新路况慢慢学起来的（又不能把之前的忘掉），而不是给它看到所有情况让它一口气学完，这种任务天生就不能独立式学习。第三个例子是聊天机器人，人类说的话是无穷无尽的，总是能碰到没见过的对话，人类的处理方式通常是结合自己学过的知识猜出新话的意思，而很多”人工智障“的机器人就傻傻不知道做什么了！（之前 B 站的梗联通客服图灵测试给大家一乐～）所以聊天机器人也必须持续学习起来！终身学习定义这里只能给一个概念上的、形而上的定义，因为终身学习的研究繁多，只有统一的思想，而没有统一的数学框架。下面就像《人工智能》课程讲的各种 agent 一样，一张流程图即可描述：独立式学习很简单，目标就是一个任务 \\(T\\)，用一个数据集 \\(D\\)，用学习算法学到一个模型，再去应用（测试）。终身学习的目标是一系列任务 \\(T_1, \\cdots, T_N\\)，在处理任务 \\(T_{N+1}\\) 时，不仅用它的数据集 \\(D_{N+1}\\)（图中①），还利用了之前存下的知识（图中②）。终身学习多了一个部件用于存储过去的知识，称为 Knowledge Base（KB）。此时应当把新任务的知识存到 KB 中（图中④），为下一个任务作准备。这样学习到模型后再去应用（测试新任务，图中③），应注意这里模型只有一个在不断改进，而不是从新开始学。除了以上最基本的终身学习流程，很多终身学习模型会涉及很多不同的细节，作者也总结到图里了： 有的终身学习还要求能够自动发现新任务（图中⑤），例如无人驾驶、聊天机器人都有这需求。这其实是一件很困难的事。 还有的在应用阶段也会发现一些新东西出来，反馈给 KB （图中⑥）。 KB 可以理解为是静态的且复杂的数据库，里面都是生数据，有时需要检索工具去发掘自己需要的信息。这就是图中多出来的一个模块 Task-based Knowledge Miner（TKM）。 某些复杂的系统需要对任务调度管理，就是图中的 Task Manager（TM）。存储了什么知识？KB 是存储过去知识的地方，也就是体现终身学习思想的核心模块。它里面存了些什么呢？当然是模型设计者决定的，但总结起来常常是这几种： 直接信息（Past Information Store）：即之前任务中的直接训练结果或模型的直接输入输出，例如： 之前任务用的数据 \\(D_1,\\cdots, D_T\\)（持续学习中称为重演数据） 之前任务模型的输出（持续学习常用在蒸馏损失） 之前任务训练的整个或部分模型 比直接信息更高一层的元信息（Meta-Knowledge Store/Miner）：这里说的就是元学习的思想 知识推理机（Knowledge Reasoner）：这比前一个还厉害，不仅能把直接信息归纳成元信息，还能自我理解推断新的信息出来。我觉得这就有点像是吹牛的东西了。作者另外提到了全局知识和局部知识的概念，和他们自身科研项目有关，感觉意义不大。另外，本书多次强调好的知识应该是什么样子：首先应当保证过去知识是正确的、无偏置的（正确性），才能考虑此知识是否适用当前的新任务（适用性）。也是一种观点吧，但就是太笼统。本章最后讲了一下终身学习的评估方式，写的很水，没什么干货，就不放了。" }, { "title": "乐谱汇总：我拉过的手风琴曲（长期更新）", "url": "/posts/accordion_sheet_music_list/", "categories": "音乐", "tags": "手风琴, 乐谱, 长期更新", "date": "2022-05-06 00:00:00 +0800", "snippet": "本文列举了我拉过的手风琴曲，方便个人翻阅。我将将按照我的这篇文章乐曲风格的类别整理。有一些乐谱来源未知，但我尽可能附上乐谱来源。如无说明，均为手风琴独奏。所有乐谱均可在我的 GitHub 仓库 pengxiang-wang/accordion-sheet-music 下载。流行音乐已练熟： He’s a Pirate（电影《加勒比海盗》主题曲）：来自[风流先森]，演奏视频 （美剧《权力的游戏》主题曲）： Westlife - You Raise Me Up：四重奏四声部（社团合奏）正在练： 李健 - 贝加尔湖畔：自己编配的，见这篇文章古典音乐已练熟： 亨德尔 - Arrival of the Queen of Sheba（亨德尔清唱剧《所罗门》交响选段）：四重奏四声部（社团合奏） 埃尔加 - 威风凛凛进行曲（Pomp and Circumstance No.1）：四重奏四声部（社团合奏）正在练：暂无苏联：红歌、小调歌曲已练熟： 喀秋莎（Катюша)：扒谱 [amarcordeon] ，演奏视频，我的演奏视频 斯拉夫女人的告别（Прощание Славянки）：国内广为流传的版本 胜利日（Лень Победы）：来自苏联的手风琴书籍《Этих дней не смолкнет слава》 灯光（Огонёк）：来自苏联的手风琴书籍《Песни огненных лет》正在练： 黑皮肤姑娘（Смуглянка）：来自苏联的手风琴书籍《Этих дней не смолкнет слава》 乌拉尔的花楸树（Уральская рябинушка） 在满洲里的山岗上（На сопках Маньчжурии）：自己编配的，见这篇文章 茶炊-火绳枪（Самовары-Самопалы）：来自苏联的手风琴书籍《Песни огненных лет》 神圣的战争（Сващенная Война）：来自苏联的手风琴书籍《Песни огненных лет》斯拉夫国家：民歌已练熟：暂无正在练：暂无中国：民歌已练熟： 夜来香：二重奏二声部（社团重奏）南美：探戈已练熟： 皮亚佐拉 - Nightclub 1960：四重奏四声部（社团合奏）正在练： 一步之遥（Por una Cabeza，电影《闻香识女人》插曲）：仲凯老师版本法国：香颂、谬塞特已练熟： 人群（La Foule） La Valse d’Amelie（电影《天使爱美丽》插曲）正在练： 暴风雨（Bourrasque）爱尔兰传统音乐 几乎所有的爱尔兰传统音乐在 Tune Book 中都可以找到，我也是按照这本书练习的。以下曲目均来自此书。这些爱尔兰音乐通常是一个一个的小 piece，演奏下来可能只要一二分钟。这本书的乐谱只标注旋律与和弦，比较灵活，可以在各种乐器如小提琴、风笛、曼陀林、吉他上实现，当然大多数都可以用手风琴拉。已练熟： Kesh Jig Morrison Jig Swallowtail Jig The Irish Washerwoman (Jig) The Maid Behind the Bar (Reel) Smash the Windows (Jig)正在练：暂无美国：波尔卡、蓝调、乡村待更新。" }, { "title": "手风琴流行音乐编配", "url": "/posts/accordion_popmusic_arranging/", "categories": "音乐", "tags": "手风琴", "date": "2022-05-02 00:00:00 +0800", "snippet": "我目前拉手风琴的模式是按照曲目练习，一首曲子拉完，接着下一首。这和弹吉他完全相反，吉他独奏较难（除指弹外），作为伴奏乐器，在我学会了和弦按法后就迫不及待地想玩起来。什么叫玩起来？就是随性地去实现自己喜欢的音乐（主要指流行音乐），随性就是不需要刻意地正式地练习。对吉他弹唱来说，只要把一首歌的和弦进行搞明白，再配上适合的扫弦或分解和弦（节奏型），稍加练习就可以唱起来了。手风琴这个乐器也可以玩起来，正如我这里提到过的，它能实现完整的“旋律+和弦”。它是不仅能用左手解决伴奏，还能加入右手变成独奏，不用人去唱。我还能听音弹出旋律，所以非常适合用手风琴这样玩。可是之前几乎没考虑过这件事情，因为感觉和流行音乐不太搭，音色乍一听和流行音乐不那么契合，节奏型也没有那么多样。后来看了很多[风流先森]发布的视频，觉得用手风琴这样玩完全没有问题。这里就先开个坑，让我慢慢去探索吧！接下来要讨论的并不是让手风琴在一个乐队或者与他人合奏，充当一个不痛不痒的乐器，而是想一个人把一首流行歌快速掌握并拉下来，自娱自乐。和弦、节奏型以及旋律的处理，这三个要素组成了一首歌，以下就按这三点讨论。 我暂时只能做一些简单的尝试，待阅读更多资料后，再做进一步的更新。和弦手风琴处理和弦是非常方便的。一是它左手就有现成的和弦，按一个键就能实现；二是每个调和弦的相对位置不变，左手的安排是固定的，转调时只需上下串一下把位（类似于吉他加变调夹，但是同一首歌中吉他总不能现场去调变调夹，所以转调只能手动）。如果对手风琴左手键位非常熟悉，就能记住不同调的各级主要的和弦，这直接解决了我玩吉他的一个痛点！流行歌曲都有固定的和弦进行。有的歌曲通篇只有一个和弦进行重复，有的歌曲没有固定 pattern，但也是那几个和弦来回倒，顶多需要处理一些歌中间连接处的 solo（可能会在另一个调上用不同的和弦），这就是所谓的套路（参考官大维的视频：为什么流行歌听起来都这么像）。我在这篇文章中收集了很多流行歌的和弦进行，方便查阅。最简单的流行音乐只需用到大三和弦、小三和弦、属七和弦。稍复杂一点的会用到大七和弦、小七和弦、减三和弦。还有一些起装饰作用的挂四和弦、挂二和弦，带引申音的六和弦、九和弦、十一和弦等。大三和弦、小三和弦、属七和弦、减三和弦分别在手风琴左手下方的四排，一个键就实现了。事实上大三、小三、属七才是必备的和弦，编配完全可以搞简单点，比如用大三替代大七，小三替代小七，挂留和弦直接右手加个四音或二音，带引申音的一律不管引申音。如果想更精细一点，可以参考下面的实现方式： 大七和弦的实现方式是以小三和弦替代，加上根音。以 Fmaj7 为例，Fmaj7 = F + Am。小指按根音 F，食指跨到 Am 上。（参见仲凯老师的这个视频） 小七和弦的实现方式是直接加上七音。以 Dm7 为例，用小指去够C（七音）。（我个人是很难一起按出来，那就分解和弦是交替到小指上。） 挂留和弦的实现方式是直接加上四音，以 Csus4 为例，用小指去够F（四音）。要营造四音转三音的效果，一开始就不要出现三音，即先无名指加小指（C + F），再无名指加中指（C大三和弦）。挂二和弦同理。左手节奏型最简单的方式就是左手和弦一按到底了，这样出错风险低，但是也太蠢了。手风琴常用的几种节奏型：4/4 拍的 1大2大，3/4 拍的 1大大2大大，有时候就和歌曲很不搭。流行音乐中经常用切分音节奏型（参见各种吉他扫弦节奏型），还有吉他的分解和弦，都很难在手风琴上实现。所以这里我暂时也不知怎么办，只能凭凭感觉。但是有些东西用手风琴左手实现起来很方便。一个是低音 bass 线，一个是转位和弦（改变了根音的和弦），它们都要用到和弦相关的音（不太专业，我记得乐理里有个名词的！），在手风琴左手中都在和弦根音的旁边，很容易找到。右手旋律的处理待更新。" }, { "title": "浅谈手风琴的各种风格（附个人推荐资源）", "url": "/posts/accordion_style/", "categories": "音乐", "tags": "手风琴", "date": "2022-04-28 00:00:00 +0800", "snippet": "手风琴一直被加上了很多刻板印象，例如气突苏、土掉牙的乐器、大爷大妈喜欢等等。国内大部分人对手风琴可能的认知是：老一辈人还是毛主席时代部队里文娱活动的印象；中年家长认其为小孩学乐器考级的一个选择；年轻人可能压根没咋听说过，觉得是洋东西、太小众（我就曾是这个年轻人）。手风琴不只有苏联风情、毛时代红歌风情，这些刻板印象很多是历史政治因素造成的。手风琴作为和声乐器，理论上能适应大部分“旋律 + 和弦”的音乐，甚至可能比吉他适应范围还广（吉他很难兼顾和声与旋律）。我也有一段时间迷恋苏联歌曲，也从社团大佬了解到他们当年考级的经典曲目（例如《牧民之歌》……），但手风琴世界里总有大量未知的风格待探索，会发现这个乐器的魅力。中文互联网关于手风琴风格的资料过少，且手风琴爱好者们通常是乐曲导向、而不是风格导向的，喜欢听到一首好听的曲目就直接上手，不作系统的了解。（其实其他乐器也是如此，但也有很多反例，例如吉他上的蓝调即兴都是学习原理后自由发挥，而不是学习特定的乐曲。）因此我在这里整理了一下我了解的手风琴风格，列举一些经典曲目，也附上一些参考资料，供感兴趣的朋友参考（不了解的同学也可以看啊，就当音乐鉴赏嘛～）。流行音乐这里指的流行音乐包括但不限于各种流行歌、电影主题曲、游戏或动漫主题曲。这些歌曲耳熟能详，易被接受。有的歌曲适合手风琴的音色，而很多歌曲不属于手风琴但是经过手风琴的编配依然好听。央视春晚李健与手风琴家吴琼演绎《贝加尔湖畔》手风琴最快速的圈粉方式就是拉流行音乐了（此规律也适用于多数乐器），在视频网站上它们的播放量也是最高的。B站大部分有名的手风琴 up 主喜欢编配流行音乐的手风琴演奏，即使他们是专业的或者有相当厉害的技术，例如：风流先森、仲凯手风琴、小王的手风琴、修仙大神李小肠、刘宽音乐工厂。YouTube 上则有更多：Moshe Zuchter、Jo Brunenberg、amarcordeon、CrazyAccordion Trio、Just Duet曲目推荐： 贝加尔湖畔：仲凯老师版本，我的编配版本 漠河舞厅：仲凯老师版本，乐队版本（伴奏有手风琴 美剧《权力的游戏》主题曲：仲凯老师版本，北大手协合奏版本 电影《加勒比海盗》主题曲：风流先森版本 《塞尔达传说：荒野之息》主题曲：风流先森版本 We Are Young：CrazyAccordion Trio 三重奏版本古典音乐伦敦手风琴交响乐团（London Accordion Orchestra）手风琴由于诞生时间较晚（已是古典乐的浪漫主义晚期），以及音色尖锐、某些特殊的社会属性的关系（这里不再展开讲了），一直没有入流严肃的古典音乐，即没有多少古典曲目诞生时就为手风琴设计，在交响乐等乐团中也很少见到手风琴的存在。这里所指的是用手风琴去拉一些古典曲目：事实上有很多古典作品在手风琴的音色也是很好听的，而且别有一番风味。很多曲目也是严肃的手风琴正式演出的经典曲目。曲目： 维瓦尔第《四季》小提琴协奏曲：《夏》第一-三乐章，《夏》第三乐章（风暴）二重奏，《冬》第一乐章 巴赫 《d 小调托卡塔与赋格》（BWV 565）：巴赫管风琴曲的代表作。Sergei Teleshev 版本，北大手协二重奏版本 勃拉姆斯《匈牙利舞曲第五号》：手风琴家 Martynas 专辑录音版本 肖斯塔科维奇《第二圆舞曲》：仲凯老师版本 《阿斯图里亚斯的传奇》：是古典吉他著名曲目，也可手风琴演奏。Gevorg Gasparyan 版本，北大手协独奏版本苏联歌曲手风琴可以说是工人阶级的乐器，拉手风琴的热潮起源于苏联，后来才传到同为社会主义国家的中国、朝鲜。苏联歌曲的特点：内容上红而专，旋律上坚定而带有忧伤（喜欢用和声小调），融合了社会主义意识形态和俄罗斯民歌风格。亚历山大红旗歌舞团B 站存在“精苏”文化，上面苏联歌曲资源非常丰富，也有很多 up 主用手风琴演奏，知名的有毛茸茸的热水袋。另外，推荐苏联的国家级官方乐团——亚历山大红旗歌舞团（上图），在配器上常使用手风琴。你可以在欣赏歌曲同时，感受手风琴音色在苏联歌曲中的魅力。曲目推荐： 喀秋莎（Катюша)：考级版本，小王版本（技巧较多），本人版本（扒谱amarcordeon 版本） 斯拉夫女人的告别（Прощание Славянки）：广为流传的版本（刘宽老师演奏） 莫斯科郊外的晚上（Подмосковные Вечера）：朴震老师版本，Вячеслав Абросимов 歌唱版本（手风琴伴奏） 乌拉尔的花楸树（Уральская рябинушка）：苏联电视台版本（手风琴伴奏） 黑皮肤姑娘（Смуглянка）：电影原版（伴奏有手风琴） 在满洲里的山岗上（На Сопках Маньчжурии）：本人版本（扒谱Sergus 版本） 红军最强大（Красная Армия Всех Сильней）：毛茸茸版本（基于红旗歌舞团合唱版） 为了你，祖国母亲（За тебя, Родита-мать）：这首歌是现在俄罗斯的柳拜乐队新创作的，是苏联风格。仲凯老师版本中国歌曲我国在毛主席时代非常流行拉手风琴，手风琴考级中的中国曲目大部分来自那个时代。这些中国歌曲有红色革命歌曲，也有很多民歌，如草原主题、西北、新疆主题等。中央音乐学院录制过很多考级曲目示范视频，其中有很多手风琴大师：杨屹、陈伟亮等。曲目推荐： 草原牧歌：手风琴考级三级曲目，考级示范版本（杨屹老师演奏） 牧民之歌：手风琴考级五级曲目，考级示范版本（杨屹老师演奏） 弹起我心爱的土琵琶：刘宽老师版本，仲凯老师重奏版本 保卫黄河：手风琴考级九级曲目，考级示范版本（杨屹老师演奏） 我为祖国守大桥：手风琴考级五级曲目，考级示范版本（杨屹老师演奏） 打虎上山（样板戏《智取威虎山》选段）：手风琴考级六级曲目，仲凯老师版本 花儿与少年：手风琴考级三级曲目，考级示范版本（杨屹老师演奏）斯拉夫国家：民歌、舞蹈 本节受语言限制，我了解程度不够，无法作系统的介绍。待俄语有所长进后更新。此节将斯拉夫国家的民歌与舞蹈单独介绍，它是苏联风格的根基。斯拉夫国家民族众多，仅俄罗斯就有几百个民族。他们的民歌有共同特征，也在很多方面有巨大差异。俄罗斯民族舞蹈一般非常欢快，很多曲调还有渐快的部分，如卡林卡。恰斯图什卡（Часту́шка）是一个典型的代表。曲目推荐： 卡林卡（Калинка）：1860年，俄罗斯民谣。苏联版本（伴奏有手风琴） 货郎（Коробейники）：1861年，俄罗斯民谣。作为俄罗斯方块背景音乐。仲凯老师版本 小苹果（Яблочко）：20世纪10年代，水手舞。舞蹈表演（伴奏有手风琴），普通俄罗斯人聚会家中跳舞（手风琴伴奏） 芭勒娘舞曲（Барыня）：手风琴独奏，手风琴独奏版本 哥萨克跨过多瑙河（Їхав козак за Дунай）：乌克兰民歌。Sergio Gladkyy 版本 妈妈我要出嫁（Как хотела меня мать）：白俄罗斯民歌。二重唱版本（手风琴伴奏） 你就是忽悠了我（Ти ж мене підманула）：乌克兰民歌。老爷爷版本南美：探戈南美音乐有名的就是探戈（Tango），探戈舞曲很适合用手风琴演奏。不过请注意，探戈常用的是班多钮手风琴（Bandoneón），主要在阿根廷等南美地区流行。阿根廷作曲家皮亚佐拉（Piazzolla）是南美探戈的代表人物，也是一位（班多钮）手风琴演奏家。他的贡献就是将探戈音乐从通俗的舞曲升华为具有艺术性的古典音乐，让探戈登上了大雅之堂。这也导致了探戈经常是在音乐厅里演出，比其他民间舞曲看起来更“高级”更优雅。曲目推荐： 皮亚佐拉《自由探戈》（Libertango）：皮亚佐拉最有名的作品。俄罗斯爱乐团版本，皮亚佐拉本人演奏版本 化装舞会（La Cumparsita）：最经典的探戈曲之一，乌拉圭的文化代名词，“第二国歌”。音乐会版本，纯手风琴版本 一步之遥（Por una Cabeza）：电影《闻香识女人》插曲。电影原版，仲凯老师版本 就在半明半暗间（A Media Luz）：Brunenberg 版本 火之吻（El Choclo）：音乐会版本（手风琴协奏曲），amarcordeon 版本法国：谬塞特法国歌曲称为香颂（chanson），就是法语“歌曲”的意思。在很多人印象里，手风琴和法国也密不可分，因为听到很多法式浪漫的曲子是手风琴的音色。这种“法式浪漫”的刻板印象可以归结为一种音乐风格——谬塞特。Édith Piaf 和她的手风琴家谬塞特（musette，又称 bal-musette），于 1880 年代在巴黎流行，最开始是用风笛等乐器演奏的舞曲，后来演变为手风琴伴奏，逐渐成为法国流行文化的一个代表。二战前后，歌手伊迪丝·琵雅芙（Édith Piaf，如图）演唱谬塞特歌曲一举成名，直至现在她可能仍是法国最令人熟知的国宝级歌手（你可能不知道这个名字，但一定知道《玫瑰人生》这首歌！）。推荐 YouTube 上一位法国姐姐：Lucy Riddett，她的频道专注于法国曲目的演奏。曲目推荐： 伊迪丝·琵雅芙 - 人群（La Foule）：原版歌曲（伴奏有手风琴），手风琴独奏版本 La Valse d’Amelie（电影《天使爱美丽》插曲） ：原版手风琴独奏 伊迪丝·琵雅芙 - 玫瑰人生（La Vie en Rose）：手风琴独奏版本，有其他伴奏的手风琴曲，演唱版（手风琴、吉他等伴奏） 暴风雨（Bourrasque）：amarcordeon 版本 巴黎的快乐时光（Happy Days in Paris）：French Café 版本 谬塞特女皇（Reine de Musette）：手风琴家 Yvette Horner 版本爱尔兰传统音乐近年来爱尔兰的文化复兴，使得其音乐越来越被世人知晓，例如 U2、小红莓乐队、恩雅等流行音乐家，以及游戏中使用的越来越多的凯尔特音乐。本节讨论的是其传统音乐。爱尔兰圣帕特里克节上的手风琴在传统音乐中，除了小提琴外，最常用到的就是手风琴了。它们也多数是舞曲：吉格舞（Jig）是 6/8 拍的，速度非常快；里尔舞（Reel）是 4/4 拍的，也比较欢快；还有很多其他欧洲舞融入到爱尔兰风格中的。爱尔兰国内的音乐组织出版了一本 Tune Book，汇总了几乎所有的爱尔兰民间旋律，是按照上述舞曲的类型分类的，非常值得参考（下面列举的曲目在此书中都可以找到）。爱尔兰的音乐喜欢用五声音阶，风格与中国民乐有些相像，给人心里暖暖的感觉。拉这种典型的民间的、欢快的舞曲调调，一定不能搞得太严肃，像拉古典音乐、皮亚佐拉那样。曲目推荐： 爱尔兰洗衣妇（The Irish Washerwoman）：吉格舞。 Brendan Kavanagh 示范 Kesh Jig：吉格舞。传统乐队版本 Swallowtail Jig：吉格舞。小提琴版本 吧台后的侍女（The Maid Behind the Bar）：里尔舞。Tom Willis 版本美国：波尔卡、蓝调、乡村美国作为欧洲的新大陆，手风琴一经发明就随移民传过去了。美国发展出了很多自己的流行音乐，在其中手风琴也起到的重要作用。波尔卡（Polka）也是一种舞曲，2/4 拍的，节奏非常欢快。本身是欧洲（特别是波兰、捷克等地）的舞曲，但在这里我归到美国，因为波尔卡在美国有很广泛的影响。我们有时候听到美国的一些很欢快的 Party 音乐（例如有名的啤酒桶波尔卡），不同于欧洲优雅的华尔兹、芭蕾舞等，就是波尔卡风格的。这些曲子通常由波尔卡乐队演奏，主要乐器是手风琴。蓝调（Blues）在我们印象里一般用的是吉他、电吉他、口琴这类的乐器，实际上手风琴也可以演奏蓝调，甚至在历史上是演奏蓝调的主要乐器之一（参考这篇文章）。用手风琴演奏蓝调类似钢琴上的 Boogie-Woogie。乡村音乐（Country）中也有手风琴的身影，虽然其常用乐器是吉他、小提琴。在上世纪五六十年代美国的电视节目中可以见到手风琴伴奏的乡村音乐（见下图）。由于乡村音乐旋律和和弦偏简单，用手风琴拉起来也简单，所以一般很少独奏，都是与乐队进行配合。乡村音乐的经典组合 Carter Family 中出现的手风琴曲目推荐： 单簧管波尔卡（Clarinet Polka）：Walter Ostanek（加拿大波尔卡之王）版本 啤酒桶波尔卡（Beer Barrel Polka）：Frank Yankovic (美国波尔卡之王）版本，波尔卡乐队版本 蓝调手风琴家 Clifton Chenier：Accordion Boogie，Zydeco sant pas sale，Bon Ton Roulet You are My Sunshine：Frank Yankovic 波尔卡版本 Woody Guthrie - This land is Your Land：美国最有名的民谣、爱国歌曲之一，属于乡村音乐。二重奏版本" }, { "title": "MacOS 快捷键整理", "url": "/posts/Mac_shortcuts/", "categories": "其他", "tags": "技术", "date": "2022-04-27 00:00:00 +0800", "snippet": "引言快捷键是一个好东西，解决了很多用电脑时的痛点。但是去百度、B站、知乎等搜一下，就有大量带有营销包装性质的文章或视频。个人认为看这些东西也就图一乐，更多的人是在看节目/满足自己的好奇心/获得产品使用的幸福感，很难做到记住并实际使用。这些营销号也是利用了信息不对称，其实所有快捷键都在苹果官方支持中列出来了。这有点类似 CS:GO 的投掷物，后者也是在网上有大量的教程，但那条弹幕总是少不了：“进我的收藏夹吃灰吧！”要让快捷键这个东西发挥作用，还是要长期观察自己的使用习惯，了解自己的需求，多动脑筋，尽量少而精。另外，有的时候快捷键反而不方便，比如比较懒的时候，一手托着腮，另一手用触控板，只想点点点，何必上快捷键找麻烦呢？另有很多人会推荐第三方 App，美誉其为“神器”。但收费、兼容性问题、配置麻烦、跑路风险都是其缺点。这些第三方App包括但不限于 Alfred、Widgets。很多特色功能其实只要稍加思考就能找到原生系统里不错的快捷接口。举例来说，Alfred 就是一个高级版 Spotlight，它最亮眼的功能：打开应用、搜索引擎快捷搜索都可以在下文找到原生版的解决方案。以下是整理的部分快捷键，它们完全解决了我用 Mac 快一年的痛点，且我认为已经完备，没有必要再掌握其他的快捷键。还是要重申，每个人情况不同，仅供参考，需要开发出自己的舒适区。一、打开与关闭 App打开与关闭 App 是最常用的操作，基本上每个人都有自己常用的 App，有些 App 还有即时性的特点：打开看一下即关闭。做法是： 打开：使用 Spotlight (Cmd + Space) 输入 App 的关键词，敲回车 关闭：快捷键 Cmd + W，等效于左上角红色关闭按钮。强制退出用 Cmd + Q因此只需记住常用App的关键词即可，都是英文的前几个字母而已。总结如下： App 关键词 常见使用场景 访达 （Finder.app） f 类似于打开我的电脑（Win + E） Safari浏览器（Safari.app) sa 使用搜索引擎（下面第二节的基础） 微信 （WeChat.app） w, we, wec 打开，回消息，关闭 系统偏好设置（System Preferences.app) sys 打开某项常用设置，如我常用的 iPad 副屏 终端（Terminal.app) ter 程序员常用 VS Code （VSCode.app） vsc 程序员常用 二、快速搜索Spotlight 可以完全解决搜索问题，什么东西都可以搜，但这也是其缺点：不够精确，搜出来的东西需要挑选，浪费时间。它的这种设计个人认为更适合带有寻找灵感性质的搜索，但很少有人有这种习惯。最好是细分一下搜索的领域： 快速搜索本机文件（呼出 Finder 内的搜索）：Option + Cmd + Space （用空格的左边两个） 快速搜索引擎：用一的方法打开 Safari， Cmd + T （新标签页） 第一种方法：Cmd + L（激活地址栏），输入网站前几个字母，敲回车（有时需要Tab一下激活搜索框） 第二种方法：将常用的搜索引擎保持在书签栏前几位（例如我的是 Google, Baidu, Bing），使用快捷键 Cmd + 1/2/3 打开网页 三、FinderFinder 是 Mac 的文件资源管理器，下面一些快捷键非常有用。请注意，用快捷键时一定要唤醒 Finder 哦！ 后退 / 前进： Cmd + [ / ] 上一级文件夹：Cmd + 上， 强烈建议使用，因为 Finder 不像 Windows 有向上一级的按钮 查看并复制路径：按住 Option，下面会出现路径，右键可复制；文件右键菜单也可以复制路径。此功能程序员常用 新开 Finder 窗口： Cmd + N，适用于已经打开了一个文件夹，想不关掉它再开一个的场景四、Safari 浏览器 新标签页：Cmd + T 激活地址栏：Cmd + L，配合 Cmd + T 太完美了 关闭标签页：Cmd + W ，没错，Safari 这里只是关闭标签页而不是窗口 后退 / 前进： Cmd + [ / ] 左右切换打开的标签页： Cmd + Option + 左 / 右 （Option 用右边的）。其实还有一种方式是 Ctrl （+ Shift） + Tab 类似于切换 App，但还是推荐这一种，如果 Option 用右边的 话，左手只有大拇指按在 Cmd 上，方便基于 Cmd 的快捷键操作例如，左右切换标签页的目的经常是在一堆打开的标签页挑些没用的用 Cmd + W 关掉 重新打开刚关闭的标签页：Cmd + Shift + T 开关书签边栏：Ctrl + Command + 1（Cmd 最好用右边的）。有时候开了边栏很占地方，想快速隐藏掉，也可用这个快捷键五、文本编辑Mac 系统缺失了很多 Windows 中一个键就可以解决的，但都可以通过组合键实现： 向前删除：Fn + Delete，相当于 Windows 的 Delete 光标放到此行开始 / 末尾处：Option + 左 / 右，相当于 Windows 的 Home，End我的切换输入法方案是用 caps lock 切换中英文输入法，用 Fn 切换其他输入法（个人偶尔用到繁体中文、日语、俄语键盘）。caps lock 键不再用作大小写开关，因为大量输入大写字母的情形不多，少量输入使用 Shift 即可。特殊符号的输入： 重音符号：英文输入法下，按住带重音的字母 中文常用字符：中文输入法下 Shift + Option + B Emoji表情：Ctrl + Cmd + Space 或 Fn + E 另外，按住 Option 或 Shift + Option 也可输入特殊的字符，但是需要记住一张字符映射表。特殊字符本身并不常用，没有必要，不推荐。六、截图与录屏Mac 不需要任何第三方截屏工具，自带的功能强大，完全够用。 截取整个屏幕并保存：Cmd + Shift + 3 截取选取的部分并保存：Cmd + Shift + 4 高级截取（内含录屏）：Cmd + Shift + 5常识性的通用快捷键这些列举的是其他常识性的，在 Windows 系统也常用的快捷键： 撤销：Cmd + Z 恢复：Cmd + Shift + Z （与 Windows 不同） 全选：Cmd + A 拷贝 + 粘贴：Cmd + C, Cmd + V 剪切 + 粘贴 （文件）：Cmd + C, Option + Cmd + V （与 Windows 不同） 剪切 + 粘贴 （文本）：Cmd + X, Cmd + V 查找：Cmd + F （注意：替换没有固定的快捷键） 打开：Cmd + O 保存：Cmd + S键鼠解决方案在工位时我一般用机械键盘和鼠标，在这种键盘正下方不是触控板的情况下，有些快捷键就不太方便了。以下列举一些此场景的替代方案。触控板手势将触控板换成鼠标后，方便了一些操作，例如拖动；也平替了一些操作，例如滚动。但有些手势就不友好了，只能以其他方式替代： 左右切换桌面：四指左右滑动手势，改用 Ctrl + 左/右（Ctrl 用右边）。这样可以左手按快捷键，右手操作鼠标。 显示桌面：设置为右下角触发角，鼠标移到右下角即显示桌面。" }, { "title": "论文笔记：IIRC (Incremental Implicitly-Refined Classification)", "url": "/posts/papernotes_IIRC/", "categories": "科研", "tags": "论文笔记, 机器学习, 持续学习", "date": "2022-04-22 00:00:00 +0800", "snippet": "论文信息IIRC: Incremental Implicitly-Refined Classification 会议：CVPR 2021 作者：蒙特利尔大学等本文最主要的贡献就是提出的持续学习新场景：IIRC (Incremental Implicitly-Refined Classification)，文章也围绕此场景： 形式化定义此场景； 提供了Benchmark，以供模型在此场景的评估； 实验评估已有的经典持续学习模型在此场景上的表现。这个 IIRC 与导师提给我的一个具体场景相像，算是涉及粗细标签的基础场景，值得仔细研究。当持续学习遇上粗细分类持续学习已有很多流行的场景，如任务增量学习、类别增量学习，目前大部分持续学习模型都是在这几个流行场景的 benchmark 上测试的。类别增量学习应该是最流行的，即一个任务来一整个（或多个）类的数据。这个场景是假定了各任务的类别标签在语义上是同一层级的。但如果来的几个类有粗细的包含关系（本文只研究这种关系），那就有意思了。例如，第一个任务学“熊”类，第二个任务学“北极熊”类，这种包含关系的信息，模型从哪儿得知呢？只能从数据里找规律了：但是现在不仅要像原来那样学到两个类的区别，还要学到两个类的联系。让模型自己通过训练去找这种包含关系显然是为难它了。何不直接告诉它，这两个类是有包含关系的呢？也就是说，喂给模型的某些数据里就显示地表明了类之间的包含关系。 最直观的做法是对数据的结构作扩展：同一个数据允许用多个由粗到细的标签来反复使用。举个例子，对同一张熊的照片 \\(\\mathbf{x}\\)，我允许在第一个任务中使用数据 (\\(\\mathbf{x}\\), “熊”) 来学习粗类“熊“，也允许在后面的任务使用 (\\(\\mathbf{x}\\), “北极熊”) 来学习细类“北极熊“。这是IIRC相较于类别增量学习作的核心改变。IIRC 形式化定义每个数据允许有多个粗细的标签，本文限定为两个，分别称为超类（superclass）与子类（subclass）。如果某个标签没有更粗或更细的标签，则认其为子类。对于任务序列 \\(\\tau_1, \\cdots, \\tau_N\\)，每个任务 \\(\\tau_k\\) 会新来一个或多个类 \\(C_{k1}, \\cdots, C_{kp_k}\\)，这个类可以是超类或子类中的某一个。与类别增量学习一样，假设它们没有重复。另外要求，不允许在之前任务没有涉及对应的超类的情况下，出现子类。（也排除掉了同一个任务同时出现超类和子类的情况。）这样的话，第一个任务出现的只能都是超类。打个比方，如果把整个任务序列中涉及的类放到表格里，对于每个任务，类别增量学习就是从下面一个一维表格中不重复地选几个类： Bear Bus Dog Lamp Bird Truck IIRC则从二维表格中不重复地选，但必须先把超类选掉，才能解锁它的子类。 Bear Bus Dog Lamp Bird Truck Polar Bear Minibus Whippet       对于第 \\(k\\) 个任务的模型 \\(f_k\\)： 训练阶段只能看到训练数据的这一个类标签 \\(C_{k1}, \\cdots, C_{kp_k}\\)； 但是要求此模型能准确预测出这些类以及（如果这个类是子类的话）各自的超类，因此测试阶段的测试数据标签是同时包含这些类以及（如果这个类是子类的话）各自的超类的。上图是一个例子，展示了 \\(N=3\\) 个任务，每个框框表示此任务要学习的一个类。右上角表示该类（可见有超类也有子类），也是训练数据只能看到的标签；下方的 Target 表示模型预测的目标（可见已包含子类的超类）。评价模型时，需要计算每个任务的评价指标，每个任务右下角“Evaluate On”展示了这一信息，可见对于子类来说，既要评价是否正确分到子类上，还要评价是否正确分到超类上。Benchmark数据集：IIRC-CIFAR、IIRC-ImageNet作者将 CIFAR100 与 ImageNet 数据集加以改造，得到了这两个适用于 IIRC 场景的数据集。在代码上，就是在构造Dataloader时加了一些预处理。首先要将类别分成两层（超类、子类）的关系。CIFAR100自带了超类与子类；而ImageNet里的类别层级关系非常庞大，有远大于2层的层级关系，作者是采用了自己的一套标准压缩成2层类别的。这样就可以了吗？No！上面说了要想子类出场必须先解锁超类，但是目前数据都是划到第2层的子类的，需要拨一些给它们的超类使用。如何拨、拨的比例都在文中给出，不再详述。评价指标：pw-JS对于一个输入 \\(\\mathbf{x}_i\\)，IIRC 场景下模型预测结果可能不再只输出一个类标签了，而可能是1-2个类标签，记为集合 \\(\\hat{Y}_i\\)： 1个标签：必须是超类，指属于此超类，且不属于它的任何子类； 2个标签：指属于超类下的子类。真实标签也可能是1-2个标签，同上，记为集合 \\(Y_i\\)。评价指标不再是0-1准确率，而应基于这两个集合的关系。作者给出的评价指标称为准确率加权的Jaccard相似度（pw-JS）：\\[R=\\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\left|Y_{i} \\cap \\hat{Y}_{i}\\right|}{\\left|Y_{i} \\cup \\hat{Y}_{i}\\right|} \\times \\frac{\\left|Y_{i} \\cap \\hat{Y}_{i}\\right|}{\\left|\\hat{Y}_{i}\\right|}\\]其中第一个因子为 Jaccard 相似度，第二个因子为准确率。对于只有2层类别的 IIRC 场景，其实可以把所有情况穷举出来，列到下面这个表里： 例子：\\(\\hat{Y}_i\\) VS \\(Y_i\\) 描述 Jaccard相似度 准确率 结果 均为{Bear} 或 均为{Bear, Polar Bear} 预测完全正确 \\(1\\) \\(1\\) \\(1\\) {Bear} VS {Bear, Polar Bear} 预测粗了 \\(\\frac12\\) \\(1\\) \\(\\frac12\\) {Bear, Polar Bear} VS {Bear} 预测细了 \\(\\frac12\\) \\(\\frac12\\) \\(\\frac14\\) {Bear, Polar Bear} VS {Bear, Brown Bear} 细类预测错了 \\(\\frac13\\) \\(\\frac12\\) \\(\\frac16\\) {Bear, (随意)} VS {Dog, (随意)} 粗类预测错了 \\(0\\) \\(0\\) \\(0\\) 可以看到，这个指标有一个致命的问题。假设模型已经将粗类预测正确，那么它现在有两个选择：继续预测细类/不预测细类。观察上表，可以看到：预测成功的收益都是1；前者预测错误即预测细了收益为 \\(1/4\\)，是小于后者预测错误即预测粗了的收益 \\(1/2\\)的。所以，一个聪明的模型会发现，那些努力的人（预测细了）得不到表扬，费力不讨好，还不如偷懒（往粗了预测）呢！虽然这个指标没有加到损失函数里参与到了模型的训练中，但这个指标确实是不科学、不公平的。实验已有的经典持续学习模型如何套到IIRC场景中？实际上此场景只是把普通的多分类改为了多标签分类。处理方法很简单，只需将最后一层 Softmax 分类器改为多个 Sigmoid 分类器，分类损失选择二分类损失（BCE）即可。实验为测试经过改造的经典持续学习模型在本文 Benchmark 上的表现，不再展示。" }, { "title": "编配：《贝加尔湖畔》手风琴独奏", "url": "/posts/accordion_transcribed_Baikal-Lake/", "categories": "音乐", "tags": "手风琴, 乐谱, 音乐编配", "date": "2022-04-17 00:00:00 +0800", "snippet": "乐谱已上传至 MuseScore 和 B站，有实时的声音与乐谱对照。我的编配版本忠于李健《依然》专辑录音原版，力求还原原版的细节与感觉，兼顾曲谱的难易程度。第一段和最后一段模拟开头和结尾的钢琴即兴，针对手风琴做了一些简化。打谱软件不太给力，请自行脑补渐快渐慢。自我感觉模拟的不是很好，欢迎大家提提意见。旋律中加入的李健声线和原曲手风琴中的装饰音，我觉得它们是这首歌的灵魂。实际试过难度不大。有一处细节是歌曲中的 F 和弦都按大七和弦 Fmaj7 处理（实现方式是 F + Am），但副歌部分都改回了 F，因为右手跳音 (Do, Mi) 有大七的成分。乐谱PDF请扫码获取或访问：https://disk.pku.edu.cn:443/link/D5F5BEB9667011C9AAD63F6FE3409E2E，密码 3Og9。" }, { "title": "我与手风琴", "url": "/posts/accordion_and_me/", "categories": "音乐", "tags": "手风琴", "date": "2022-04-13 00:00:00 +0800", "snippet": "我喜欢音乐，自然喜欢捣鼓各种乐器。要说也不算多，我玩过电子琴、吉他、口琴之类的，有跟老师上过课的，也有自学的，大部分都是基本功不扎实的那种。我和手风琴的结缘可以说相当有趣了：从完全不了解+不感兴趣，到现在经常需要去拉两次过过瘾，还会翻遍网上各种资源。不承认不行，目前我已经是彻底爱上这个乐器了。在本科三年级的时候，我选修了一门艺术类通识课，它兼古典音乐赏析与实践于一体，其中实践部分就是手风琴入门。只是抱着水学分的态度，因为当时我正苦于练吉他遇到瓶颈，抱着难得有的不求多而求精的思想。但是上课之后就上手很快，进度差不多是其他人的2倍吧。一年之后，我又选了同一位老师的通识课，属于是进阶的手风琴课，这个时候已经不太听话，开始自己玩自己的了。玩得很开心，可是已经要毕业了，要去另一个地方读研了。来到读研的新学校，惊喜地发现居然有专门的手风琴社团（虽然是第二年发现的），决定立即加入组织。还记得那天迎新会是刚考完博士生资格考试的晚上，经历了一个月抑郁的复习时光的我，快被感动哭了：原来黑暗尽头是光明，生活真的可以叫生活，不在那种心境下怎么体会得到呢。来到新太阳地下，那是我第一次知道这里社团的大本营，感觉当年本科开心的感觉突然找回来了。每周的合奏排练，还有与几位新同学的重奏，感觉这半年一大半的快乐都在这里了吧。我开始重新拾起当年的那份快乐，去琴房频繁起来，对这个乐器也越来越熟悉。回到手风琴本身。为什么上手快呢？首先手风琴是键盘乐器，与我小时候喜欢瞎弹的电子琴完全一致。小时候我就有听到旋律能直接复现到键盘上的能力，那时候就爱尝试各种听过的歌，再配上电子琴左手录好的和弦弹一弹。以弹钢琴的标准来看，我的左手是纯纯的残废，甚至连分解和弦都做不到。而恰好手风琴的左手也是一个“自动挡”，常用的大三、小三、属七等和弦只需按一个键：从这个角度看，如果只对美妙的旋律配上动人的和弦感兴趣，那手风琴是一个非常容易上手的乐器了。我从不考虑那些强调技巧的考级曲目，难度高的严肃的古典曲目也玩不来。一是没那时间，二是就图一乐。作为一个业余音乐爱好者，我深受“音乐 = 旋律 + 和弦”思维的影响，因此我感兴趣的乐器中，大部分都是能够完整实现这两个要素的：键盘右手旋律 + 左手伴奏，人唱旋律 + 吉他和弦，等等；而对只能发出单音的非和声乐器不感兴趣，如交响乐里的乐器。在我看来它们偏单调，更强调严肃与技巧，最重要的是难以实现自娱自乐。自娱自乐的一个重要方式就是将自己喜欢的歌曲在和声乐器上实现。奈何手风琴资源实在是太少，有时候抄作业都没得地方，或者网上流传的版本不满意，只好自己动手丰衣足食——扒谱、编配。流行歌曲可以听原版音轨，从伴奏中的各种和声中挑选满意的映射到手风琴左手（例：贝加尔湖畔）；传统歌曲有很多爱好者自己拉的视频（B站、YouTube 等），可供结合不同版本的灵感，打造自己喜欢的编配方式（例：喀秋莎）。编配乐谱和做饭是一个性质，是学习，也是再创作。我会不定期更新手风琴的内容，包括我编配的手风琴谱等。期待大家的关注，也期待大家因我对这个乐器产生兴趣。" }, { "title": "论文笔记：无监督持续学习论文一篇", "url": "/posts/papernotes_Representational-Continuity-for-Unsupervised-Continual-Learning/", "categories": "科研", "tags": "论文笔记, 机器学习, 持续学习, 无监督学习", "date": "2022-04-12 00:00:00 +0800", "snippet": "论文信息Representational Continuity for Unsupervised Continual Learning 会议：ICLR 2022 (Oral) 作者： 纽约大学、韩国科学院、清华大学智能产业研究院等 一、场景：无监督持续学习设持续学习包含 $T$ 个任务，当前正在学习第 $\\tau$ 个任务。持续学习要求不仅对新数据做Fine-tuning，还要复习过去的知识，这两部分体现在优化目标 $\\mathcal{L}$ 的两项，记为\\[\\mathcal{L} = \\mathcal{L}^{\\text{FINETUNE}} + \\mathcal{L}^{\\text{REVIEW}}\\]我用一张表来比较有监督和无监督的区别（以分类为例）：   有监督持续学习（Supervised CL） 无监督持续学习（Unsupervised CL） 新来的数据 \\(\\mathcal{D}_{\\tau} = \\{(\\mathbf{x}_{i,\\tau}, y_{i,\\tau})_{i=1}^{n_\\tau}\\}\\) \\(\\mathcal{U}_\\tau = \\{(\\mathbf{x}_{i,\\tau})_{i=1}^{n_\\tau}\\}\\) 模型与参数 \\(X_\\tau \\rightarrow \\mathcal{Y}_\\tau\\)，可看成 表示函数 \\(f_\\Theta : X_\\tau \\rightarrow \\mathbb{R}^D\\) 和 分类器 \\(h_\\psi: \\mathbb{R}^D \\rightarrow \\mathcal{Y}_\\tau\\) 只有表示 \\(f_\\Theta : X_\\tau \\rightarrow \\mathbb{R}^D\\) \\(\\mathcal{L}^{\\text{FINETUNE}}\\) \\(\\mathcal{L}_{\\text{SCL}}^{\\text{FINETUNE}} = CE(h_\\psi(f_\\Theta(\\mathbf{x}_{i,\\tau})，y_{i,\\tau})\\) $$\\mathcal{L}_{\\text{UCL}}^{\\text{FINETUNE}}$$ \\(\\mathcal{L}^{\\text{REVIEW}}\\) 由现有的各种持续学习框架定义 $$\\mathcal{L}_{\\text{UCL}}^{\\text{REVIEW}}$$ 对于不同的任务有不同的分类器（注意：$h_\\psi(\\cdot, \\tau)$），分类器是一个比表示$f_\\Theta$简单得多的网络，对每个任务 $\\tau$ 都会根据当前的 $f_\\Theta$ 作微调。虽然这里是有参数的，但完全可以是无参数的，如K近邻分类器。这个分类器不是持续学习的学习目标，只是模型里的一个必要的输出头。无论有监督还是无监督，都是为了学到一个好的表示 $f_\\theta$ ，但由于有监督的任务要求输出类别 $\\mathcal{Y}_\\tau$，不可避免地需要接一个分类器。无监督的任务虽然只需要得到表示即可，但是我们无法评估这个表示的好坏，因此一般的评估方法也是接一个分类器，以输出结果与真实类别的比较作为衡量标准，计算准确率等评价指标。这里真实类别的标签只用在评估时，没有用在训练时，损失是一种自监督的损失（self- supervised loss），这是有监督和无监督的本质区别。设计 \\(\\mathcal{L}_{\\text{UCL}}^{\\text{FINETUNE}}\\) 和 \\(\\mathcal{L}_{\\text{UCL}}^{\\text{REVIEW}}\\)是设计无监督学习框架的主要任务，分别放在第二、第三部分讲述。二、无监督模型目前大火的无监督表示学习模型就是对比学习（Contrastive Learning）了，作者选用了其中一些比较适合的持续学习场景的模型，它们都是基于孪生网络的思想。孪生网络（Siamese Network）是要求两个输入的网络，通过网络后分别得到这两个输入的表示。它可以看成一个网络，也可以看成两个共享权重的网络（所以叫孪生网络）。得到的两个表示一般要算一下相似度（最简单的是余弦相似度，即两个向量的夹角），这个相似度用来构造损失，具体什么样的损失由网络要完成的任务来定。任务只有一个要求——两个输入，比如判断两个图片是否是同一类，等等。孪生网络如何用在无监督学习中呢？自然，对于一个输入 $\\mathbf{x}$，无监督学习要学的表示函数就是这个孪生网络。但是它要求两个输入怎么办呢，接下来就是主要思想：这两个输入是原始输入的两个Augmentation $\\mathbf{x}^1,\\mathbf{x}^2$，如果这个表示认为由同一个输入 $\\mathbf{x}$ 变换出来的 $\\mathbf{x}^1,\\mathbf{x}^2$ 是相似的，那它就是一个好的表示。因此优化目标是尽量让得到的两个表示接近，构造的损失函数也就用到了上文提到的相似度，一般来说是迫使相似度尽量大，例如\\[\\mathcal{L} = - D(z_1, z_2)\\]其中 $z_1, z_2$ 是 $\\mathbf{x}^1,\\mathbf{x}^2$ 通过孪生网络 $f$ 后的两个表示，$D$ 是余弦相似度。SimSiam对于无监督的表示学习，Facebook何恺明、陈鑫磊等人提出了一个简单的孪生网络SimSiam，但是比上面最简单的还是多了一些东西的：它在表示网络后接了一个预测头 $h$（也是一个网络），这样两个输入就有了四个表示：$z_1 \\triangleq f(\\mathbf{x}_1), z_1 \\triangleq f(\\mathbf{x}_1), p_1 \\triangleq h(f(\\mathbf{x}_1)), p_2 \\triangleq h(f(\\mathbf{x}_2))$。最终交叉混淆处理相似度：\\[\\mathcal{L} = \\frac12 D(p_1, z_2) + \\frac12 D(p_2, z_1)\\]这个模型最重要的事情是要求把 $z_1,z_2$ 看作常数，而不是含模型参数的函数（在PyTorch里就是detach一下），记作 \\(\\text{stopgrad}\\)。统一到本文的符号，写作：\\[\\mathcal{L}_{\\mathrm{UCL}}^{\\mathrm{FINETUNE}}=\\frac{1}{2} D\\left(p_{i, \\tau}^{1}, \\text {stopgrad}\\left(z_{i, \\tau}^{2}\\right)\\right)+\\frac{1}{2} D\\left(p_{i, \\tau}^{2}, \\text{stopgrad}\\left(z_{i, \\tau}^{1}\\right)\\right)\\]Barlow Twins为了得到一个好的表示，除了使同一个输入 $\\mathbf{x}$ 产生的两个变换表示尽量接近，还可以使不同输入的表示尽量地远（其实这样有可能带来问题，暂且不讨论）。Facebook的另外一些人，包括Yann LeCun提出的Barlow Twins，就在损失函数中加了第二项：\\[\\mathcal{L}_{\\mathrm{UCL}}^{\\mathrm{FINETUNE}} = \\sum_i (1 - \\mathcal{C}_{ii})^2 + \\lambda \\cdot \\sum_i \\sum_{j\\neq i} \\mathcal{C}_{ij}^2\\]这个损失每次考虑多个输入 $\\mathbf{x}^1, \\cdots, \\mathbf{x}^i, \\cdots$，同样地，每个输入 $\\mathbf{x}^i$ 都有两个Augmentation $\\mathbf{x}^i_1,\\mathbf{x}^i_2$，$\\mathcal{C}_{ij}$ 就是两个输入不同变换之间的相似度 \\(\\mathcal{C}_{ij} = \\mathcal{D}(z^i_1, z^j_2)\\)。这个损失的第一项其实就是上面最简单的相似度损失（累加了多个输入的），第二项的损失给了一个可调的超参数 $\\lambda$。三、如何持续起来？大部分持续学习模型都是针对有监督场景的，难以直接应用到无监督，但有一些确实是可以推广的。作者从三类持续学习模型各取了一个代表：重演法：DER重演法一般要在记忆 $\\mathcal{M}$ 里存放重演数据，而有些是连同标签一起存进去，训练新任务时构造进REVIEW损失里去，这样的重演模型就不好推广到无监督。作者找到的DER（Dark Experience Replay）用不着重演数据 $x$ 的标签，而是存旧模型预测的过Softmax之前的logit $p$。防止遗忘的方法是让新模型预测重演数据的logit尽量与存的接近：\\[\\mathcal{L}_{\\mathrm{SCL}}^{\\mathrm{DER}}=\\mathcal{L}_{\\mathrm{SCL}}^{\\text {FINETUNE }}+\\alpha \\cdot \\mathbb{E}_{(x, p) \\sim \\mathcal{M}}\\left[\\left\\|\\operatorname{softmax}(p)-\\operatorname{softmax}\\left(h_{\\psi}\\left(x_{i, \\tau}\\right)\\right)\\right\\|_{2}^{2}\\right]\\]推广到无监督，只需将logit换成网络输出的表示：\\[\\mathcal{L}_{\\mathrm{UCL}}^{\\mathrm{DER}}=\\mathcal{L}_{\\mathrm{UCL}}^{\\text {FINETUNE}}+\\alpha \\cdot \\mathbb{E}_{(x) \\sim \\mathcal{M}}\\left[\\left\\|f_{\\Theta_{\\tau}}(x)-f_{\\Theta}\\left(x_{i, \\tau}\\right)\\right\\|_{2}^{2}\\right]\\]其实最经典的iCaRL也可以推广到无监督（它的REVIEW损失是蒸馏损失），但是有点古老了效果已达不到SOTA，所以作者没有用它。加正则项法：SISI（Synaptic Intelligence）是对著名持续学习算法EWC 的改进，在EWC中，由上一个任务 $\\tau-1$ 学习下一个任务 $\\tau$ 时，\\[\\mathcal{L}_{\\mathrm{SCL}}^{\\mathrm{EWC}}=\\mathcal{L}_{\\mathrm{SCL}}^{\\mathrm{FINETUNE}}+\\frac{\\lambda}{2} \\cdot \\sum_{i} F_{i, i}\\left(\\theta_{i}-\\theta_{\\tau-1, i}^{*}\\right)^{2}\\]可见加的REVIEW正则项只与之前任务学到的模型参数有关，不涉及数据的标签信息，SI也是如此。因此有监督损失可以轻易推广到 \\(\\mathcal{L}_{\\mathrm{UCL}}^{\\mathrm{SI}}\\)。网络结构法：PNN这个方法是对网络结构本身动手脚，在不同的任务阶段，网络结构是不同的，持续并不是通过添加正则项 \\(\\mathcal{L}^{\\text{REVIEW}}\\)、而是通过训练新的网络参数实现的。在这个PNN（Progressive Neural Network）中，每次新任务都会在图中右边多出一列网络出来，冻结原有的权重（虚线），只训练新的权重（实线）。对无监督学习，只需将网络改成上节的孪生网络，此图用MLP结构，则孪生网络也用MLP即可。训练时的损失 \\(\\mathcal{L}_{\\mathrm{UCL}}^{\\mathrm{PNN}}\\)就用上节的无监督损失 \\(\\mathcal{L}_{\\mathrm{UCL}}^{\\mathrm{FINETUNE}}\\)。四、Mixup技巧作者也借鉴了Facebook实验室提出的Mixup技巧。这是一个比较直观的训练上的trick，即任取两个训练数据作线性组合，得到的这种混合数据（基本上在原始训练数据的周边）也拿去训练。机器学习理论中，用最小化由训练数据集构造的损失这种机制被称为经验风险最小化（ERM，Empirical Risk Minimization），现在这种最小化原始训练集的周边风险最小化（VRM，Vicinal Risk Minimization）。这样做的优点显然是能提高模型的鲁棒性，所以作者拿过来用在了他的无监督损失上，这也是本文的主要创新点吧，取名为LUMP（Lifelong Unsupervised Mixup）。具体的用法是在对新数据微调的损失 \\(\\mathcal{L}_{\\text{UCL}}^{\\text{FINETUNE}}\\) 中，不仅使用新数据 $x_{i,\\tau}$，而使用新数据与过去知识（注意：LUMP是对DER的改进，需要记忆重演数据 $\\mathcal{M}$）的混合：\\[\\tilde{x}_{i, \\tau}=\\lambda x_{i, \\tau}+(1-\\lambda) x_{j, \\mathcal{M}}\\]。这样，无监督损失 \\(\\mathcal{L}_{\\text{UCL}}\\)不仅在\\(\\mathcal{L}_{\\text{UCL}}^{\\text{REVIEW}}\\) 中考虑了过去的知识，也在 \\(\\mathcal{L}_{\\text{UCL}}^{\\text{FINETUNE}}\\) 考虑了。这里也给了可调的超参数 \\(\\lambda\\)，用以trade-off持续学习模型的可塑性与稳定性。五、实验与结论总结一下上面提到的无监督持续学习方法，按照 \\(\\mathcal{L}_{\\text{UCL}}^{\\text{FINETUNE}}\\) 分有SimSiam、Barlow Twins共2类，按照 \\(\\mathcal{L}_{\\text{UCL}}^{\\text{REVIEW}}\\) 分有改造成无监督场景的SI、PNN、DER、外加作者针对DER的改进LUMP共4类，所以一共有 \\(2\\times 4 =8\\) 个无监督持续模型。本文的实验是连同有监督的持续学习一起，对这些方法做一个对比。和其他持续学习一样，评价指标有各任务平均准确率 \\(A = \\frac1T \\sum_{\\tau=1}^T acc_\\tau\\) 和 各任务平均遗忘程度 \\(F = \\frac1{T-1}\\sum_{\\tau=1}^{T-1} \\max_{1\\leq t \\leq \\tau}(acc_t - acc_\\tau)\\) 。应注意这里有监督和无监督实验的关系是：数据集是相同的，无监督就是直接去掉了有监督数据集的标签。作者的实验居然发现少了标签信息的无监督学习，效果都比有监督好！可以看到，在这个实验结果中无监督基本是碾压有监督的，根据我的猜测，可能是因为无监督学习使用了两个Augmentation起了主要作用，很大程度上丰富了训练数据。本文提出的LUMP效果也要高于其他方法，可能也是Mixup技巧引入了更多训练数据导致的。小伙伴们觉得这个实验公平吗？" }, { "title": "建站的一路坎坷", "url": "/posts/building_this_site/", "categories": "生活", "tags": "技术", "date": "2022-04-09 00:00:00 +0800", "snippet": "今天我的个人网站就搭建成功啦。为什么要搭个人网站呢，其实主要还是导师的建议：做科研要有笔记。之前的习惯是用iPad在论文上或书上勾勾画画，缺点就是太容易忘，需要用到时全凭记忆。整理成笔记的过程也是内化为自己知识的过程，还可以和别人分享，省去亲自讲的时间。希望这次搭的网站能让这个习惯坚持下去，为自己带来一些提升吧。说到分享，其实做过一个公众号，叫“应用数学博士周记”，在刚入学的时候试图严格规律自己的学习科研（以周记的形式），发了几个周以后，发现事情远没有那么容易，因为一年级的时间完全无法自己掌控，周记这种形式又太死板，确实很难坚持下去；而且公众号这种推送给用户的形式越发觉得尬…… 后来就在上面发了一些读书笔记，但也过于麻烦：需要调的格式过多、大量依赖第三方软件（秀米、墨滴等）、处理公式不方便、微信公众平台的审核机制。公众号本质上是自媒体，所以一开通，然后被我宣传了一小下，就有好多朋友关注催更，感觉自己这么高调，像一个up主一样哈哈哈。从经济角度讲，这种面向观众的性质给我添了很多负担，有时为满足观者的需求甚至要在内容和素材上妥协，如果不以盈利为目的，就会花掉很多对自己无用的时间。（我认为这也是为什么很多个人公众号坚持不下去的原因，试过才知道）所以就试试个人网站吧，没有定时，没有题材限制，且平台的权限全在我，想发就发，我想做成什么样子都可以。我希望每篇文章更加随性一点，对个人成长更有帮助一点。网站也不主动向他人推送，不会打扰到不想被打扰的朋友。只把网站放到自己的名片一栏里，若你感兴趣，自然会点进来一堵为快的。建站过程这个网站是用GitHub Pages搭建的，是一个静态网站，不需要自己买域名组服务器，整个网站的文件放到一个GitHub的仓库里即可，由他们部署托管，应该是所有建站方法中最省事的吧（个人还没有技术能力搭建动态网站）。GitHub还给了很多模版，用模板就不用自己写前端了，每次只需把要发的文章写成MarkDown格式，模版的工具jekyll会自动生成网页源代码。就是这么一个简单的东西，只需几步：clone模版仓库、安装jekyll工具、修改模版里的个人信息、push到GitHub上，结果花了我将近一天才成功。我面临的诸多，对于熟悉前后端的程序员来说可能就是小bug的问题，让我绞尽脑汁研究了很长时间： 工具jekyll需要安装Ruby和它的包管理工具gem，我心想这不就类似Python和pip么，小菜一碟，结果装完后遇到了各种无法调用jekyll的问题。最后发现，Mac系统自带了一个低版本的Ruby，新安装的Ruby和它的环境变量混淆了，导致安装jekyll时依赖关系混乱、版本不匹配。最终解决方法是重新安装Ruby，安装jekyll时注意gem命令的环境变量（找环境变量的which命令没用过，甚至还可以一个which -a找出所有，这个命令就苦寻了很久）让它装到非系统自带的Ruby下，在调用jekyll时也注意环境变量。 网站生成出来后界面巨丑。我知道一点CSS的原理，于是下意识进网页代码里看，确实是CSS路径不对，找不到CSS文件。可是我在本地看是没问题的呀，怎么部署上去就找不到了呢。知乎上有人问了这个问题，一位程序员轻描淡写地解决了： 瞬间感觉到降维打击…… 我于是全文搜索baseurl，终于在一个配置文件里找到了它的赋值，改了一下就成功了。 我用的这个chirpy 模版，有一个巨坑，它在某一个不起眼的文档里说了这样一件事： 结果就是我无论怎么调，随后打开我的网站查看效果时，永远都是第一次失败的那个网页。瞬间感觉自己太sb了，如果是个经验丰富的前端程序员，恐怕早就想到这个问题了。这个坑浪费了我大半的时间。 我深刻感受到了，永远要相信内行人的内行。之前选修信科（相当于贵校的计算机学院）的《数字图像处理》课程，老师说，咱作业项目就做成网页demo吧。我们几个数院的顿时冒了很多问号，嗯？？做网页？再次向老师“哭诉”不会做网页时，老师便让助教现场搭了一个Flask模版给我们用。回去之后，好家伙项目花多少时间，套这个模版就花多少时间。真的是数不清的小bug啊，图片加载不出来、字号跳不了、点按钮没反应，只能让计算机专业的同学见笑了。不管怎么说，网站搭成了，靠自己解决了这些技术问题，也很有成就感。就让这个网站一路陪我走下去吧。 :-)" }, { "title": "编配：《钢铁洪流进行曲》手风琴四重奏", "url": "/posts/accordion_transcribed_March-of-Steel-Torrent/", "categories": "音乐", "tags": "手风琴, 乐谱, 音乐编配", "date": "2022-03-28 00:00:00 +0800", "snippet": "乐谱已上传至 MuseScore 和 B站，有实时的声音与乐谱对照。我的编配版本忠于中国人民解放军军乐团《钢铁洪流进行曲》官方原版。一声部负责短笛的装饰；二、三两个声部负责主要旋律，增加气势；四声部负责低音与和声。各声部选择合适的变音器即可，二、三声部尽量区分高低。后面有一段滑音是模仿了原曲（听上去是木管乐器）的滑音，可能有些诡异，想修改但没什么思路。欢迎大家提提意见。乐谱PDF请扫码获取或访问：https://disk.pku.edu.cn:443/link/D5F5BEB9667011C9AAD63F6FE3409E2E，密码 3Og9。" }, { "title": "PyTorch 学习笔记（四）：深度学习的训练", "url": "/posts/studynotes_PyTorch_deep_learning_training/", "categories": "科研", "tags": "PyTorch, 读书笔记, 《动手学深度学习》, 技术", "date": "2022-02-11 00:00:00 +0800", "snippet": "本文总结 PyTorch 是如何实现深度学习训练中的各种技巧与细节，包括防止过拟合、参数初始化、优化器、损失函数、超参数优化等等。关于这部分知识，我在这篇笔记中有系统的总结。本文的编排顺序基本与这篇笔记对应（数据预处理部分在介绍 Dataset 类型的笔记中，调参、学习曲线等放在 PyTorch 工程性知识的笔记中）。此外，还有一篇笔记总结了深度学习训练的实践经验，可供参考。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 4.4 节：模型选择、欠拟合与过拟合； 4.5 节：权重衰减； 4.6 节：暂退法； 4.8 节：数值稳定性与模型初始化； 4.9 节：环境和分布偏移； 第 11 章：优化算法一、激活函数激活函数在形式上应该是一个 Python 函数，它接受 Tensor 类型的输入并输出相同维度的 Tensor 变量。在笔记（一）MLP 的从头开始实现中可以看到：def relu(x): a = torch.zeros_like(x) return torch.max(X, a)在 torch.nn.functional 中有各种预定义的激活函数，见文档：https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions torch.nn.functional 提供了各种深度学习可能用到的预定义函数。这里都是用 Python 函数实现的，相对来说封装程度没有那么高，一般用于自己模型设计的零件；下面可以看到，很多这种函数如激活函数、损失函数是实现为一个可调用类的，封装程度更高。在深度学习中，激活函数的用处就是作为 nn.Module 模型的一个组成部分。使用的方法就是将其套在 forward 函数中。以下是笔记（三）自定义块中见到的例子：class MLP(nn.Module): def __init__(self): super().__init__() self.hidden = nn.Linear(784, 256) self.out = nn.Linear(256, 10) def forward(self, X): return self.out(nn.functional.relu(self.hidden(X)))因此在实际使用 PyTorch 的高级 API 中，激活函数被看成是一个层，是 nn.Module 类型。我们也在笔记（三）自定义层中见到过：class ReLULayer(nn.Module): def __init__(self): super().__init__() def forward(self, X): return nn.functional.relu(X)当然 PyTorch 也预定义了很多这种激活函数层，它们放在 torch.nn 中（文档），与其他预定义的有实际参数的层（如 nn.Linear）并列。这种预定义层的用法就是放在 nn.Sequential 容器里，与其他层串联。例如，上述自定义层等价于如下预定义层：relu = nn.ReLU()三、网络结构注：“二、数据预处理” 我放到了笔记（二）中，和对 Dataset 的介绍放在了一起。下面从 “三、网络结构” 开始。对网络结构下手的一些训练 trick 与 nn.Module 是兼容的，可以看作一种特殊的层。PyTorch 为 Dropout 和 Batch Normalization 都提供了高级的 API：nn.Dropout()、nn.BatchNorm1d()。对于图像等数据，还提供了 2D、3D 等版本。这里需要注意的是，Dropout 和 Batch Normalization 都是训练和测试不一样的层（在训练阶段引入随机性，在测试阶段以期望值代替来消除随机性），所以这些层前向传播时，必须要有指示告诉它们是训练还是测试。PyTorch 设计了这个指示变量封装在 nn.Module 类型的实例属性 training 中（布尔变量）；方法 .train() 与 .eval() 可以修改此变量，把它放在整个训练或测试阶段开始前即可。四、参数管理与初始化这节的内容是参数初始化，我会连带讲解 PyTorch 的模型参数管理机制，即与 nn.Module 对象定义的模型参数有关的操作。笔记（一）的最后一个模型已经看到，nn.Module 的模型参数都属于 nn.Parameter 类。这是一个封装模型参数的类（文档），将其与普通的 Tensor 区别开，便于训练和管理（我们在以后可以看到封装的优势）。此类在构造时接受两个参数： data：参数数据，Tensor 或 nn.Parameter 类型（允许递归嵌套）；它直接构造了实例属性 .data，要直接取得 nn.Parameter 类封装的 Tensor，应访问该实例属性； requires_grad：指定 data 是否需要梯度。当然，参数指的是模型参数，是与模型挂钩的，一般不单独实例化 nn.Parameter 对象，而是在模型 nn.Module 实例化时就已经存在了。笔记（一）的最后一个模型只是起了演示的作用。访问模型的参数模型的参数应该是模型的一个属性。上图是笔记（三） NestMLP 的参数图（它与笔记（三）图的区别在于，有参数的层外挂了一个绿色的叶子结点）。可以看到参数放在了模型的 weight、bias 属性中。因此，访问模型某层的参数可以直接按照图中绿色叶子结点的调用方式。PyTorch 也设计了访问模型所有参数的方法，注意，在 nn.Module 类中定义的所有 nn.Parameter 的实例属性都被视为模型参数（不管它是否参与到 forward 函数）。与 nn.Module 同理，其算法也是递归地遍历树的叶子。API 有： .parameters() 方法：返回一个生成器。这种方式访问通常用于直接传入优化器的 params 参数，print 无法直接显示，需要遍历其元素 print（或者强制转化为列表）； .named_parameters() 方法，返回生成器生成的是 (参数名字, 参数数据) 对。参数的命名空间与模型一致； .state_dict() 方法：返回一个 collections.OrderedDict 类型，字典键值为 {参数名字:参数数据}， print 可以显示。参数初始化这里讨论封装在 nn.Parameter 中的参数的初始化。参数初始化当然也可以直接取出 data 属性，对其赋值或修改。但更好用的是能直接对 nn.Parameter 对象作初始化的修改函数，这也是封装 nn.Parameter 的意义。PyTorch 提供了很多初始化函数，它们作用在 nn.Parameter 对象上。这些函数定义在 nn.init 模块中，以下列举几个常用的，其他的详见文档：https://pytorch.org/docs/stable/nn.init.html。 nn.init.normal(tensor, mean, std)：从正态分布 \\(N(mean, std)\\) 初始化 nn.init.constant(tensor, val)：全部以常量 val 初始化 nn.init.uniform(tensor, a, b)：从均匀分布 \\(U(a,b)\\) 初始化 nn.init.xavier_uniform(tensor, gain), nn.init.xavier_normal(tensor, gain)：Xavier 初始化 nn.init.kaiming_uniform(tensor, a, mode), nn.init.kaiming_normal(tensor, a, mode)：何恺明的初始化当然，这些函数内部细节就是取出 data 属性后对 Tensor 的操作，也可以自己按照的方式定义一个初始化函数。 PyTorch 预定义的 nn.Module 层带有默认的初始化方法（reset_parameters()），是一些优秀初始化方法的汇总和精调，在实例化模型时就会调用它。PyTorch 为每种层都涉及了适合它们的默认初始化方法。因此，除了研究不同初始化方法的需要，在高级 API 搭建深度学习模型的流程中可以省略初始化这一步。在实际中，参数初始化一般是整体地对一整个模型初始化。通常是打包成一个 init_parameters 函数，通过 nn.Module 的 apply 方法（可以将传入的函数递归地作用到它包含的所有层上）作用到模型参数上。以下是[笔记（一）]中出现的例子：def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01)net.apply(init_weights)可以看到，一个函数也能搞定对不同层的不同初始化，只要加 if 规则判断就行。这样打包成一个函数的好处是方便维护代码。对于预训练模型用别人训好的参数作初始化，涉及模型的读写文件，见这一篇笔记，不在这里讲。五、优化器优化器就是一个对参数的修改函数，它接受待更新的参数，利用参数中存储的梯度信息（见笔记（一）自动微分部分，计算的梯度存放在参数 Tensor 的 grad 属性中，因此不需要传梯度参数），在函数主体中完成对参数的一步更新（如梯度下降法）。最后要有一步梯度清零的操作，也是在这里实现的。还要注意这部分代码要用 with torch.no_grad(): 包裹，这些在笔记（一）中都提到了。一个优化器函数形式如下：def optimizer(params, hyperparams): with torch.no_grad(): for p in params: # 遍历模型参数 g = p.grad # 提取梯度 ... # 对 p 的修改，是基于 g 的公式 p.grad.zero_()其中的超参数除学习率外可能还有很多，为了形式统一，往往打包成字典的形式，在用的时候取字典值。例如动量法传入的超参数形式为 {&#39;lr&#39;:lr, &#39;momentum&#39;:momentum}。之前实现的是随机梯度下降（SGD），相对比较简单，只需用一下梯度下降公式即可。更复杂的优化器如 Momentum、AdaGrad、Adam 等，往往需要维护一组状态值，它们随着训练过程也像参数一样进行迭代，且需要初始化。这种状态值的处理也很简单，可以放在全局变量或优化器函数的参数里，不再赘述。实际上在 PyTorch 中，优化器并不是简单的修改函数，而是继承的 torch.optim.Optimizer 类，这样有利于提供更完整、健全的优化器功能，例如设置默认值等，PyTorch 提供的 API 也都是 Optimizer 类型，详见文档；对参数的修改定义在 step() 方法里。自己写优化器时，如果有需求，可以按这种方式写比较复杂的类（但写成函数基本就够用了）：class MyOptimizer(torch.optim.Optimizer): def__init__(self, params, hyperparams): ... @torch.no_grad() def step(self, closure=None): ...# 对 self.params 的修改，梯度清零等PyTorch 提供了方便的优化器 API，在 torch.optim 中（文档：https://pytorch.org/docs/stable/optim.html#algorithms），在搭建项目时，如无特殊需求，也不必自己写优化器： torch.optim.SGD：实现了 SGD、SGD + Momentum； torch.optim.Adagrad、torch.optim.RMSprop：实现了 AdaGrad、RMSProp； torch.optim.Adam、torch.optim.AdamW：实现了 Adam 及其一些改进算法。学习率调整器上述每个优化器都有学习率超参数，都可以设计学习率调整策略。学习率调整就是在训练过程中不断地改变学习率，这样看的话学习率就类似于一种参数。PyTorch 将学习率看做了一种参数，设计了专门针对这个“参数”的优化器——学习率调整器（learning rate scheduler），放在 torch.optim.lr_scheduler 中。它的使用方法与 Optimizer 类似，但也有一个重要的区别（这也是为什么单独设计一个类的原因）：lr_scheduler 类传入的不是超参数，而是优化器，以指数衰减调整器为例，它的实例化：from torch.optim.lr_scheduler import ExponentialLRoptimizer = SGD(model, 0.1)scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)这样是为了不想让学习率裸露在训练过程外面，而是始终与优化器捆绑在一起，为了代码的模块化。如果不是专门研究学习率，PyTorch 提供的 API 就够用了（见文档），这种东西用的不多，也几乎没有需要自己写的场合，所以知道有这种东西就差不多了。六、损失函数损失函数在形式伤也是一个 Python 函数，它接受数据预测值和真实值的 Tensor 类型的输入并输出一个数。在笔记（一）MLP 的从头开始实现中可以看到：def squared_loss(y_hat, y): return (y_hat - y) ** 2 / 2在 torch.nn.functional 中也有各种预定义的损失函数，见文档：https://pytorch.org/docs/stable/nn.functional.html#loss-functions与激活函数类似，损失函数也被 PyTorch 的高级 API 实现为一个 nn.Module 层，它们也放在 torch.nn 中（文档），与其他预定义的有实际参数的层（如 nn.Linear）并列。这里预定义的都是最基本、原始的损失函数，使用方法相对来说比较固定，例如： 交叉熵损失，用于分类问题：nn.CrossEntropyLoss()； 平方损失，用于回归问题：nn.MSELoss()； …在深度学习中，除了原始的损失函数，还会加正则项，实现其他目的，如防止过拟合、迁移学习的迁移、持续学习的防遗忘等。实现正则项的方式是自由的，可以与原始损失函数打包到一个 nn.Module 类（用 forward 函数嵌套的原理）或 Python 函数，也可以单独写成一个损失函数，在训练过程中计算损失的时候加进来：class RegLoss(nn.Module): def __init__(self, factor): ... self.factor = factor # 正则化系数 def forward(self, y_hat, y): reg_loss = ... # 使用 y_hat 和 y 定义的公式 return self.factor * reg_lossloss = nn.CrossEntropyLoss()reg = RegLoss(factor=FACTOR)# 训练过程for epoch: for batch: ... l = loss(net(X), y) + reg(net(X), y) l.backward() ......正则化系数可以放在训练过程外面，也可以像上面这样打包到损失函数里面。具体怎么实现，全看代码模块化程度的需要。值得注意的是 L2 正则化，因为它与修改梯度下降公式的权重衰减等价，除了在损失中实现 L2 正则项，还可以直接在优化器中实现。在 PyTorch 的高级 API 中，大部分优化器提供一个超参数：weight_decay，传入即可实现权重衰减。" }, { "title": "PyTorch 学习笔记（五）：计算性能", "url": "/posts/studynotes_PyTorch_computing/", "categories": "科研", "tags": "PyTorch, 读书笔记, 《动手学深度学习》, 机器学习, 技术", "date": "2022-02-11 00:00:00 +0800", "snippet": "本文介绍 PyTorch 与计算性能有关的代码知识，包括如何使用 GPU、并行计算、多服务器计算等等。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 5.6 节：GPU； 第 12 章：计算性能；深度学习与 GPU众所周知，深度学习计算可以使用 GPU，往往能极大提高效率。GPU 用于深度学习时与其他任务不同，它更偏向于关注显存而不是算力（显存至少 8G 以上，见“效率问题”一节），因此也没有一个像桌面级显卡那样的天梯图可供参考。这里列举比较有名的型号： Nvidia GeForce 系列：个人电脑显卡，目前到 30 系列，价格万元以下，一般装在个人电脑上，跑一些深度学习程序够用； Nvidia RTX A6000：一般装在服务器上，价格比较昂贵，一块几万元； Nvidia Tesla V100：一般装在服务器上，价格比 A6000 贵； Nvidia Tesla A100：比 V100 更强大，目前成为大公司主流使用的。跑深度学习的设备可以： 使用自己的电脑： 不带 GPU 的电脑（如 MacBook）：没有 GPU 的加持，只用 CPU 跑会跑得很慢； 带 GPU 的电脑（如 Windows 游戏本有 Nvidia 的独立显卡的）可以跑得快； 使用单台服务器（如组内的服务器）； 使用多台服务器（如大公司或机构公用的计算集群），方法见本文最后一节。PyTorch 与 CUDA深度学习框架为我们提供了使用 GPU 硬件的高级 API，只需简单的代码即可使用 GPU 作深度学习的计算，甚至无需了解原理。这些深度学习框架的高级 API 要与 GPU 打交道，但并不是直接打交道的。与普通的程序一样，通常调用操作系统提供的 SDK，操作系统与底层的 CPU 等硬件直接打交道；GPU 制造商也提供了类似 SDK 的接口，使用 GPU 的程序只需调用这个接口即可。Nvidia 公司的 GPU 提供的接口叫 CUDA，PyTorch 使用 GPU 的程序也是调用 CUDA 写的。所以要注意： 要使用 PyTorch GPU 计算，首先显卡本身要支持 CUDA。这要求至少是 Nvidia 的显卡； 要安装好 CUDA 才能使用 PyTorch。CUDA 理论上需要去 Nvidia 官网下载，但是不用担心，一般的机器只要有 Nvidia 的显卡，都是要安装显卡驱动的（否则显卡也没什么用），这个驱动里一般都带着 CUDA。没有 CUDA 的就安装驱动即可，也不用去手动下载。如果要手动安装 CUDA，要查好自己的 GPU 是否支持 CUDA，以及对应的 CUDA 版本。可以见 Nvidia 官方公布的对应列表：https://developer.nvidia.com/cuda-gpus； CUDA 有很多版本，PyTorch 也为不同 CUDA 版本写了不同的代码，在安装的时候必须指定正确（和机器上的 CUDA 版本一致），否则在实际运行时由于不兼容，会跑不通。安装了 CUDA 的机器，可以直接用命令 nvidia-smi 查看 CUDA 版本（见下）; PyTorch 版本也要和 CUDA 版本匹配（但不需要很严格），详见：https://pytorch.org/get-started/previous-versions/。 另外提一下，CUDA 是 GPU 完成通用计算任务的接口。GPU 一开始只是用来处理图像的，当时的接口只能完成图像处理任务；后来才开发出来其他用处，包括深度学习在内的并行计算，CUDA 就是也能完成这些任务的接口。可以通过 torch.cuda.is_available() 验证是否配置成功。查询与表示计算设备安装了 CUDA 的，用终端命令 nvidia-smi（已配置环境变量）可以查看此机器的 GPU 信息，包括名称、型号、显存以及正在占用的程序等。每个计算设备（包括 GPU、CPU）用 torch.device 对象表示，此类构造接受一个字符串参数，字符串只允许以下几个： torch.device(&#39;cpu&#39;)：表示 CPU； torch.device(&#39;cuda:i&#39;)：表示第 i 个 GPU（i 是自然数）； torch.device(&#39;cuda&#39;)：等价于 `torch.device(‘cuda:0’). CPU 是每台机器都有的，torch.device(&#39;cpu&#39;) 对象创建后总是与之绑定的；GPU 不是每台都有，而且可能也不会有多个，但仍然可以创建 torch.device{&#39;cuda:i&#39;} 对象，只是以后调用它时会报错指示 GPU 不存在。书中写了两个方便的函数，可以自动查询 GPU，提供了简单的纠错机制，防止出现上述错误。纠错机制是通过一个查询可用 GPU 数量的 API：torch.cuda.device_count()。 def try_gpu(i=0): &#39;&#39;&#39;如果可用，返回第 i 个 GPU 的设备对象（i 默认为 0）；否则返回 CPU 的设备对象&#39;&#39;&#39; if torch.cuda.device_count() &amp;gt;= i+1: return torch.device(f&#39;cuda:{i}&#39;) return torch.device(&#39;cpu&#39;)def try_all_gpus(): &#39;&#39;&#39;返回所有可用 GPU 的设备对象列表；否则返回 CPU 的设备对象（也是列表） &#39;&#39;&#39; devices = [torch.device(f&#39;cuda:{i}&#39;) for i in range(torch.cuda.device_count())] return devices if devices else [torch.device(&#39;cpu&#39;)] 将 Tensor 放到 GPU 上在没有涉及 GPU 时，PyTorch 所有 Tensor 都是存放在内存里，此内存与 CPU 挂钩，做计算时将其运送到 CPU 中处理。所以使用 GPU 做计算要做的事情很简单：只需将要用 GPU 计算的 Tensor 放到它的内存（称为显存）里，计算时自动会运送到 GPU 中处理。PyTorch 提供了简单的 API 将 Tensor 从不同计算设备来回转移： 在创建 Tensor 时只需为创建函数指定参数 device=，即可在指定设备创建，例：X = torch.ones(2,3,device=torch.device(&#39;cuda&#39;)； 只有同一个计算设备的 Tensor 才能一起计算，在 PyTorch 中如果试图对来自不同设备的 Tensor 做计算，会直接报错。深度学习计算涉及的 Tensor 有：数据、模型参数。只要所有的数据和参数都在同一计算设备上，就可以做训练了。 转移数据： 转移模型参数：模型和参数是绑定的，无需把参数从 nn.Module 对象取出来，PyTorch 的 API 形式上只需转移模型：net.to(device=torch.device(&#39;cuda&#39;))效率问题以下是一些关于效率的 tips： 在计算设备间转移 Tensor 时间开销是很大的，甚至比真正做计算都大很多。这也是 PyTorch 对不同设备计算直接报错，而不是尝试隐式转移到同一设备，这是为了防止用户发现不到问题造成大量的时间损失。 选择哪个计算设备要根据实际情况而定。有的机器 GPU 性能还不如 CPU，那就不要盲目地用 GPU 计算了。 而且在一个深度学习流程内，不是所有计算都放在 GPU 上效率才高。GPU 主要的优势是并行计算以及图像处理，主要是用在训练过程中，前面的预处理可能发挥不到 GPU 的优势。通常在临训练前才将数据转移到 GPU 上，但也有例外，如预处理需要处理图像。转移时机应根据实际情况把握。 GPU 在训练过程中优势主要发挥在一个 batch 的矩阵并行运算效率很高，所以 batch_size 开得越大越容易发挥计算上的优势。 数据 batch 和模型参数都占用了显存。因此 batch_size 越大占用显存越大，容易出现显存不够用的情况。所以减少显存占用的一种方法是将 batch_size 调小，但会牺牲 GPU 的计算效率；另一种方法是减少模型参数、选用更小的模型。PyTorch 并行训练上述涉及的都是在一个 GPU 上跑项目。当跑大型项目时，更需要在多个 GPU 上跑项目，甚至在多台机器上的多个 GPU，此时需要用到 PyTorch 并行训练的 API，前者称为 DP（Data Parallel），后者称为 DDP（Distributed Data Parallel）。目前我的计算资源不够，暂时用不到，不打算学习总结这一部分。感兴趣的可以自行了解，见 PyTorch Tutorial：https://pytorch.org/tutorials/intermediate/ddp_tutorial.html。附：集群中的 GPU使用多台服务器跑深度学习的情况，一般是大公司或机构公用的计算集群，即许多台服务器放在一个机房，它们通过统一的软件调度系统管理和使用。如果要使用，我们必须了解一些基本概念。在集群中： 一个节点（node）表示一台机器； 一个节点即一台机器一般有多个 GPU； 一个分区（partition）表示一群节点，通常是按照功能等分类。例如我们数学学院的集群分 cpu 和 gpu 两区，cpu 区的节点机器 CPU 性能比较好（为了计算数学专业的同学跑 MATLAB 程序用），gpu 区 GPU 性能比较好（为我们做深度学习、图形学的同学）；公司的集群有的会根据 gpu 的性能来分。集群的调度系统一般会限制用户不能直接登录具体的计算节点，而是先登录一个管理节点。在这个管理节点下，需要向调度系统申请使用计算节点。一般来说，用户需要提交一个申请命令，填写使用几个节点、GPU、多长时间等（一般不能指定使用哪个节点或哪个 GPU，这个是由调度系统分配的，不能我行我素），申请成功后即可以登录到计算节点。调度系统有一个非常厉害的功能就是，申请的计算节点和管理节点实时共用存储，内容是完全一样的，用户的数据只能存放在管理节点，并无需复制到计算节点，这一点要注意。其他关于调度系统的细节请自行查看你使用的集群的文档。" }, { "title": "PyTorch 学习笔记：工程性知识", "url": "/posts/studynotes_PyTorch_Projects/", "categories": "科研", "tags": "读书笔记, 《动手学深度学习》, 机器学习, 技术", "date": "2022-02-11 00:00:00 +0800", "snippet": "本文汇总使用 PyTorch 搭建项目时的一些边缘性的工程性知识，让代码真正地成为一个完整的深度学习项目。这部分内容包括如何可视化数据、读写训练进度等。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 5.5 节：读写文件；我也写过一个持续学习项目 HAT 的代码笔记，有助于理解一个完整的深度学习项目是如何写出来的，请参考：&amp;lt;&amp;gt;。读写训练进度深度学习程序的一个特点是运行时间长，一个任务经常需要跑几天、几个月。可以把深度学习程序比作 RPG 游戏，打通关时间长的 RPG 时我们需要定期存档，不仅为了下一次打开游戏时接续进度，还能预防电脑未响应、死机等突发情况导致游戏白打，甚至有时需要换台电脑玩这个进度、应当存档拷贝到新电脑；而且有时候会存多份档，为了预防游戏中某一次策略错误（如，买错了道具；打 boss 打不过去或者游戏有 bug 导致的陷入死循环，俗称坏档）导致的严重后果，起到后悔药的作用。大型的深度学习程序需要定期存档且存多份档，和上面是一个道理，不必多解释了。它与游戏的不同在于用户无法在运行过程中手动控制，只有停止程序这一个选择；定期存档的操作需要预先写进代码里。深度学习程序也是 Python 程序，当然可以用 Python 自带的文件读写功能，将变量保存于本地文件。但 PyTorch 为深度学习设计了专门更高级的 API，更加方便，最好使用这套 API。PyTorch 可以读写 Tensor 对象，nn.Parameter 对象，还可以是 {字符串:Tensor或Parameter} 的字典： torch.save(obj, path)：将对象 obj 存到路径为 path 的文件中； obj = torch.load(filename)：将 filename 文件存储的变量赋值到 obj。此类文件属文本文件，PyTorch 推荐使用扩展名 .pt,.pth（书中使用了 .params）。存档文件最好存储在项目单独的一个子目录下。深度学习最需要存档的东西是模型参数，它是训练的目标。网络结构无需保存，因为它就写在代码里，只需保存其参数即可。保存模型参数的推荐方法是存它的 .state_dict()（前面说过它是存所有模型参数的字典），因为 nn.Module 有一个方便的 API：net.load_state_dict(state_dict)，能将 state_dict 一步读取所有参数到模型 net 中。除了模型参数，还有一些必须存档的信息：当前 epoch 轮数，优化器里还有一些状态量（optimizer.state_dict()），如果用了调度器它也有状态量 scheduler.state_dict()，等等。可以将其统统打包成一个字典，类似下面的做法：checkpoint = { &#39;epoch&#39;: epoch, &#39;net&#39;: net.state_dict(), &#39;optimizer&#39;: optimizer.state_dict()}除此之外，为了方便，也可以打包进去其他需要记住的东西，如超参数、配置变量、当前 loss 等统计信息，等等。写到字典里是为了方便程序内使用，如果只是给人看一下，一些小的信息也可以传给 path，写到文件名内。下面讨论存档的频率。首先要说一点，为了实现多份存档，文件名最好不一样，防止覆盖。存档太频会浪费硬盘空间，例如一个 batch 或 epoch 一存；太不频则有更大的重新训练风险。而且并不是所有的档都需要存，和游戏一个道理，一般是在比较关键的进度存一下档。常见的做法是在训练循环体中设置条件判断语句写的检查点（checkpoint），判断是不是关键的存档。以下是一套完整的流程（引自知乎，作者“”人类之奴）：start_epoch = -1# 如果接续训练（RESUME=1），则加载 checkpointif RESUME: path_checkpoint = checkpoint_path checkpoint = torch.load(path_checkpoint) start_epoch = checkpoint[&#39;epoch&#39;] net.load_state_dict(checkpoint[&#39;net&#39;]) optimizer.load_state_dict(checkpoint[&#39;optimizer&#39;]) scheduler.load_state_dict(checkpoint[&#39;lr_schedule&#39;])for epoch in range(start_epoch+1, num_epochs): train(net, train_loader) test_loss = test(net, test_loader) # 检查点：测试集 loss 小于一定阈值。epoch 小于一半总训练轮数时认为训练不够，不设检查点 min_loss_val = 1 if epoch &amp;gt; int(num_epochs/2) and test_loss &amp;lt;= min_loss_val: min_loss_val = test_loss checkpoint = { &#39;loss&#39;:test_loss, &#39;epoch&#39;:epoch, &#39;net&#39;:net.state_dict(), &#39;optimizer&#39;:optimizer.state_dict(), &#39;lr_schedule&#39;:scheduler.state_dict()} if not os.path.isdir(r&#39;tf-logs/&#39;+save_model): os.mkdir(r&#39;tf-logs/&#39;+save_model) torch.save(checkpoint,r&#39;tf-logs/&#39;+save_model+&#39;/ckpt_best_%s.pth&#39;%(str(epoch+1))) 读写功能除了上述存档接续训练进度，还有其他常见的应用场景：例如保存训练好的参数给别人使用。常见的大型网络可以使用别人预训练的权重，再在自己的任务上微调，这些预训练权重通常保存在 .pth 文件中，从网上下载。可视化深度学习里很多内容需要可视化，辅助研究并呈现结果，例如： 数据集中的数据； 网络结构； 学习曲线、指标的变化曲线等。为实现此目的，除了可以手动调用例如 Matplotlib 等天然的可视化工具，深度学习框架也开发了自己的可视化工具。本章介绍 TensorBoard 的使用，它是 TensorFlow 的可视化工具，目前也支持了 PyTorch。TensorBoardTensorBoard 逻辑TensorBoard 的逻辑可以看成一个画家，以及一个画布，给画家各种作画指示，它就会按要求在画布上作出各种图。工具的两个部分： 画家和各种作画指示：在 torch 库中，存放在 torch.utils.TensorBoard。画家是 TensorBoard.SummaryWriter 类，作画指令就是类方法 add_xx(...)（xx 表示各种支持的内容，例如 scalar、graph 等），每调用一次就会在画布上画方法参数中对应的内容； 画布：是一个本地软件，在本地端口运行（浏览器打开，类似于 Jupyter Notebook），需要额外安装。必须启动画布，才能看到画家作的画。问题来了，画家和画布是两个程序，画布怎么知道画家的作画内容呢？这是通过日志实现的。画家作画其实是输出了一些画布能读懂的日志，画布通过输入日志来呈现画家的作画。这些日志存放在文本文件（称为日志文件），并通常放于专门的日志目录下（在代码中，画家和画布都是从指定目录下输出、输入日志），使用时应当为画家和画布指定相同的日志目录。命令总结： 安装画布：conda install tensorboard； 启动画布：tensorboard --logdir=log（runs 为日志目录，必须指定），并按提示打开浏览器端口； 召唤画家： from torch.utils.tensorboard import SummaryWritersummaryWriter = SummaryWriter(log_dir=&#39;log&#39;) # 实例化画家，log_dir 为日志目录 日志文件的组织方式：每运行一次（一个 “run”，即每实例化一个画家 SummaryWriter）都会产生一个新的日志文件。日志文件中记录了时间、设备等元信息与该画家的作画内容信息。画布会呈现所有日志文件所画内容的并集（可以在界面左下角选择部分的 “run” 显示），因此画家之间唯一的区别方式就是日志目录。TensorBoard 能画什么官方文档：https://pytorch.org/docs/stable/TensorBoard.htmlTensorBoard 通过 SummaryWriter 类的 add_xx 方法来画不同的内容，呈现在画布的各个版块上（上方选项卡），每个版块都有包含若干子版块（右方）；画布是交互式的而非静态，可以在画布上进一步调整可视化的效果，甚至导出（左方）。add_xx 方法有共同的参数： tag 参数：这部分内容的名字（字符串），必须指定。 walltime 参数：默认为系统时间 time.time()。可以在 TensorBoard 界面红可视化这一信息。在画布 TIME SERIES 版块可以查看所有作画历史记录，会将调用的 add_xx 作出的内容按照该时间顺序排列。以下是 TensorBoard 能画的东西（详细用法见文档，我只总结核心的东西）： 画曲线 add_scalar 呈现在画布 SCALARS 版块； 在曲线 tag 上添加一个坐标为 (global_step, scalar_value) 的点（注意 global_step 必须为整数）； 不同的曲线画在不同的图上，一个子版块对应一个图；add_scalars 可以把多个曲线画在同一个图上； 同理可以画直方图：add_histogram、二维图 add_mesh 等。 画图像：add_image 呈现在画布 IMAGES 版块； 在 IMAGES 板块的子版块 tag 上打印格式为 Tensor 类型的图像 img_tensor； add_images 可以在一个子版块打印多个图像； 同理可以画其他数据：音频 add_audio，文本 add_text，视频 add_video，Matplotlib 的 figure add_figure； 画表格：add_hparams 呈现在画布 HPARAMS 版块； 一次调用就添加一条表格记录（表格的一行）； 传入字典，字典的键对应属性，值对应属性值； 应传入两个字典，一个是自变量，一个是因变量；区分它们的意义是画布有对因变量做数据分析的交互功能； 画计算图：add_graph 画 tensor 中存储的计算图； 传入 torch.nn.Module 模型即可； 画 PR 曲线：add_pr_curve 传入预测标签和真实标签，会自动计算准确率和召回率； 一次调用就在 PR 坐标上添加一个点。 在低维空间（不超过 3 维）上展示高维数据：add_embedding 采用的降维方法是 t-SNE，是数据可视化常用的降维方法； 传入数据矩阵 Tensor； 可以传入类别标签，则会以不同颜色显示类别；也可以传入其他类型的标签如字符串乃至图像，则图中的数据点会显示字符串或图像。 TensorBoard 在深度学习中的用处从上面看，TensorBoard 和通用的可视化工具的功能与逻辑差别不大。但它一开始就是为深度学习可视化设计的，主要兼容深度学习框架 Tensor 类型的数据，设计的可画内容都是深度学习需要可视化的。深度学习需要可视化： 数据：add_images, add_video 等； 学习曲线、loss 曲线等指标（随时间变化）：add_scalar； 网络结构图：用 add_graph； 不同超参数的效果比较：add_hparams（顾名思义，add_hparams 画表格就专门为超参数这事的）； 在低维空间可视化模型中间层 Embedding：add_embedding； PR 曲线：add_pr_curve。深度学习的程序往往耗时很长，需要有存档机制，在代码中设置一些检查点（checkpoint），保存、加载训练到一半的模型参数等在这篇笔记中有详细的讨论。TensorBoard 也需要有存档机制，根据日志文件的组织方式——每次运行都会保存一个日志文件，画布会加载日志目录下的所有日志文件，所以我们无需手动保存、加载 TensorBoard 画图的进度。##本文介绍 Python logging库的使用方法，该库是 Python 中更复杂的调试工具，可以将程序的调试信息输出为格式更丰富的日志或日志文件，适合大型项目（例如深度学习）的调试与监控。loggingtqdm超参数优化（待更新）对于很大的实验，在真正开始训练前，最好对代码做一个完整的检查。学习曲线过拟合、欠拟合的判断。随机数种子深度学习有很多地方涉及随机数： 数据增强； 持续学习里构造数据集的 Permuted 操作；为了避免每次实验因为随机数的原因都不一样，也为了别人能够复现，一个完整的深度学习项目通常要设定随机数种子（seed）。计算机里的随机数生成算法都是伪随机数，算法接受一个随机数种子作为输入，通过固定的计算过程模拟某个分布得到这种随机数的。因此，一但随机数种子被人为设定，生成的随机数也就固定了，设定随机数种子起到固定随机数的作用。Python 中凡是随机算法的程序都有设定种子的接口，包括两种： 局部变量设定：随机数函数一般有指定种子的参数如 random_state； 全局设定：对随机数模块调用 seed 函数；深度学习程序中，最省心的做法是在程序开头对所有随机数模块（包括 Python 内置 random 模块、numpy.random、torch 等），也一般打包成函数：def setup_seed(seed): random.seed(seed) np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed_all(seed) torch.backends.cudnn.deterministic = Truesetup_seed(0) # 调用 使用随机数种子应当注意： 设定随机数种子会拖慢程序，对运算量大的深度学习程序有影响； 随机数的随机性一般对深度学习结果的影响不会太大； 随机数种子是依赖于机器的，设定同样的种子，在不同的机器上结果会不同。应当根据需要，决定需不需要设定种子。 " }, { "title": "闽南语学习笔记：语音系统", "url": "/posts/studynotes_Minnan_pinyin/", "categories": "有趣的事情", "tags": "学习笔记, 语言", "date": "2022-02-10 00:00:00 +0800", "snippet": "在中国的方言中，粤语和闽南语属于是有一整套体系的。我一个正宗北方人，最近对这些总也听不懂的语言产生了好奇，想了解一下。以我的直观感受，我觉得粤语听起来很飒、意气风发，闽南语更加温婉、亲切，有很重的烟火气。个人还是更喜欢闽南语的腔调，这篇文章就是我这个北方人试图牙牙学语总结的东西，应该是很生硬、不地道的，闽南和台湾的朋友见笑啦～ 不过我是很认真在学哦！我在百度上是在搜不出系统的闽南语资源，想学都没地方学啊！难道是一直在推广普通话的缘故？然而 Google 一下发现了新天地，对岸很重视闽南语的教育，官方就有相当多而且系统的学习资源，甚至还有组织考试！我能利用的所有很好的学习资源都在这个考试的官网，也是本文的主要参考资料。本文目的是搞定闽南语的语音系统。（文字都是汉字，除了个别特有的字词，就不用搞定啦）我想把重点放在与普通话的对比上，另外闽南话是唐代从河南传过去的，被称为古汉语活化石，对比可能还能一窥古汉语的面貌。台罗拼音汉语的标音最常用的是拼音，从清朝外国人发明的威妥玛拼音到现在使用的拼音，它是一个很好的工具，还起到把汉语罗马化的作用，和西方世界接轨。和大陆的汉语拼音方案一样，在闽南语上对岸也发明了一套系统的拼音方案——台湾闽南语罗马字拼音方案，简称台罗拼音。和普通话拼音类似，台罗拼音同样有声母、韵母、音调等体系，而且有很多和普通话非常相似。除了两岸习惯导致某些记号上的不同，闽南话和普通话还是能找到很多对应的。但这种对应只为方便学习，切不可按普通话的习惯去发音。台罗拼音官方标准见这里。韵母普通话的韵母分单韵母、复韵母。单韵母就是 a o e i u ü，复韵母是两个单韵母的组合，或单韵母与鼻音（-n,-ng）的组合。闽南语的单韵母也是 6 个： a i u e o oo，和汉语拼音不是很相像（读一下就知道了），更像是英语的 /ʌ/（=a） /ɪ/（=i） /ʊ/（=u） /e/（=e） /o/（=oo） 加了一个 /ə/（=o）。可以看到，普通话里的 ü 在闽南话里没有。复韵母也是两个单韵母的组合，或单韵母与其他东西的组合，其组合方式和汉语拼音是一个原理。两个单韵母的组合（见下图，不需要刻意记），看似没什么区别，但因为单韵母本身与普通话有区别，所以听起来也很不一样。单韵母与其他东西的组合，比普通话情况复杂得多。普通话里“其他东西“只有前后鼻音（-n, -ng)，闽南话有： 鼻音： -m, -n, -ng 鼻化音：-nn 入声韵母：-p, -t, -k, -h此外，m 和 ng 还能单独当作韵母（称为声化韵母，不归为单韵母）。这几个音的发音是难点。我的理解是： 鼻音：就按普通话习惯来 鼻化音：韵母加了 -nn 后，就是靠前发音，要有咧开嘴笑起来的感觉；不加 -nn 靠后发音，倾向于山东人说话后鼻音的方式。（这个音大概是闽南台湾同学讲话很嗲很可爱的原因吧。要注意和 n 没有任何关系！） 入声：-p,-t,-k,-h 就是在后面加上 p, t, k, h 的音，但是只做口形不发声，短促，有卡住的感觉。声母闽南话的声母比较简单，基本上是普通话声母的子集，即它有的普通话几乎都有。在下表中没见过的都是对岸的另一种记法而已，例如加个 h 是清化的音（ph 相当于普通话 p），不加是浊化的音；ts 类似普通话的 c，等等。即然学的是人家的拼音方案，就努力适应这些记法吧。我们一点点对比普通话的声母： b p m f：p 对应 b，ph 对应 p，m 对应 m，f 对应…？没错！闽南话里没有 f 这个音（这可能是福建口音不分 f、h 的原因）！ d t n l：t 对应 d，th 对应 t，n对应 n，l对应…l？错了！闽南语没有普通话 l 这个音。这里的l是加重版的普通话 d（平时听闽南话好多 d 开头的音，难道是这个原因？） g k h：k 对应 g，kh 对应 k，h 对应 h。完美！ j q x：闽南话里没有jqx的音！（好像也是外国人发不出来的音hhh） z c s：ts 对应 z，tsh 对应 c，s 对应 s。完美！ zh ch sh r：都没有！这都是翘舌音！ y w：也没有，这两个可能是汉语拼音强行加上的，类似于半元音，可以用 i u 替代闽南语里多出来的普通话没有的声母： ng：n 的鼻音版。 b, l, g, j：加重版的普通话 b, d, g, z。注意 l 和 j 不是想象中的发音哦。声调这大概是最难的地方。普通话有 4 个声调，闽南话有 8 个声调：这套声调难就难在大致有三个高度（普通话有两个），很难把握，需要反复听来找感觉。这张图标的音高只是示意，不能生搬硬套！普通话有一个连续变调的规则：两个 3 声连在一起，前一个要读 2 声。闽南话有近 10 个连续变调规则（每个调都有一个规则）：还有很多没有列出、但在学词语时听到的变调规则。我理解的变调是为了让语言讲起来不呆板、有抑扬顿挫之感，所以对前一个音作了各种处理。我按照我的方式来记： 1 调放低：1变7，1接2变7 7 调放低：7变3 8 调放低：8变4，h音变长 2 调扳平：2变1，2接1变1，2接3变1，2接5变1，2接7变1 5 调扳平：5接非高调变7（我选择偏漳口音）。另外注意 5 调只有后面不接东西才有拐弯的感觉。 3 调拉高：3变2 4 调拉高：4变8，h音变长最后，闽南语也有轻声声调，在台罗拼音中以 -- 标注：-- 前的音重读，-- 后的音为轻声。好了闽南语的语音系统就了解完毕了，但整理完这个没什么用，还是要多听然后跟着念，大量练习啊。推荐去这个台罗拼音教学网好好看看。再之后就该像一年级小朋友一样，对着拼音，一个字一个字地学，一句话一句话地读了。积累字词，对话，正音，要花大量的工夫，这都是以后的事了。" }, { "title": "PyTorch 学习笔记（三）：自定义网络结构（nn.Module）", "url": "/posts/studynotes_PyTorch_nnModule/", "categories": "科研", "tags": "PyTorch, 读书笔记, 《动手学深度学习》, 机器学习, 技术", "date": "2022-02-08 00:00:00 +0800", "snippet": "本文介绍如何自定义模型，即 nn.Module 模块的逻辑与使用方法。由此可以搭建自己的深度网络结构。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 5.1、5.6 节：层和块，自定义层。nn.Module 官方文档：https://pytorch.org/docs/stable/generated/torch.nn.Module.html之前见到的所有高级 API 定义的模型全是使用 PyTorch 现有的模版：由 nn.Sequential() 包裹的 nn.Linear(), nn.Flatten()，用它们定义出的模型非常默认，不够灵活。设计更复杂的网络结构，这种默认的就无法满足需求了，需要自定义模型。在逻辑层面上，所有网络模型都是由块（block）组成的，块与块之间可以有各种顺序、嵌套、并列等关系。块中包含一个或多个层（layer）。在 PyTorch 的语义中，模型最小单位不是神经元而是层。通用 nn.Module 模版在代码写法上，所有块、层都是 nn.Module 对象，包括 PyTorch 现有的 nn.Linear(),nn.Flatten() 甚至 nn.Sequential()。下面是通用的自定义 nn.Module 对象的写法。class MyModule(nn.Module): def __init__(self, ???) super().__init__() self.参数 = ...??? def forward(self, X): return ... # X 的表达式定义的规则是，只要定义好前向传播 forward 函数，里面包含的是 torch 运算，再确保把这些运算的参数（即网络参数）封装到此模型的类中即可。forward 函数是核心，在自定义 nn.Module 对象时必须要写，__init__() 函数只是一个将 forward 函数所需变量绑定于此类的容器。下面做一个小试验，体会其重要性：M = nn.Module()M()M.forward()这段代码第二、三行都会报 NotImplementedError，提示 forward 函数未定义。为什么会这样？nn.Module 模块的源代码解释清楚了：def _forward_unimplemented(self, *input: Any) -&amp;gt; None: raise NotImplementedError(f&quot;Module [{type(self).__name__}] is missing the required \\&quot;forward\\&quot; function&quot;)class Module: #... forward: Callable[..., Any] = _forward_unimplemented def _call_impl(self, *input, **kwargs): forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward) #... result = forward_call(*input, **kwargs) #... return result __call__ : Callable[..., Any] = _call_impl #...nn.Module 设计的机制就是要求继承时必须重写一个 forward 函数。此外，可以直接调用 nn.Module 对象，其实就是给定输入 \\(x\\) 前向传播一遍得到预测结果，它由 __call__() 方法定义（见 Python 笔记），而实现中可以看到出现了 self.forward，所以不写 forward 函数，在训练时前向传播也会报 forward 函数未定义的错误。另外，在构造函数中必须调用父类 nn.Module 的构造函数：super().__init__()，为了把 nn.Module 定义的一些实例属性继承过来，只能这样写。写 nn.Module 对象时都要调用一下，否则会因缺少里面的属性报变量未定义的错误。感兴趣可以看看这些实例属性是什么：class Module: # ... def __init__(self) -&amp;gt; None: &quot;&quot;&quot; Initializes internal Module state, shared by both nn.Module and ScriptModule. &quot;&quot;&quot; torch._C._log_api_usage_once(&quot;python.nn_module&quot;) self.training = True self._parameters: Dict[str, Optional[Parameter]] = OrderedDict() self._buffers: Dict[str, Optional[Tensor]] = OrderedDict() self._non_persistent_buffers_set: Set[str] = set() self._backward_hooks: Dict[int, Callable] = OrderedDict() self._is_full_backward_hook = None self._forward_hooks: Dict[int, Callable] = OrderedDict() self._forward_pre_hooks: Dict[int, Callable] = OrderedDict() self._state_dict_hooks: Dict[int, Callable] = OrderedDict() self._load_state_dict_pre_hooks: Dict[int, Callable] = OrderedDict() self._load_state_dict_post_hooks: Dict[int, Callable] = OrderedDict() self._modules: Dict[str, Optional[&#39;Module&#39;]] = OrderedDict() #...有了以上模版，我们可以使用 nn.Module 写一个层，也可以写一个块，乃至块组成的一整个模型，非常灵活。网络的结构取决于 __init__() 和 forward 函数怎么写。以下每种情况分别给出两段代码：一段是调用现有的模版，另一段是自己继承 nn.Module 手写出来的；这两段代码写出来的效果是一样的。自定义层例 1：全连接层。现有的模版：net = nn.Linear(8, 128)等价于以下自定义的层：class MyLinear(nn.Module): def __init__(self, input_num, output_num): super().__init__() self.weight = nn.Parameter(torch.randn(input_num, output_num)) self.bias = nn.Parameter(torch.randn(output_num, )) def forward(self, X): return torch.matmul(X, self.weight.data) + self.bias.datanet = MyLinear(8, 128)例 2：ReLU 激活函数层，它是一个不带参数的层。现有的模版：relu = nn.ReLU()等价于以下自定义的层：class ReLULayer(nn.Module): def __init__(self): super().__init__() def forward(self, X): return nn.functional.relu(X)参数打包成 nn.Parameter 类后，直接定义为实例属性，forward 函数直接拿来用。实际使用时，一般很少自定义层，一般的网络都是使用那些常用层如全连接层、卷积层等，然后按照下面的方式组合成块。自定义块例：多层感知机（MLP）。现有的模版：net = nn.Sequential( nn.Linear(784, 256), nn.ReLU(), nn.Linear(256, 10))等价于以下自定义的层：class MLP(nn.Module): def __init__(self): super().__init__() self.hidden = nn.Linear(784, 256) self.out = nn.Linear(256, 10) def forward(self, X): return self.out(nn.functional.relu(self.hidden(X)))net = MLP()也就是说，一些定义了层的 nn.Module 对象能以这种方式嵌套进定义了块的对象。上面两者还有微小的区别：前者使用 nn.Sequential，用下标 net[0],net[1]索引各层，还能把激活函数当作层索引到；后者的层是放在实例属性上的，需要用 . 来索引。nn.Sequential 是一种特殊的 nn.Module 对象，如上所述它能起到顺序连接各层、充当列表的效果。它的原理可以参考书中的简单复现，由此例可以体会到自定义 nn.Module 对象的灵活性：class MySequential(nn.Module): def __init__(self, *args): super().__init__() for idx, module in enumerate(args): self._modules[str(idx)] = module def forward(self, X): for block in self._modules.values(): X = block(X) return X可以看到，它可以传入任意多个 nn.Module 对象（可变参数 args），将其顺序存储在 _modules（之前见过是 nn.Module 的实例属性，这里就派上用场了，是一个 collections.OrderedDict 容器），然后在 forward 函数中顺序复合到输入 X 上。（注意，这样使得激活函数也可作为可变参数传入。）自定义块组成的模型nn.Module 对象是可以一层一层地递归嵌套地定义的。以下是一个稍复杂的例子：class NestMLP(nn.Module): def __init__(self): super().__init__() self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(), nn.Linear(64, 32), nn.ReLU()) self.linear = nn.Linear(32, 16) def forward(self, X): return self.linear(self.net(X))chimera = nn.Sequential(NestMLP(), nn.ReLU(), nn.Linear(16, 10))现在思考，为什么可以这样嵌套呢？因为在前向传播时，会一层一层递归地调用被嵌套模块的 forward 函数。例如此例，调用时 chimera(X) 时，首先会调用 nn.Sequential 的 forward 函数，即依次调用 X = NestMLP()(X), X = nn.ReLU()(X), X = nn.Linear(16, 10)(X)，前面说过每一层都会调用各自 forward 函数，例如先调用 NestMLP() 的 forward，其中调用了 self.net(), self.linear()，它们又会调用里面的 nn.Sequential(...),nn.Linear(32, 16) 的 forward 函数。如此递归下去，直到遇到真正层里面的参数，例如 self.linear 里面的 weight。这种 forward 函数递归过程会把嵌套的每一块、层的参数都遍历到，从而能反向传播。此递归调用相当于遍历下面这颗树：有人会问，为何不用简单的：chimera = nn.Sequential(nn.Linear(20, 64), nn.ReLU(), nn.Linear(64, 32), nn.ReLU(), nn.Linear(32, 16), nn.ReLU(), nn.Linear(16, 10))这就要从方便性的角度考虑了。这种方便不只在命名上（见下）。在逻辑上前者是将网络分成了几个块，例如本例有点像 NestMLP 是特征提取器，后面的全连接层是分类器的意思，假如以后想换个特征提取器更方便。最后讨论一下命名问题。模型的每个块、层都有自己的名字（类似变量的命名空间），且可以通过以这个名字命名的实例属性访问。在这种封装的模型类中，嵌套关系的存在使得各块、层有树形关系。例如上面的 NestMLP 模型各块、层的名字如下： print() 函数能以一种规整的方式打印出网络结构（是由类特殊方法 __print__() 和 __repr__() 定义的，见 Python 笔记），会显示各块、层的名字、网络结构。也可以使用 Tensorboard 等工具可视化，见 Tensorboard 笔记。" }, { "title": "PyTorch 学习笔记（二）：自定义数据集，数据预处理", "url": "/posts/studynotes_PyTorch_Dataset_and_Transform/", "categories": "科研", "tags": "PyTorch, 机器学习, 《动手学深度学习》, 技术", "date": "2022-01-23 00:00:00 +0800", "snippet": "本文总结 PyTorch 中与数据集以及对它的预处理的知识。知乎的这篇文章讲得不错，言简意赅。也可参考官方教程。PyTorch 中的数据集都是定义了一个 torch.utils.data.Dataset 类型，数据集都是这个类型的实例。必须这样做，因为后面构造 Dataloader 只接收 Dataset 类型，而整个训练过程都是对 Dataloader 的操作。我们已经在笔记（一） 中学习了 Dataloader，所以本文专心于学习 Dataset。PyTorch 预定义了很多数据集，将其打包成 Dataset 类型，定义在 torchvision.datasets 中，常用基本的 MNIST、CIFAR、ImageNet 等数据集都有，更多的见文档：https://pytorch.org/vision/stable/datasets.html。但在实际项目中，大多需要用自己定制的数据集，这时需要自定义 Dataset 类型（例如持续学习里需要构造任务数据集）。通用 Dataset 模版自定义数据集就需要自己写 Dataset 类。一个数据集对应一个该类的实例。最简单的自定义 Dataset 需要定义三个方法：class MyDataset(Dataset): def __init__(self, ...): ... def __len__(self): ... return len # 返回数据集数据个数 def __getitem__(self, index): ... return image, label # 返回第 index 个数据 + 标签在 Python 笔记 中说过，__getitem__(),__len__() 属于特殊类方法，前者规定了 len() 函数作用在类实例上的返回值，后者规定了索引 mydataset[index] 的返回值。要注意的是，除了构造函数 __init__()，__geiitem__(),__len__() 也是必须实现的，因为数据生成器 Dataloader 的核心业务就是在调用这两个方法，入股不定义会报错。对 __geiitem__(),__len__() 的实现是灵活的，记住，不管你怎么存储数据集的数据（放在什么数据结构里），按什么顺序实现，只要把 __getitem__() 确实做了它该做的事情，就大功告成了。比如： 有人喜欢在 Dataset 构造时就把数据集全部读取出来（数据集本体存放在 __init__() 某个实例属性中），__getitem__() 直接索引即可； 还有的人喜欢在 __init__() 中只给本地文件路径，在调用 __getitem__() 时现场读取相应的数据。区别只是效率问题；几个细节： 数据集一般需要从网上下载，会有一个 download 参数，检查本地路径中否有数据集文件，如没有则执行下载操作。下载操作涉及很多网络通讯的代码，也一般是打包在一个 download() 函数中的； 数据集在定义时一般就区分训练和测试集，会有一个 train 参数，下文代码以判断 train=True 的条件语句读取训练还是测试集。数据预处理变换在 PyTorch 中，数据预处理都归结为设计变换函数 transform，形式上是一个 Python 函数，输入处理前的数据（一般是 Tensor），输出处理后的数据（保持维度不变）。这个变换函数作用到数据的方式是：包裹在 Dataset 类里，并在 __getitem__() 方法中作用到数据上，即索引到某数据时对其临时做预处理：class MyDataset(Dataset): def __init__(self, ..., transform, target_transform): ... self.transform = transform self.target_transform = target.transform ... def __getitem__(self, index): ... if self.transform: image = self.transform(image) if self.target_transform: label = self.target_transform(label) return image, label传入的 transfrom 和 target_transform 分别是对数据和标签的预处理变换。PyTorch 预定义了很多这种 Python 函数，完成预处理变换，称为函数型变换（functional transforms）：https://pytorch.org/vision/stable/transforms.html#functional-transforms。这里不展开讲解，因为更常用的实现方式是下述的可调用类。数据预处理变换通常是有一些超参数的，例如旋转变换的角度，等等。上述这种传参的方式，如果 transform 是普通的 Python 函数，那么这些超参数将无法一并传入，所以数据预处理变换一般实现为可调用类，在类的构造函数中包裹超参数。下例来自官方文档：class MyRotationTransform: def __init__(self, angles): self.angles = angles def __call__(self, x): angle = random.choice(self.angles) return TF.rotate(x, angle)PyTorch 也预定义了很多实用的预处理变换类，定义在 torchvision.transforms 中，包括数据标准化、降维、数据增强等，在文档中有详细描述：https://pytorch.org/vision/stable/transforms.html。其中常用的值得学习一下： 数据类型转换：记住一个即可，很多预定义的数据集都是 PIL 类型的（Python Pillow 库定义的类型），无法直接用于训练或测试，ToTensor()将其转换为 Tensor 类型； 数据标准化：Normalize()； 数据增强：提供了对图像的各种增强方法，如缩放（Resize()）、裁剪（CenterCrop()、FiveCrop()、RandomCrop()）、旋转（RandomRotation()）等。 Dataset 的这种构造方式似乎只能传一个变换，如果是自定义的可以把各种操作写在同一个复杂的变换里。对于上述预定义变换，如果要应用多个变换，PyTorch 也设计了一个 Compose() 变换，用于组合多个预定义的变换以方便传入 Dataset 的参数。PyTorch 预定义的数据集与预处理变换本节我们学习几个 PyTorch 预定义的例子，看看官方是怎么写 Dataset 和 transform 的，对我们自己写也会有所启发。官方教程示例第一个例子是官方 tutorial 里的示例：import osimport pandas as pdfrom torchvision.io import read_imageclass CustomImageDataset(Dataset): def __init__(self, annotations_file, img_dir, transform=None, target_transform=None): self.img_labels = pd.read_csv(annotations_file) self.img_dir = img_dir self.transform = transform self.target_transform = target_transform def __len__(self): return len(self.img_labels) def __getitem__(self, idx): img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0]) image = read_image(img_path) label = self.img_labels.iloc[idx, 1] if self.transform: image = self.transform(image) if self.target_transform: label = self.target_transform(label) return image, label这个 Dataset 子类可以从本地读取带标签的图像数据集。本地数据要求图像存放在一个 img_dir 目录下，另有一个标签数据文件 annotation_file，是 csv 文件，第一列为图像文件名，第二列为对应图像的标签。可以看到属于上面的第二种实现方式：在 __getitem__() 根据 annotation_file 的信息获得图像本地路径，现场读取图像。MNIST第二个例子是 torchvision.datasets 里的数据集 MNIST。这些 Dataset 类的一大特点是不仅可以读取本地的 MNIST 数据集，还能在本地文件存在时从指定网站上下载（通过 download 参数控制）。它属于第一种方式：在 __init__() 方法中事先将数据集本体存放在了实例属性 self.data,self.targets 中了，__getitem__() 直接索引这两个变量即可。注意读取进来的源格式是 PIL，通常机器学习需要转化为 Tensor，一般会传入 transform=ToTensor()。此外，这个代码加入了好多纠错机制，非常地健壮。class MNIST(VisionDataset): &quot;&quot;&quot;`MNIST &amp;lt;http://yann.lecun.com/exdb/mnist/&amp;gt;`_ Dataset. Args: root (string): Root directory of dataset where ``MNIST/raw/train-images-idx3-ubyte`` and ``MNIST/raw/t10k-images-idx3-ubyte`` exist. train (bool, optional): If True, creates dataset from ``train-images-idx3-ubyte``, otherwise from ``t10k-images-idx3-ubyte``. download (bool, optional): If True, downloads the dataset from the internet and puts it in root directory. If dataset is already downloaded, it is not downloaded again. transform (callable, optional): A function/transform that takes in an PIL image and returns a transformed version. E.g, ``transforms.RandomCrop`` target_transform (callable, optional): A function/transform that takes in the target and transforms it. &quot;&quot;&quot; mirrors = [ &quot;http://yann.lecun.com/exdb/mnist/&quot;, &quot;https://ossci-datasets.s3.amazonaws.com/mnist/&quot;, ] resources = [ (&quot;train-images-idx3-ubyte.gz&quot;, &quot;f68b3c2dcbeaaa9fbdd348bbdeb94873&quot;), (&quot;train-labels-idx1-ubyte.gz&quot;, &quot;d53e105ee54ea40749a09fcbcd1e9432&quot;), (&quot;t10k-images-idx3-ubyte.gz&quot;, &quot;9fb629c4189551a2d022fa330f9573f3&quot;), (&quot;t10k-labels-idx1-ubyte.gz&quot;, &quot;ec29112dd5afa0611ce80d1b7f02629c&quot;), ] training_file = &quot;training.pt&quot; test_file = &quot;test.pt&quot; classes = [ &quot;0 - zero&quot;, &quot;1 - one&quot;, &quot;2 - two&quot;, &quot;3 - three&quot;, &quot;4 - four&quot;, &quot;5 - five&quot;, &quot;6 - six&quot;, &quot;7 - seven&quot;, &quot;8 - eight&quot;, &quot;9 - nine&quot;, ] @property def train_labels(self): warnings.warn(&quot;train_labels has been renamed targets&quot;) return self.targets @property def test_labels(self): warnings.warn(&quot;test_labels has been renamed targets&quot;) return self.targets @property def train_data(self): warnings.warn(&quot;train_data has been renamed data&quot;) return self.data @property def test_data(self): warnings.warn(&quot;test_data has been renamed data&quot;) return self.data def __init__( self, root: str, train: bool = True, transform: Optional[Callable] = None, target_transform: Optional[Callable] = None, download: bool = False, ) -&amp;gt; None: super().__init__(root, transform=transform, target_transform=target_transform) self.train = train # training set or test set if self._check_legacy_exist(): self.data, self.targets = self._load_legacy_data() return if download: self.download() if not self._check_exists(): raise RuntimeError(&quot;Dataset not found. You can use download=True to download it&quot;) self.data, self.targets = self._load_data() def _check_legacy_exist(self): processed_folder_exists = os.path.exists(self.processed_folder) if not processed_folder_exists: return False return all( check_integrity(os.path.join(self.processed_folder, file)) for file in (self.training_file, self.test_file) ) def _load_legacy_data(self): # This is for BC only. We no longer cache the data in a custom binary, but simply read from the raw data # directly. data_file = self.training_file if self.train else self.test_file return torch.load(os.path.join(self.processed_folder, data_file)) def _load_data(self): image_file = f&quot;{&#39;train&#39; if self.train else &#39;t10k&#39;}-images-idx3-ubyte&quot; data = read_image_file(os.path.join(self.raw_folder, image_file)) label_file = f&quot;{&#39;train&#39; if self.train else &#39;t10k&#39;}-labels-idx1-ubyte&quot; targets = read_label_file(os.path.join(self.raw_folder, label_file)) return data, targets def __getitem__(self, index: int) -&amp;gt; Tuple[Any, Any]: &quot;&quot;&quot; Args: index (int): Index Returns: tuple: (image, target) where target is index of the target class. &quot;&quot;&quot; img, target = self.data[index], int(self.targets[index]) # doing this so that it is consistent with all other datasets # to return a PIL Image img = Image.fromarray(img.numpy(), mode=&quot;L&quot;) if self.transform is not None: img = self.transform(img) if self.target_transform is not None: target = self.target_transform(target) return img, target def __len__(self) -&amp;gt; int: return len(self.data) @property def raw_folder(self) -&amp;gt; str: return os.path.join(self.root, self.__class__.__name__, &quot;raw&quot;) @property def processed_folder(self) -&amp;gt; str: return os.path.join(self.root, self.__class__.__name__, &quot;processed&quot;) @property def class_to_idx(self) -&amp;gt; Dict[str, int]: return {_class: i for i, _class in enumerate(self.classes)} def _check_exists(self) -&amp;gt; bool: return all( check_integrity(os.path.join(self.raw_folder, os.path.splitext(os.path.basename(url))[0])) for url, _ in self.resources ) def download(self) -&amp;gt; None: &quot;&quot;&quot;Download the MNIST data if it doesn&#39;t exist already.&quot;&quot;&quot; if self._check_exists(): return os.makedirs(self.raw_folder, exist_ok=True) # download files for filename, md5 in self.resources: for mirror in self.mirrors: url = f&quot;{mirror}{filename}&quot; try: print(f&quot;Downloading {url}&quot;) download_and_extract_archive(url, download_root=self.raw_folder, filename=filename, md5=md5) except URLError as error: print(f&quot;Failed to download (trying next):\\n{error}&quot;) continue finally: print() break else: raise RuntimeError(f&quot;Error downloading {filename}&quot;) def extra_repr(self) -&amp;gt; str: split = &quot;Train&quot; if self.train is True else &quot;Test&quot; return f&quot;Split: {split}&quot;torchvision.transforms.Normalize()第三个例子是经典的数据标准化 torchvision.transforms.Normalize()。可以看到，这个可调用类只是一个壳，真正的实现包裹在了 F.normalize() 这个函数中。我们在写自己的变换时也最好这样模块化，这是一个好习惯。另外一个有趣的地方是，这个变换类继承自 nn.Module，也就是说，它也可以当做一个网络层，放在由 nn.Module 组织的网络结构里使用。这样的二用，巧妙地省去了很多代码。（这也是为什么需要提供均值、方差两个参数的原因，数据预处理一般默认是数据集的均值、方差，而在网络结构中，它们可能是要学习的参数。）class Normalize(torch.nn.Module): &quot;&quot;&quot;Normalize a tensor image with mean and standard deviation. This transform does not support PIL Image. Given mean: ``(mean[1],...,mean[n])`` and std: ``(std[1],..,std[n])`` for ``n`` channels, this transform will normalize each channel of the input ``torch.*Tensor`` i.e., ``output[channel] = (input[channel] - mean[channel]) / std[channel]`` .. note:: This transform acts out of place, i.e., it does not mutate the input tensor. Args: mean (sequence): Sequence of means for each channel. std (sequence): Sequence of standard deviations for each channel. inplace(bool,optional): Bool to make this operation in-place. &quot;&quot;&quot; def __init__(self, mean, std, inplace=False): super().__init__() _log_api_usage_once(self) self.mean = mean self.std = std self.inplace = inplace def forward(self, tensor: Tensor) -&amp;gt; Tensor: &quot;&quot;&quot; Args: tensor (Tensor): Tensor image to be normalized. Returns: Tensor: Normalized Tensor image. &quot;&quot;&quot; return F.normalize(tensor, self.mean, self.std, self.inplace) def __repr__(self) -&amp;gt; str: return f&quot;{self.__class__.__name__}(mean={self.mean}, std={self.std})&quot;def normalize(tensor: Tensor, mean: List[float], std: List[float], inplace: bool = False) -&amp;gt; Tensor: _assert_image_tensor(tensor) if not tensor.is_floating_point(): raise TypeError(f&quot;Input tensor should be a float tensor. Got {tensor.dtype}.&quot;) if tensor.ndim &amp;lt; 3: raise ValueError( f&quot;Expected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = {tensor.size()}&quot; ) if not inplace: tensor = tensor.clone() dtype = tensor.dtype mean = torch.as_tensor(mean, dtype=dtype, device=tensor.device) std = torch.as_tensor(std, dtype=dtype, device=tensor.device) if (std == 0).any(): raise ValueError(f&quot;std evaluated to zero after conversion to {dtype}, leading to division by zero.&quot;) if mean.ndim == 1: mean = mean.view(-1, 1, 1) if std.ndim == 1: std = std.view(-1, 1, 1) tensor.sub_(mean).div_(std) return tensor" }, { "title": "PyTorch 学习笔记（一）：自动微分，简单模型的实现", "url": "/posts/studynotes_PyTorch_autograd_and_pipeline/", "categories": "科研", "tags": "PyTorch, 读书笔记, 《动手学深度学习》, 机器学习, 技术", "date": "2022-01-22 00:00:00 +0800", "snippet": "本系列博文是我学习深度学习框架的学习笔记。深度学习框架大同小异，只须学习一种的原理，其他的都可以快速上手。我使用的是 PyTorch。笔记将着重强调代码原理、思想的理解，而不是具体的代码。 PyTorch 官方文档：https://pytorch.org/docs/stable/index.html PyTorch 中文文档：https://pytorch-cn.readthedocs.io/zh/latest/本系列博文的参考书为亚马逊团队编写的 Dive into Deep Learning (PyTorch 版)，编排顺序基本遵从此书。导师最近很推荐这本书。这是一本把深度学习从头开始讲的技术书，虽然大部分内容是会的，但难得找到一本书在代码上讲得系统，看一遍也是很有好处的。我计划是利用寒假时间看一看，整理出一套笔记。B 站上有李沐 主讲的配套课程可供参考。本文介绍深度学习框架的基本数据结构——Tensor 及其核心功能——自动微分，并搭建几个最简单的监督学习模型，主要参考书的： 2.5 节：自动微分； 3.2-3.3 节：线性回归的从零开始实现、简洁实现； 3.5-3.7 节：Softmax 多分类的从零开始实现、简洁实现； 4.1-4.3 节：多层感知机（MLP）的从零开始实现、简洁实现。在开始前还是提示一下如何安装 PyTorch。去官网翻到 Install PyTorch，根据自己机器的系统等信息选择后，用下面生成的指令安装。如果不想用或没有 GPU，选择 CPU 版本即可；如果想用，请参考 PyTorch 学习笔记：使用 GPU，了解 CUDA 的意思后选择合适的 CUDA 版本安装。安装过程如果报错，尝试使用国内镜像，参见 Conda 学习笔记。基本数据结构：TensorPyTorch 是深度学习框架，预备知识一定是基本的数据结构、数据操作。张量（Tensor）是 PyTorch 的基本数据结构，它的性质和用法就是数学上的张量，在这篇博文已详细讲述。书中 2.1,2.3,2.4 等节大部分篇幅在讲述 Tensor 的基本用法，这些与 Numpy 也是一致的，就跳过了。这篇博文也总结过，PyTorch 和 Numpy 的基本数据结构本质都是数学上的张量，而且 PyTorch 是基于 Numpy 的，为什么还要自己封装一个 Tensor 类型？书中第 2 章开头总结的不错，PyTorch 在 Tensor 中融入了深度学习相关的功能： 在 GPU 上加速计算（Numpy 只能在 CPU）； 储存梯度、计算图等信息，实现自动微分功能。自动微分自动微分（求导）是深度学习框架的主要功能，顾名思义就是给出一个函数后，即可直接算出在某点的导数（梯度）值（注意，并不是待求导函数的表达式）。计算图、链式法则是自动微分基于的原理，但也不需要搞明白其底层实现方式，只要会用即可。 自动微分只能完成数值计算，而不是fu只能求在某点的导数值，而不能求出导函数的表示需要理解的是，自动微分功能是实现在 Tensor 里的，自动微分的计算过程和结果都是存的 Tensor 的属性中的： grad_fn：存放待求导函数（的计算图）； grad：存放求得的导数向量（Tensor）。这个 Tensor 即为被求导点。假设要求 \\(\\frac{\\operatorname{d} y}{\\operatorname{d} \\mathbf{x}}_{\\mathbf{x}=\\mathbf{x}_{0}}\\)，以求 \\(y = 2\\mathbf{x}^T \\mathbf{x}\\) 在 \\(x_0 = (0,1,2,3)^T\\) 点的梯度为例，完整的自动微分过程如下： 定义 \\(x_0\\)：将 \\(x_0\\) 点的值以 tensor 的形式赋给变量 x x = torch.arange(4.0) 开启求导模式：把 tensor x 的 requires_grad 属性设为 True x.requires_grad_(True) 求导模式可以在 Tensor 构造时即刻开启，只需在构造的函数传入参数 requires_grad=True。例如上面 1,2 两步可合为 x = torch.arange(4.0, requires_grad=True)。 定义被求导函数 \\(y\\)：将含 x 的 torch 表达式赋给变量 y （此时 tensor y 存放了计算图） y = 2 * torch.dot(x, x) 求导：调用 y 的 backward 方法，导数值存放在 x 的 grad 属性中（与 x 维数相同） y.backward() 注意点： 存放求导结果的 grad 属性是累加的：第一次求导前默认为 0，求导后将结果叠加到 0 上，第二次求导后会叠加到第一次的结果上。所以如需反复求导一定要清零。清零的方法： x.grad.zero_() 被求导函数可以额外打包成一个 Python 函数赋给 y（只要函数里面用的都是 torch 的表达式）； 构建计算图极容易粗心，一定注意好求导模式的开关，不在不该的地方引入计算图。除了修改 requires_grad_ 属性，还可以： 全局地关闭求导模式，用以下代码包裹： with torch.no_grad(): 分离：即去掉 grad_fn 存放的计算图，只保留 tensor 值。以下代码将 y 分离成 u： u = y.detach() 上面求导要求 \\(y\\) 必须是标量，而 \\(x\\) 可以是向量。事实上 \\(y\\) 也可以是向量，需要在 backward 函数中加参数，见下例。 使用自动微分工具可以画一个函数导数的图像，参见书第 4 章画各种激活函数及其导数。例，Sigmoid 函数： import matplotlib.pyplot as pltx = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)y = torch.sigmoid(x)y.backward(torch.ones_like(x))plt.plot(x.detach(), x.grad) 注意此时 y 是向量，要令其每一维对 x 每一维分别求导，须在 backward 函数中加 torch.ones_like(x) 参数。深度学习模型的 Pipeline深度学习的完整流程如下： 数据预处理； 定义模型、损失函数、优化器、初始化等； 训练模型； 测试模型。以下各节具体讲解细节。在 PyTorch 实现中，以上每一步都包含许多值得单独讲的专题。作为学习笔记系列的第一篇，先从简单模型出发，将这些流程实现一遍，好对 PyTorch 有个整体的认识。将介绍三个简单模型，分别是： 线性回归； Softmax 多分类； 多层感知机（MLP）。每个模型都分从头开始实现和简洁实现两种实现方法。简洁实现是调用 PyTorch 提供的高级 API，从头开始实现是自己写训练过程等细节，仅利用 PyTorch 的自动微分功能。这样有利于理解深度学习框架相比于其他包为深度学习带来的极大方便。一、线性回归本节欲训练线性回归模型：\\(\\mathbf{y} = \\mathbf{X}\\mathbf{w} + b + \\epsilon\\)。PyTorch 作深度学习使用的数据集都是它定义的 Dataset 类型。这里用到的数据暂时不涉及该类型，而是手动生成的普通的 Tensor。本例生成方法：给定 \\(\\mathbf{w}, b\\) 的真实值，按正态分布（torch.normal）生成 \\(\\mathbf{X}, \\mathbf{y}\\)，用一个 synthetic_data(w, b, num_examples) 函数实现（可以自己写一下试试，练练 Tensor 的使用）。从头开始实现深度学习通常是按批（batch）训练的，因此数据 \\(\\mathbf{X}, \\mathbf{y}\\) 还需按一定的批数据量（batch_size）划分成各批数据。代码没有简单地切片成 batch 并存到列表里，而是通过生成器（generator）生成（参考我的 Python 笔记），这样的好处是每次训练需要时调用一次生成器，它现场给你生成新的一个 batch 的数据（是这些数据拼接成的矩阵），而无需一开始就划好，否则一开始切片 batch 这种预处理工作就要花费很多时间，会使训练过程迟迟不能开动。生成器函数 data_iter(batch_size, features, labels) 也可以自己试试，要注意两个细节：shuffle 的实现只需 shuffle 索引；如何 num_examples 不能整除 batch_size，尾部如何处理。先看训练框架：for epoch in range(num_epochs): for X, y in data_iter(batch_size, features, labels): l = loss(net(X, w, b), y) l.sum().backward() sgd([w, b], lr, batch_size) with torch.no_grad(): train_l = loss(net(features, w, b), labels) print(f&#39;epoch {epoch + 1}, loss {float(train_l.mean()):f}&#39;)中间的三句分别是前向传播、后向传播、梯度下降。每次从生成器生成一个 batch 的数据训练用。最外层循环为轮数，每一轮结束都要统计一下当前训得的模型在整个训练集上的 loss。 统计的时候不需求导引入计算图，可以用 with torch.no_grad(): 包裹，纯粹为了减少计算量，不包裹也不会出错（例如下面的简洁实现就没有包裹）。但是有的地方引入计算图会引起混乱，如下面 sgd() 函数里的梯度下降更新式，一定要包裹。训练过程就是不断更新参数 w,b，中间三句是如何更新的呢？答案就是自动微分。在此代码前应对 w,b 作初始化：w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)b = torch.zeros(1, requires_grad=True)开启求导模式后，定义被求导函数，在深度学习中就是损失函数 \\(l = \\sum_{i=1}^{batch_size} loss(net(X, y, b))\\)。net() 是模型函数（输出预测值），loss() 是损失函数，它们都是事先定义的 Python 函数（参见“自动微分”的注意点 2）：def linreg(X, w, b): return torch.matmul(X, w) + bdef squared_loss(y_hat, y): return (y_hat - y) ** 2 / 2net = linregloss = squared_loss 要注意的是 \\(X\\) 不是一个数据点，而是一批数据拼成的矩阵，在写上述函数时要注意应当完成对一整个 batch 的数据的操作。被求导函数就是通过 l = loss(net(X, w, b), y) 和 l.sum() 定义的。注意前者得到的 l 是一个长度为 batch_size 的向量，因为 X,y 是一个 batch 的数据，它们是一起计算的（矩阵化比 for 循环要快），求和后才是这一 batch 的损失函数。接下来 .backward() 执行求导。由于只有 w,b 开启了求导模式，也只会求 \\(\\frac{\\partial l}{\\partial w}, \\frac{\\partial l}{\\partial b}\\)，导数结果存放在 w,b 的 grad 属性中。下一步是梯度下降，打包成一个函数 sgd(params, lr, batch_size)。首先一个小细节是捆起来传参数列表 params，除了代码易于维护，另外就是将其变为可变类型，直接修改 w,b 而不需返回。再说一遍，无需单独传导数，已经存放在 grad 属性中。def sgd(params, lr, batch_size): with torch.no_grad(): for param in params: param -= lr * param.grad / batch_size param.grad.zero_()主体部分是每个参数都执行梯度下降，lr 是事先定义好的学习率，还要除以 batch_size 是因为之前 l 的计算没有平均，放到这里，也起到规范化步长的作用。注意梯度下降的表达式会构造新的计算图，导致混乱，一定要以 with torch.no_grad(): 包裹。梯度下降结束后，w,b 的值随即更新。由于下一个 batch 还要求导，不要忘了给 grad 清零。这里把它巧妙地写在 sgd() 函数里面，能充分利用 for 循环遍历参数，而不需分别写 w,b。训练结束后，我们得到 w,b，就得到了训练好的模型，调用 net(x, w, b) 可对 x 进行预测。本文比较了训练的 w,b 和生成数据时真实的 w,b 的误差。简洁实现上述实现中很多步骤可以换成 PyTorch 简洁的 API 实现。首先是现成的生成器，PyTorch 里有现成的 Dataloader 类可使用（文档）。这个类的实例就是生成器，构造函数为from torch.utils import datadata_iter = data.Dataloader(dataset, batch_size, shuffle=True)此句为从 dataset 构造大小为 batch_size 的数据生成器。dataset 是 PyTorch 的 Dataset 类型（torch.utils.data.Dataset），需要按规则构造，当然也有现成的数据集（从 torchvision.datasets 里 import 即可）。构造规则是一个比较麻烦的事，将在别的笔记中再讨论。书中这里的代码暂时省略了对它的讨论。其他选项： shuffle：指定需不需要打乱数据的顺序； sampler,batch_sampler：自定义采集 batch 的方式，传入的是 torch.utils.data.Sampler 类型。不指定则采用顺序采集（shuffle=False）或随机采集（shuffle=True）。第二是现成的模型、损失函数。上述线性模型函数 linreg、平方损失 squared_loss 无需自己定义，在 PyTorch 中有现成的：from torch import nnnet = nn.Sequential(nn.Linear(2, 1))loss = nn.MSELoss()PyTorch 中的线性模型就是 nn.Linear(m, n)，m, n 分别为输入、输出神经元数。nn.Sequential() 将不同的 Layer 串联起来构造成一个大的模型，它其实是一个容器，通过下标索引 net[0] 可以选中各层。它们的类型是 PyTorch 自己的 torch.nn.modules 里的“模块”类型，各个“模块”具有树状的父子关系，例如本例是父模块 net（nn.Sequential）下嵌套子模块 net[0]（nn.Linear）。在此笔记中将介绍复杂的深度网络，将见到更多复杂的模块组合。这些现成的函数作用相当于“自动微分”注意点 2 的所说的函数，但是还是有不一样的地方。它们的一个重要特点是模型参数都存放到这里面了，它是真正意义上的模型。通过以下代码，体会一下如何查看模型参数：# 参数初始化net[0].weight.data.normal_(0, 0.01)net[0].bias.data.fill_(0)# 打印参数print(f&#39;{net[0].weight.data:f} {net[0].bias.data:f}&#39;)优化器也是事先构造好的：import torchtrainer = torch.optim.SGD(net.parameters(), lr=0.01)即实例化一个 Optimizer 优化器类，优化器可从 torch.optim 里挑选，有 SGD, Adam 等。注意这里 net.parameters()，模型参数从一开始就与优化器绑定到一起了。（注意这个事情，有助于理解下面 trainer 不需要传模型参数。）训练步骤简化为：for epoch in range(num_epochs): for X, y in data_iter: l = loss(net(X), y) trainer.zero_grad() l.backward() trainer.step() l = loss(net(features), labels) print(f&#39;epoch {epoch + 1}, loss {l:f}&#39;)到现在看，简洁性体现在以下几个方面，PyTorch 的设计者把所有有门槛的、需要深度理解的细节全都隐藏了： 不需要自己写模型、损失等函数，省去了考虑那些复杂的矩阵操作； 优化器不需要自己写，而且 trainer 什么参数也不用传（甚至模型参数），调用一下 step() 搞定；甚至 grad 清零的工作挪到了 trainer.zero_grad()，同样不需要传模型参数。还有几个小细节也足以体现： net()不需要显式地传入模型参数，直接写 net(X) 即可； l 不需要 sum() 了，直接 backward() 后面也能知道什么意思；一开始学习深度学习框架，只需看懂高级 API 表面的工作流程，会写即可，并不特别需要了解这些 API 背后的细节。二、Softmax 多分类第二个模型是 Softmax 多分类模型：\\(\\mathbf{O} = \\mathbf{X} \\mathbf{W} + \\mathbf{b}, \\mathbf{y} = Softmax(\\mathbf{O})\\)。此部分做的是图像分类问题，用的是 Fashion-MNIST 图像数据集，做 10 分类。这是会涉及使用 Dataset 类型的使用，但仅限于调用 PyTorch 自带的 Dataset 数据集实例。在安装 PyTorch 时可以看到，它包含 3 个库，torch 即深度学习框架，是工具；而 torchvision，torchaudio 是专门提供例子的库：数据集，网络，变换，分成视觉和语音两部分。因此 PyTorch 提供的图像数据集在 torchvision.datasets 里。这里面常见的 MNIST、CIFAR、ImageNet 数据集都有，见官方文档。读取数据集即从其中的类中创建实例：import torchvisionfrom torchvision import transformstrans = transforms.ToTensor()mnist_train = torchvision.datasets.FashionMNIST(root=&quot;../data&quot;, train=True, transform=trans, download=True)mnist_test = torchvision.datasets.FashionMNIST(root=&quot;../data&quot;, train=False, transform=trans, download=True)它将从 root 指示的目录（路径格式见Linux 学习笔记）中寻找 FashionMNIST 数据，如果没有：download=False 时报错，download=True 时将从网上下载数据到该目录内，同时语句返回 Dataset 类型的变量 mnist_train,mnist_test，它包含一对对 \\((X,y)\\) 元组，可以像列表一样中括号索引。要注意，需要规定 transform=transform.ToTensor()，这样里面的 X 才是 Tensor 类型，否则默认为 PIL 类型（Python Image Library，是 Python 图像处理标准库 Pillow 表示图像的类型）。从头开始实现对于数据生成器，这里直接使用简洁实现——Dataloader，没有再从头实现。要注意测试数据也需要构造 Dataloader。train_iter = data.Dataloader(mnist_train, batch_size, shuffle=True)test_iter = data.Dataloader(mnist_test, batch_size, shuffle=True)以下是从头开始实现定义的模型和损失函数，这里不打算细讲，看看就好，基本上是各种矩阵操作、广播机制的巧妙运用。可以看到，自己写这些东西是比较麻烦的，就是因为需要注意一整个 batch 的数据传入的问题，这就涉及更高阶的矩阵操作。def softmax(X): X_exp = torch.exp(X) partition = X_exp.sum(1, keepdim=True) # keepdim 是为了下面用广播机制 return X_exp / partitiondef softmax_linreg(X): return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)def cross_entropy(y_hat, y): return - torch.log(y_hat[range(len(y_hat)), y])net = softmax_linregloss = cross_entropy注意，X 是 (28,28) 图像，将其拉直这一操作放到了模型里（X.reshape(-1, 28*28)），而不是数据预处理过程中。其他小细节是没有把参数 W,b 传入函数参数，而是当作全局变量了（其实这样不太好）。W,b 也像之前一样手动构造并初始化：W = torch.normal(0, 0.01, size=(28*28, 10), requires_grad=True)b = torch.zeros(10, requires_grad=True)书中到这里第一次涉及测试过程的写法。测试过程涉及准确率，在从头实现中也是要自己写的。注意它和 loss 一样要考虑一整个 batch 传入的问题，if 语句就是在检查是否为单个数据的：def accuracy(y_hat, y): if len(y_hat.shape) &amp;gt; 1 and y_hat.shape[1] &amp;gt; 1: y_hat = y_hat,argmax(axis=1) cmp = y_hat.type(y.dtype) == y return float(cmp.type(y.dtype).sum()) / len(y)书中用了自己构造的数据结构 Accumulator 作统计工作，有点麻烦，我翻译了一下伪代码：with torch.no_grad(): for X, y in test_iter: # 计算累加 loss(net(X), y) # 计算累加 accuracy(net(X), y) # 打印统计后的 loss 和 accuracy从此模型开始，作者自己写了一个 Animator 类用于展示每个 epoch 训练情况，可实时画出训练 loss，测试准确率等。这个东西实在没必要自己写，有现成的工具 TensorBoard 很好用，参考我的 TensorBoard 学习笔记。简洁实现简洁实现仍然使用了现成的模型、损失函数、优化器。优化器的简化同上，这里就看一看模型和损失函数的定义：from torch import nnnet = nn.Sequential(nn.Flatten(), nn.Linear(784,10))loss = nn.CrossEntropyLoss()等等！Softmax 函数哪儿去了？这是一个重要的问题。实际上，Softmax 函数放到了 CrossEntropyLoss 里面了，也就是说 net(X) 输出的是未经 Softmax 规范化的预测。PyTorch 这样设计的原因涉及背后的计算机理，是为效率服务的，详见书 3.7.2 节。这里的初始化用了另一套 API：def init_weights(m): if type(m) == nn.Linear: nn.init.normal_(m.weight, std=0.01)net.apply(init_weights)对于 nn.Module 模块，它的 apply 方法可以将传入的函数递归地作用到它包含的所有模块上。本例即将 init_weights 作用在 net（nn.Sequential），net[0]（nn.Flatten），net[1]（nn.Linear）。由于 init_weights 里的 if 语句，只对 net[1] 应用 nn.init.normal_。nn.init 的用法详见笔记（三）。另外，PyTorch 里没有实用的求准确率的 API，因为实在是没必要，自己写两个小函数就解决了。测试过程同上从头开始实现。三、多层感知机（MLP）本节问题仍为图像多分类问题。MLP 模型与上面 Softmax 多分类相比，无非是网络层数由一层变为多层，层间引入了激活函数。从头开始实现模型定义和初始化大同小异。这里值得关注的新东西是：参数打包成 nn.Parameter 实例。前面见过简洁实现中模型的参数 net.parameters() 就是 nn.Parameter 类型的，它是进一步封装的类。而这里即使没有用到简洁实现的 nn.modules，也能当作一般的 Tensor 正常使用，还是很灵活的（原因：nn.Parameter 源代码定义了 __new__() 方法，它返回 Tensor 类型）。另外，下面的 @ 运算符是 PyTorch 重载的，等价于矩阵乘法 torch.matmul()。W1 = nn.Parameter(torch.randn(784, 256, requires_grad=True) * 0.01)b1 = nn.Parameter(torch.zeros(256, requires_grad=True))W2 = nn.Parameter(torch.randn(256, 10, requires_grad=True) * 0.01)b2 = nn.Parameter(torch.zeros(10, requires_grad=True))params = [W1, b1, W2, b2]def relu(x): a = torch.zeros_like(x) return torch.max(X, a)def mlp(X): X = X.reshape((-1, num_outputs)) H = relu(X@W1 + b1) return (H@W2 + b2)net = mlploss = nn.CrossEntropyLoss()简洁实现这里唯一的变化是定义模型多了两个模块：隐藏层和激活函数。不再详述。net = nn.Sequential( nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), nn.Linear(256, 10))总结一下，有了自动微分这一工具后，深度学习看似简单，但是上面所有的从头开始实现，写起来真的特别麻烦，要顾虑很多细节如矩阵化，有很多坑。深度学习框架的高级 API 不仅写法简单，写模型就跟搭积木一样，不用考虑细节，而且采取了额外的预防措施确保数值稳定性，帮助编程人员避免从头实现可能遇到的陷阱。所以以后如非学习目的，能用框架就不要自己手写！" }, { "title": "俄语学习笔记：字母表、语音与文字系统", "url": "/posts/studynotes_Russian_ABC/", "categories": "有趣的事情", "tags": "学习笔记, 语言", "date": "2022-01-16 00:00:00 +0800", "snippet": "我学习俄语的主要动机是能够利用互联网上的俄语资源。我的打算就是简单地入个门，就像之前选过通识课《法语二外》一样，把语音文字系统搞明白：基本要求是遇到单词可以流利地拼出来（可以不知道意思）。这比每次查东西只会复制粘贴、丢到谷歌翻译好，还能积累一些单词。我参考的教材是《大学俄语》。和《简明法语教程》如出一辙，前八课也是语音入门课。教材的模式是一课一课地学，每一课混杂了语音、语法、生词各种知识。我是抱有快速入门的心态的，想各个击破，一次搞定一个专题。本文专题就是搞定俄语的语音与文字系统，即给一段俄语，会读，会写，会打字。现在就开始吧。字母表字母文字的语言一开始入门都是差不多的，先找俄语字母表背一下吧（可以用字母歌洗脑）。俄语使用的西里尔字母，一共有 33 个。这个教程总结的不错，把俄语字母分成以下四类： 和英语字母完全一样（True Friends）：Аа Кк Мм Оо Тт 和英语长得一样，但读音不同（False Friends）：Вв Ее Нн Рр Сс Уу Хх 和英语长得不同，但有相同的读音对应（New Friends）：Бб Гг Дд Зз Ии Лл Пп Фф Ээ 和英语完全不同（Strangers）：Ёё Жж Йй Цц Чч Шш Щщ Ъъ Ыы Ьь Юю Яя一、会读俄语的一个好处是像法语一样，单词拼写完全决定单词发音，因此发音规则非常有逻辑，不像英语那样，单词是无法拼读出来的，每个词的读音必须现学。因此“会念所有单词”这个技能是可以直接在初学阶段 get 到的。俄语的每个字母基本上对应一个读音，所以理论上只要首先学习每个字母的发音，读单词时顺着拼出来就可以了。学习俄语使用的读音是第一步。但俄语读音也不是这么简单的，除了字母本身的音，还有一些音是通过一些变音规则把一些字母原本的音变成其他的音，所以第二步是熟悉这些变音规则。掌握这两点后，确实是可以完全拼读出单词，但是各个音节没有起伏，是像和尚念经一样平着发出来的。因此第三步是掌握单词的音调、音律，包括重音、调型。在把单词串联成句子时，有句子的重音、调型。俄语使用的读音会读俄语，首先应该搞清楚俄语使用的读音有哪些。首先是每个字母本来的读音： 元音字母，发元音 Аа Ии Уу Ээ Оо：= a i u e o（啊依呜爱哦），每个语言应该都有的基本元音 Ыы：读“恶意、无意”，俄语特有的元音 Йй：半元音，它与 Ии 的区别是英语 /y/ 和 /ɪ/ 的区别 Яя Юю Ее Ёё：复合元音，即 й+а й+у й+э й+о （= 汉语ia, iu, ie, io） 辅音字母，发辅音 清辅音字母，发清辅音：Пп (= p) Фф (= f) Тт(= t) Сс(= s) Кк(= k) Хх(= h) Цц(= 汉语ci) Чч(= 汉语chi) Шш(= 汉语sh) Щщ(= 汉语shi) 浊辅音字母，发浊辅音：Бб(= b) Вв(= v) Дд(= d) Зз(= z) Гг(= g) Лл(= l) Мм(= m) Нн(= n) Рр(弹舌音r) Жж(= 英语/ʒ/) 特殊符号：Ъъ, Ьь（不单独发音，必须与其他字母结对） л、м、н 和英语 l、m、n 一样，在音节末尾时读 “奥、hmm、嗯” 的音。对应的软辅音 ль、мь、нь 同理。俄语特有的语音现象是存在颚音化（palatalization）：辅音跟在“i 元音” 后要发生软化，变成不同于原辅音的新的音——软辅音（软化前的相应地叫硬辅音）。俄语软化的读法是舌中部向上抬起，通俗点可以理解为接了一个短的 и。俄语中，除了以下的辅音，其他辅音都是硬辅音，有对应的软辅音。在音标上以撇号标注，如 м’ 无法软化的硬辅音：ж、ц。ж、ц后面接и读类似汉语的“r+i”、“c+i”，读起来很别扭，所以没有软化； 本身就是软辅音：ч。类似汉语的 chi，本身就自带了一个 i 音（问题：能不能硬化？）； 软辅音和硬辅音分写成两个辅音字母：ш、щ。类似汉语的 sh、shi。（至于什么时候软化为软辅音，我将其视为变音规则的一部分，见下一节。）总结一下，俄语的音有： 6 个元音： а и у э о ы（复合元音 я ю е ё 是半元音 й + 元音，不列为元音）； 1 个半元音： й （半元音读音像元音и，所以称为半元音，但本质上是辅音）； 35 个辅音：辅音字母一共有 20 个，除去那 5 个特殊的，每个字母都有硬辅音和加撇的软辅音两个音，所以一共有 15*2 + 5 = 35 个辅音。变音规则先是与腭化有关的规则： 颚音化（硬音软化）：俄语读 “i” 的音有 и、й 及其衍生的 я、ю、е、ё，我称为 и 系列音。根据腭化规则，除ж、ц、ч、ш、щ 五个音之外的辅音在接 и 系列音时自然发生软化。需要注意以下这几个软辅音的读法：т’、д’ 读汉语 j 的音； 五个特殊硬辅音的规则： 只能作硬辅音的（ж、ш、ц）无法软化，接 и 系列音时会很别扭，所以： 不能接 я、ю； 接 и、ё、е 时要变音，变为易读的元音：и 变 ы，ё 变 o，e 变 э（非重读变 ы）； 只能作软辅音的（ч、щ）读作 chi、shi 本身带着一个 i，它们接 а、у、э、о 与 я、ю、е、ё 读音一样；既然一样，二者中有些写法就没有了，有些保留： 没有 щя 只有 ща，没有 щю 只有 щу，没有 щэ 只有 ще 都允许的：ча-чя、чу-чю、чо-чё、чэ-че、що-щё 隔音：软音符号ь、硬音符号ъ都是起隔开音节的作用，它们不发音。符号出现在辅音后面，把音节从此处隔开，不能与后面的音连读作一个音节： 只能作硬辅音的（ж、ш）：只能用硬音符号隔音； 只能作软辅音的（ц、ч、щ）：只能用软音符号隔音； 其他辅音：可以用硬音符号隔音，也可以用软音符号隔音，后者将该辅音变为软辅音。 其他的变音规则： 元音弱化：o 非重读时读轻 a， я е 非重读时读轻 и 。离重读音节越远，读得越轻（重读见下一节）； 浊辅音和清辅音转换，仅限发音相似的这几对：Пп - Бб、Фф - Вв、Тт - Дд、Сс - Зз、Кк - Гг、Шш - Жж 清辅音浊化：在浊辅音前； 浊辅音清化：两种情况，1. 在词末；2. 或在清辅音前。 清浊辅音的转换可以这样理解：只发生在两个辅音相连的情况，前者应保持与后者的清浊情况一致（变动前者而不是后者）。词末的辅音可以视为后面有一个清辅音。 其他变音：сч 读 щ。重音与调型和英语一样，每个俄语单词由一个或多个音节组成，其中的某个音节是重音音节，以重音符号（´）标在该音节的元音字母上。 俄语的重音符号只供初学者或小朋友识字用，和拼音道理相同，在正式文字中一般不标。大写字母上面省略重音符号。标注重音后，重音处的元音要稍微拉长。俄语没有长短元音之分，语音的节奏是通过重音拉长实现的。接下来，单词有有多种抑扬顿挫的读法，称为调型。一般来说，平时读单词使用陈述调型（书上称调型1），即重音前调稍高，重音处开始下降（注意，已经低于重音前），重音后调比较低，但也不是特别低，有悬浮的感觉（建议多听听书中生词的录音找感觉）；基本不用英语的调型（书中称调型2），即重音处最高，两边低。单词连成句子后，有句子的重音、调型。| 调型 | 重音前 | 重音 | 重音后 | 使用场景 | | :-: | :-: |:-: |:-: | ::- | | 陈述调型 | 中调 | 低于重眼前的降调 | 低于重音前 | 句子用于陈述句 | | | 特殊疑问调型 | 中调的上限 | 降调或平调（加强词重音） | 低于重音前 | 句子用于特殊疑问句（带疑问词的疑问句）| |一般疑问调型 | 平调 | 陡然上升 | 低于重音前 | 句子用于一般疑问句（不带疑问词的疑问句） || 对比调型 | | | | | | 感叹调型1 | | | | | | 感叹调型2 | | | | | 这一节的东西靠讲是学不会的，最好还是多听录音找感觉，在听的时候注意对应这里讲的知识点，从而理解这些文字的意思。二、会写印刷体已经在上面了，记住然后用手写一写就可以了。有一点要注意，除了 Ее Ёё，其他字母的大小写就是简单的大小不一样。不要搞成英语小写了！其实会这些对我来说基本足够了，反正接触的大部分是印刷体。手写体可学可不学：照着练练字就完事了。不用刻意记在哪里拐弯等类似的细节，而应注意一些大的变化，以免见到手写体不认识： Тт Пп 的小写，一个像 m，一个像 n Дд 的小写，像 g 而不是 d Ии Йй Цц Шш Щщ 长的都像 u，区别在于拐了几次和有没有小尾巴 Мм Тт Шш 小写容易混淆，以上下标记的杠区分 注意 Бб, Вв, Гг, Рр 小写的细微变化另外要注意笔画，俄语字母书写一般从左下角开始笔画，而不像英语从左上角。三、会打字俄语键盘布局（ЙЦУКЕН 键盘）长这样子。俄语比英语多 7 个字母，只能把右边 7 个标点的位置挤占了。这个键盘布局也是按照字母使用频率排布的，所以即使和英语相同的音也不对应。因此使用这种键盘相当麻烦，只能从头开始练习，没有什么窍门。可以买个俄语键盘贴辅助练习。说到和英语的对应，就是俄语罗马化，即把西里尔字母转写为拉丁字母。有很多转写方案，参见维基百科，这里我整理了一套看起来比较顺眼的：Аа - a, Бб - b, Вв - v, Гг - g, Дд - d, Ее - e, Ёё - ëЖж - zh, Зз - z, Ии - i, Йй - j, Кк - k, Лл - l, Мм - mНн - n, Оо - o, Пп - p, Рр - r, Сс - s, Тт - t, Уу - uФф - f, Хх - kh, Цц - ts, Чч - ch, Шш - sh, Щщ - shch, Ъъ - “Ыы - y, Ьь - ‘, Ээ - è, Юю - ju, Яя - ja" }, { "title": "流行音乐和声理论与配和弦方法总结", "url": "/posts/pop_music_harmony/", "categories": "音乐", "tags": "乐理", "date": "2021-08-20 00:00:00 +0800", "snippet": "本文总结流行音乐（或普通、常见的音乐)中的和声知识。整理此文的目的是能够系统地掌握流行音乐中的和声技巧，能够从理论级别听懂歌曲中的和声。说得更直接一点的话，就是为了系统掌握配和弦（harmonization）的方法，配和弦是为了用和声乐器（吉他、键盘、手风琴等）给歌曲伴奏。我自小便试图为各种歌曲配和弦，并在电子琴上以自动和弦的方式弹出来，积累了一些经验，这个事情在我这篇文章中有提过。而我发现时至今日我仍然只有经验，而没有学到一些系统的东西。我希望通过这篇文章的整理，以后配和弦的话能够说出些理由来，而不再只是凭感觉。到目前为止，像我这种走野路子的人，对流行音乐一般使用和弦的基本概念可以概括为两点： C 调歌曲，使用 C、F、G或G7、Am、Dm、Em； 其他调的歌曲，将 C 调的和弦作平移即可推导出。由于第二点的存在，本文统一只讨论 C 调。那么： 为什么 C 调歌曲常使用这些和弦，理论是什么？ 把这些和弦配到旋律中的方法与原则是什么，有没有什么套路？ 除了这几个和弦之外，有没有可能使用其他的和弦，怎么用？以下将讨论这三个问题，每一章对应一个问题。本文参考了很多官大为老师的 NiceChord 频道、David Bennett 的视频，结合自己的理解，如有误请指正。目录 流行音乐的和弦理论 自然大小调的和弦 其他音阶的和弦 编配基本的和弦进行 常见的和弦进行 三和弦进行 四和弦进行 卡农进行 五度圈进行 十二小节 Blues 其他和弦进行 和弦进行改造技巧 扩展和弦类型 七和弦 挂留和弦、附加和弦、六和弦 引申和弦 引入调外和弦 转调 副属和弦 三全音代理 高低半音的和弦 那不勒斯和弦 流行音乐的和弦理论在一般的乐理观点中，和弦建立在音阶之上的，是选取音阶中的各级音构造出来的，这一过程称为音阶的和声化（scale harmonization）。使用什么样的音阶，一般要都对应使用和声化得到的什么样的和弦。自然大小调的和弦流行音乐使用的音阶是自然大小调音阶，它是七音音阶。以下在 C 大调 或 a 小调（它们使用了同一组音，是关系大小调，可以看成同一音阶），按照隔位选取的方式得到各级三和弦： 和弦名称 C 大调和弦级数 a 小调和弦级数 C I（主和弦） III Dm II IV（下属和弦） Em III V（属和弦） F IV（下属和弦） VI G V（属和弦） VII Am VI I（主和弦） Bdim VII II 音阶中以下三级和弦是重要的，称它们承担着三种功能（功能和声）： 和弦级数 术语 作用 C 大调和弦 a 小调和弦 I 主和弦 最稳定，相当于家 C Am V 属和弦 不稳定，想解决到主和弦，相当于回家的路 G Em IV 下属和弦 起到桥接作用，相当于路上的桥 F Dm 一首曲子简单的和声就是在这些和弦中的不断切换，对应切换成大小调。大小调不明显的，可以混在一起看： 家：C, Am 回家的路：G, Em, Bdim（不常用，常以 G 替代） 桥：F, Dm（和 F 有时可以互换）它们连接的通常的逻辑为： 家应该与桥或回家的路连接； 回家的路应该与家连接（解决）； 桥可以连接回家的路，也可以直接回家。其他音阶的和弦以上讨论的是自然大小调。除此之外，还有很多其他的音阶（见我整理的音阶分类），理论上都可以找到相应的各级三和弦，构成和声系统，下面就对此讨论。在音阶分类中我是按照以 7 为周期的调式，把各种七音音阶划分为 7 个一组的类。这里我不采用这个顺序，而是把音阶都看成自然大调或自然小调的修改（哪个方便用哪个），从而和弦也有相应的修改。应用非自然大小调音阶时，只需要把握它们与自然大小调不同的音和有重要效果的和弦修改即可，并不需要当作并列的音阶与和声系统练习。 音阶可以按照色彩来排序，基本的音阶：大调明亮，小调忧郁。小调音阶是大调音阶修改 3、6、7 三个音的半音。其他音阶基本也可以按照根据修改的音参与排序： Altered &amp;lt; Locrian &amp;lt; Phrygian &amp;lt; 小调 &amp;lt; Dorian &amp;lt; Mixolydian &amp;lt; 大调 &amp;lt; Lydian &amp;lt; 增 Lydian 参考 David Bennett 的视频：The Modes Ranked by Brightness。音阶中相邻音的排列方式以 W-W-H 开头，主和弦可以构成大三和弦，常见有以下三个音阶： （大调）和弦级数 C 大调 C Lydian (#4) = A Dorian C Mixolydian (b7) = A Phrygian C Lydian Dominant(#4, b7) I（主和弦） C C C C II Dm D Dm D III Em Em Edim Edim IV（下属和弦） F (#Fdim) F #Fdim V（属和弦） G G Gm Gm VI Am Am Am Am VII Bdim Bm (bB) (bBaug) Lydian 最大的特点是 4 音升高半音导致 II 级和弦 Dm 变为了 D。Mixolydian 是 7 音（导音）降低半音导致 VII 级和弦 Bdim 变为了 Bb。因此，Lydian 的和弦进行倾向于 D 到 C 的解决（向下二度），Mixolydian 倾向于 Bb 到 C 的解决（向上二度）。（二度的解决很容易看成其他调的大调，为了突出 Lydian 或 Mixolydian 的特点，可以在和弦中加入主音 C 以示强调）Mixolydian 的 V 级和弦变成了小三和弦 Gm，解决倾向不如大三和弦了（因此这个音阶更忧郁，也没有方向性、归属感，常用于摇滚的和弦进行）。Lydian 的 VII 级和弦为 Bm，可以更方便地充当属和弦。排列方式以 W-H 开头，主和弦可以构成小三和弦，通常为 “xx 小调”；常见有以下三个音阶：a 小调升高 6、7 音： 大调和弦级数 a 小调 （= C 大调） a 和声小调（#5）（= C 自然增大调） A Dorian（#4）（= C Lydian） a 旋律小调（#4，#5）（= C 增 Lydian） VI（主和弦） Am Am Am Am VII Bdim Bdim Bm Bm I C Caug C Caug II（下属和弦） Dm Dm D D III（属和弦） Em E Em E IV F F (#Fdim) （#Fdim） V G (#G) G （#G） 和声小调最大的特点是 7 音升高半音导致属和弦 Em 变为了 E：大和弦比小和弦更有解决到主和弦的倾向，因此可以应用到大多数的小调歌曲中；Dorian 最大的特点是 6 音升高半音导致下属和弦 Dm 变为了 D，应用不如前者广，通常 Dorian 的和弦进行只是 Am 和 D 来回循环，单纯为了突出 Dorian 的特点。旋律小调结合了二者的特点，也不常用。排列方式以 H 开头，可以得到很多音阶。首先是 Phrygian 系列的音阶，它们可以看作 a 小调降低 2 音，但是放在 E 调看比较合适： 和弦级数 E Phrygian（=C Mixolydian） E 大Phrygian（#5） I（主和弦） Em E II（下属和弦） F F III G #Gdim IV Am Am V Bdim Bdim VI C Caug VII（属和弦） Dm Dm 在 Phrygian 调式中，与上述大调、小调在和声功能上发生了两个变化： V 级和弦不再是重要的和弦（因为是不协和的减和弦），它的属和弦功能被 VII 级和弦代替。因此在 Phrygian 中终止一般是 VII-&amp;gt;I 而不是 V-&amp;gt;I。但它是小三和弦，解决倾向不够强； IV 级和弦的下属和弦功能弱化了（因为变成了小三和弦），而是由大三的 II 级和弦替代。因此，Phrygian 的和弦进行非常局限，基本上只涉及 Em（或 E）、F、Dm 三个和弦，例如 Em(或E)-F，E-F-Dm。排列方式以 H 开头的第二类是 Locrian 系列的音阶，它在 Phrygian 基础上降低 5 音。因为主和弦是减和弦，非常不和谐，其他各级和弦都是相对和谐的，与减和弦非常不搭，所以实际上是没有合适的和声系统，也就没必要讨论。还有一些异域风情的音阶列举如下，由于它们音的修改方式很怪异，由它们构造的三和弦也很别扭（甚至不能构造三和弦），因此一般都按照上面常用的框架来和声，把怪异的修改音看做框架外的音即可。 大调和弦级数 a 小调 （= C 大调） A Romani 小调（#2） A 乌克兰小调（#2，#4） VI（主和弦） Am Am Am VII Bdim 没有 B I C C C II（下属和弦） Dm 没有 #Ddim III（属和弦） Em Em Em IV F F #Fdim V G Gaug Gaug 和弦级数 E Phrygian（=C Mixolydian） E 那不勒斯小调（#2） E 双和声小调（#2,#5） E 那不勒斯大调（#1,#2） I（主和弦） Em E E Em II（下属和弦） F F F Faug III G Gaug #Gm Gaug IV Am Am Am A V Bdim 没有 没有 没有 VI C C Caug #Cdim VII（属和弦） Dm 没有 没有 没有 最后单独提一下五音音阶和 Blues 音阶。最简单的和声方法就是将其看做七音音阶的子音阶，例如大小调五音音阶可以按照大小调音阶编配和弦，同理大小调 Blues 音阶也可以这样。值得注意的是，Blues 音阶还可以搭配固定的和弦进行——十二小节 Blues，至今难以从乐理上解释，见下一章“十二小节 Blues”部分。编配基本的和弦进行现在的任务是为一段流行音乐旋律配和弦（得到和弦进行），使用的和弦暂只讨论三和弦。根据官大为老师的观点，只需要把握三个原则： 旋律音应该是和弦内音； 应观察前后的和弦，使和弦之间有转换（即做到不单调，不重复）； 注意和弦转换的频率，不能太快，不能太慢。虽然第一条原则有正确的答案，但是有多个答案（旋律音是和弦内音的和弦往往有很多选择），第二、三条就是进一步选出答案的标准，是没有正确答案的。总之，配和弦不分对错，而分个人品味的高低，后者体现在第二、三条的决策上。既然没有正确答案，那么配和弦这件事就是需要积累具体的案例的，我不容易在这里总结出清晰具体的规则。因此我会另开一个笔记总结我听过的歌曲，分析和弦是如何与旋律配合的。常见的和弦进行上面说的看起来都是空话，但这不代表配和弦没有任何套路可言。很多歌曲的和弦进行有套路。这里所说的套路不是指配和弦的具体规则，而是歌曲主要部分的和弦进行重复片段在很多其他歌曲中能见到。了解这些常见的和弦进行有助于总结积累的案例，配和弦时快速识别出套路，提高效率。当然也不能有这样的观点：认为所有歌曲都在使用某种固定的和弦进行套路，只要把所有套路总结出来死记硬背，配和弦只需对应套路即可。首先，套路一定是特别多的，很难完备地总结出来；其次，一首完整的歌曲还有非主要的部分例如 bridge 等很难匹配上明显的套路，需要具体分析。以下总结这些常见的和弦进行。这些套路之所以常见，因为遵循了第一章的和声功能，一般遵循 “家 -&amp;gt; 桥 -&amp;gt; 回家的路 -&amp;gt; 家”、 “家 -&amp;gt; 桥 -&amp;gt; 家” 的顺序；三和弦进行三和弦进行是最简单的，只使用最重要的三个和弦（主和弦、属和弦、下属和弦），就能实现一套完整的功能和声。 1451：C-F-G-C，常用于经典的乡村音乐； 1451（小调）：Am-Dm-Em-Am，纯小调歌曲基本的和弦进行； 1245（大调爬升进行）：C-Dm-F-G，1451 中的 4 拆成 2-&amp;gt;4 的爬升； 1545（Major Vamp）：C-G-F-G； 6545（Aeolian Vamp）：Am-G-F-G； 1415：C-G-F-G； 4155：F-C-G，有五度圈的感觉； 1454：C-F-G-F； 4566：F-G-Am； 1524：C-G-Dm-F：包含 2-&amp;gt;4 的爬升。四和弦进行四和弦进行是指由 I、IV、V、VI 级和弦组成的和弦进行，可以有很多顺序： 1645（50 年代进行、Doo-wop 进行）：C-Am-F-G，两个家（1、6）连在一起。常用于西方五六十年代的老歌； 4516：F-G-C-Am，1645 的轮换； 6451：Am-F-G-C； 1564（流行朋克进行）：C-G-Am-F，两个家（1、6）没有连在一起，中间间隔有紧张感的 4、5 ，紧张感的收放比较均匀。在西方90、00年代的流行乐中大量使用； 6415（敏感女性进行）：Am-F-C-G，是 1564 的轮换，Am 放在第一位，适合小调歌曲； 4156：F-C-G-Am，是 1564 的轮换； 5641：G-Am-F-C，是 1564 的轮换； 15b74：C-G-Bb-F，可以看成两组 I-V （纯五度）乐句，先是 C 调，再转到 C Mixolydian（或 Bb 调）； 14b74：C-F-Bb-F； 1465：C-F-Am-G，可以看作 1564 中两个紧张的和弦 4、5 互换了位置。这个和弦进行不常用； 6534：Am-G-Em-F； 其中 4 级和弦 F 都可以替换成 Dm，得到另外一套四和弦进行，不再赘述。卡农进行卡农进行是 15634145：C-G-Am-Em-F-C-F-G，得名于巴洛克时期作曲家帕赫贝尔的 《D 大调卡农》使用了此和弦进行。卡农进行可以看做几组 I-V （纯五度）乐句：C-G、Am-Em、F-C，最后接 G 转回以便能继续循环下去。和卡农进行很接近的和弦进行是顺阶低音（leading bass）进行：C-G/B-Am-G-F-Em-Dm-G，其原理是低音使用 1765432 的顺阶低音组成和弦，最后接 G 转回以便能继续循环下去。卡农进行被大量用于华语流行歌中，据统计大概有 30% 的华语流行歌使用此进行。以下是上述进行的变种： 卡农进行 Em 变为 E； 最后一个 4 变为 2（可以是 Dm 或 D，D 见副属和弦一节）； 顺阶低音截取一部分，最常见的是 C-G/B-Am-G-F； 6543（安达卢西亚终止式）：Am-G-F-E，也是顺阶低音的一种形式，应当看做和声小调的 1765，E 是属和弦解决回到 Am 完成循环。也可以看成大 Phrygian 的 4321。 6643：Am-F-E 五度圈进行从主和弦 C 开始，沿着五度圈向前推演，得到 F-Bdim-Em-Am-Dm-G-C，其中不常用的 Bdim 换成常用的 G，得到常见的大调五度圈进行，即 4536251：F-G-Em-Am-Dm-G-C。从 Am 开始，沿着五度圈向前推演，得到 Dm-G-C-F-Bdim-Em-Am，其中 Em 以 E 替代，得到常见的小调五度圈进行：Dm-G-C-F-Bdim-E-Am。 五度圈进行的原理是五度圈中相邻的音是和谐的（纯五度）。然而，F 和 Bdim 并不是五度关系，而是三全音（介于四度和五度之间），在五度圈中不是相邻而是对面。这里用三全音的目的是避免引入根音带升降号的和弦（不在音阶内），也缩短了五度圈循环的范围（转一整圈要 12 个和弦，这样就只有 8 个）。大调五度圈进行被大量用于 00 年代的华语流行歌（尤其以林俊杰为代表），据统计大概也有 30% 的华语流行歌使用此进行。小调五度圈进行在流行音乐中也有很多应用。以下是五度圈进行的变种： 4536（王道进行）：F-G-Em-Am，截取五度圈进行的前半截，有娓娓道来地讲故事的感觉，大量用于日本的流行音乐； 2536：Dm-G-Em-Am，F 替换为 Dm 6251（Circle progression）：Am-Dm-G-C 6251（大调）（Ragtime progression）：A-D-G-C，前一个都可以看作后一个的副属和弦。 1625：C-Am-Dm-G，实际上是 1645 的变种。 Dm-G-C-F-Dm-E-Am：Bdim 换成了比较和谐的 Dm： 1b741（Mixolydian Vamp）：C-Bb-F-C，引入了 Bb 音，是 Mixolydian 音阶。还有一种正方向的五度圈（即向后推演）： 6152（Plagal Cascade）：Am-C-G-D； 1526：C-G-D-A。 爵士乐中经典的 251 进行也与此有关，放在后面专门讲爵士。十二小节 Blues十二小节 Blues 是蓝调音乐和摇滚乐基本的和弦进行：I-I-I-I, IV-IV-I-I, V-V-I-I，涉及三个属七和弦，在 C 调是 C7-C7-C7-C7, F7-F7-C7-C7, G7-G7-C7-C7。十二小节 Blues 与上述和弦进行逻辑有些不同，因为蓝调音乐中基本只用这一条和弦进行，旋律都是在此进行基础上的即兴（而不是事先谱写好的）。鉴于此特殊性，我将额外介绍由此和弦进行得到即兴旋律的逻辑。我主要参考了终极键盘手的这一条视频，系统解释了蓝调即兴的底层逻辑（其他人的教学视频一般只提一提概念，然后从实用的角度教几条乐句），非常推荐。首先，C 调十二小节 Blues 搭配的是 C 小调 Blues 音阶：1,b3,4,b5,5,b7，所有三个和弦都可以完美搭配这个音阶，而且音阶里的任何一个音都可以与三个和弦的任何一个搭配（这件事很神奇，乐理上也解释不清楚）。因此，最简单的即兴旋律只需要一直使用 C 小调 Blues 音阶即可；无论后面添加了什么音，C 小调 Blues 音阶都应看做主要部分和不变的基础。接下来将引入新的音。第一种方式：由于该进行全是属七和弦，所以可以使用相应调的 Mixolydian 音阶。这会在基础的 C 小调 Blues 音阶中引入额外的音（有时称为 Mixolydian 混合蓝调音阶）： C7 用 C Mixolydian 1,2,3,4,5,6,b7：引入了 2、6，此外还引入了 3，有把小调变大调的效果。这里 3 和 b3 可以一起使用，形成对比，这是 Blues 音乐的特殊之处； F7 用 F Mixolydian 4,5,6,b7,1,2,b3：引入了 2、6。但没有引入 3； G7 用 G Mixolydian 5,6,7,1,2,3,4：引入了 2、6、3，此外还引入了 7。总之 2、6 音都会引入，可以纳入演奏的范围；C7、G7 可以在 b3 的基础上演奏 3（而 F7 不行）；G7 还可以演奏 7。第二种方式：使用相应调的大调 Blues 音阶： C7 用 C 大调 Blues 音阶 1,2,b3,3,5,6：引入了 2、6、3； F7 用 F 大调 Blues 音阶 4,5,b6,6,1,2：引入了 2、6，还引入了 b6； G7 用 G 大调 Blues 音阶 5,6,b7,7,2,3：引入了 2、6、3、7；总之，这种方式引入的音与第一种方式类似，多了一个 F7 和弦时的 b6 音，但它并不常用。十二小节 Blues 有如下常见的变化形式： 第十小节改为 IV：I-I-I-I, IV-IV-I-I, V-IV-I-I； 转回（turn around）：最后一小节改为 V，I-I-I-I, IV-IV-I-I, V-IV-I-V； 快速四（quick to four / quick change）：第二小节就出现短暂的 IV，避免四个 I 重复无聊，I-IV-I-I, IV-IV-I-I, V-V-I-I。还有一些非十二小节的，改自十二小节 Blues 的： 八小节 blues：I-V-IV-IV, I-(V-IV)-I-V； 十六小节 blues：情况较多，不再列举。其他和弦进行一些和弦进行流行于文艺复兴、巴洛克时期，但在现代听起来比较复古，不是很常用： Passamezzo moderno：C-F-C-G, C-F-C–G-C； Passamezzo antico：Am-G-Am-E, C-G-Am-E-Am，相当于 Passamezzo moderno 的旋律小调形式； Romanesca（罗曼尼斯卡）：上述第一个和弦改为 C。最著名的例子是绿袖子； [Folia（福利亚舞曲）]：Am-E-Am-G, C-G-Am-E-Am 更复杂的和弦进行常见于爵士乐，见维基百科，在普通的流行乐中不常用，这里就不再列举了。和弦进行改造技巧本章将讨论在上述框架下加入更多的和弦改造技巧。扩展和弦类型以上只涉及到三和弦，还有更多的七和弦，九、十一、十三和弦，六和弦、挂留和弦等，本节将系统讨论这些扩展的和弦。这些和在七和弦首先是七和弦，自然大小调 Scale Harmonization 得到的七和弦如下： 和弦名称 C 大调和弦级数 a 小调和弦级数 Cmaj7 I（主和弦） III Dm7 II IV（下属和弦） Em7 III V（属和弦） Fmaj7 IV（下属和弦） VI G7 V（属和弦） VII Am7 VI I（主和弦） Bø VII II 涉及 4 种七和弦——大七和弦（maj7）、小七和弦（m7）、属七和弦（7）、半减七和弦（ø、m7-b5）。在上述和弦中： 属七和弦：G7 用于解决到主和弦 C，构成 V7-&amp;gt;I 完全终止式； 大七和弦：Cmaj7、Fmaj7； 小七和弦：Dm7、Em7、Am7 比较常用； 半减七和弦：Bø，作为小调的二级和弦，类似大调的 Dm7。上述没有涉及的七和弦种类有： 小大七和弦（mmaj7）； 减七和弦（dim7）； 减大七和弦（maj7b5）； 增大七和弦（maj7#5）。挂留和弦、附加和弦、六和弦在三和弦的基础上修改或增加其他的音（非七音）可以得到这一类和弦：第一种是挂留和弦：修改三和弦的 3 音为 4 音或 2 音，记作 sus4 或 sus2。它们有悬挂感（suspension），有向上或向下解决到三和弦的倾向，而且 sus4 比 sus2 倾向更大（因此 sus4 更常用）。也因此，sus2 可以独立出现 ，而 sus4 一般后面要跟对应的三和弦，一般使用的方法有两种： 在不需要使用属和弦（会导致和弦转换太频繁）而又需要这种解决感时，可以用主和弦的挂留和弦，即 Csus4-C； 可以用属和弦的挂留和弦，拉长属和弦解决到主和弦的进程，即 Gsus4-G-C。第二种是增加其他的音，除 7 音外只有 2 音（或高八度的 9 音）或 6 音合适，分别为附加和弦（add2、add9）和六和弦（6）。附加和弦有更温和、流行、故事性的效果；六和弦可以看做混合了大小调的主和弦，有中国风的感觉，可以根据需要改造。 挂留和弦可以加入七音，如 7sus4，这种和弦一般充当属七和弦的功能，例如 G7sus4-&amp;gt;C。附加和弦和六和弦一般不能加入 7 音。引申和弦引申和弦包括九、十一、十三和弦，都是在七和弦的上面加入引申音 2、4、6（称为 9、11、13 音）构成的和弦。要注意两个要素：七和弦、上面，缺一不可。这些和弦来自爵士理论，使用后可以产生爵士的优雅味道。但这些和弦在一些乐器中较难实现（例如手风琴），以下只作分类，并简单说明效果： 放在上面的道理是防止与和弦的 1、3、5、7 音距离太近导致有糊在一起的感觉。在演奏这些和弦时通常会省略五音，也是体现了这一目的。省略音的引申和弦通常可以看做更换根音的三和弦、七和弦等（即斜杠和弦），这种记法更方便演奏。 大七和弦： 大九和弦：大七和弦加 9 音，记作 maj9； 大十一和弦：大九和弦加 11 音，记作 maj11，和大七和弦作用类似； 大十三和弦：大十一和弦加 13 音，记作 maj13，和大七和弦作用类似； 小七和弦： 小九和弦：小七和弦加 9 音，记作 m9； 小十一和弦：小九和弦加 11 音，记作 m11，和小七和弦作用类似； 小十三和弦：小十一和弦加 13 音，记作 m13，和小七和弦作用类似； 小七降九和弦：小七和弦加 b9 音，记作 m7b9； 属七和弦 属九和弦：属七和弦加 9 音，记作 9，和属七和弦作用类似； 属十一和弦：属九和弦加 11 音，记作 11，和属七和弦作用类似； 属十三和弦：属十一和弦加 13 音，记作 13，和属七和弦作用类似； 属七降九和弦：属七和弦加 b9 音，记作 7b9，和属七和弦作用类似，比属九和弦更紧张； 属七增九和弦：属七和弦加 #9 音，记作 7#9，有摇摆、缥缈的感觉，又称 Hendrix 和弦，因为常用于以 Jimmy Hendrix 为代表的摇滚乐、Funk、R &amp;amp; B 等流行音乐中； 半减七和弦 减九和弦：半减七和弦加 b9 音，记作 dim9； 六和弦 六九和弦：六和弦加 9 音，记作 6/9； 自然大小调 Scale Harmonization 得到的九和弦如下，仅作参考： 和弦名称 C 大调和弦级数 a 小调和弦级数 Cmaj9 I（主和弦） III Dm9 II IV（下属和弦） Em7b9 III V（属和弦） Fmaj9 IV（下属和弦） VI G9 V（属和弦） VII Am9 VI I（主和弦） Bdim9 VII II 引入调外和弦调外和弦是指无法通过 Scale Harmonization 得到的，即不在上述表格中的和弦。在很多情况下，有些和弦在自然大小调看上去是调外和弦，是因为歌曲并没有使用自然大小调音阶，这些在上面讨论其他音阶时已经讨论过，例如： 出现 D 和弦：可能是 Dorian 或 Lydian； 出现 Bb 和弦：可能是 Mixolydian； …这里不再讨论这种情况。转调得到调外和弦的最直接的方式是转调，新的调中会自然引入原调的调外和弦。理论上歌曲可以转到 11 个其他调的任何调，但有一些常用的转调方式： 升一个调或半个调，在华语流行歌中常见，一般会导致整套和弦体系全都是调外和弦； 转到五度的调：根据五度圈原理，五度是关系最大的调，这样转调也会更自然； 转到同一个调相应的大调或小调，例如 A 小调（即 C 大调）转 A 大调，这种方式称为调式互换（modal interchange）或借用和弦（borrowed chords），大调和小调的互换形成色彩的对比，这种方法也大量应用于流行歌中。一些调外和弦可以解释为调式互换： 出现 Fm 和弦：C 大调转 C 小调； 出现 A 和弦：A 小调转 A 大调。 不仅自然大小调可以进行调式互换，其他的。例如 Norwegian Wood 使用了 Mixolydian 与 Dorian 的转换，二者也是关系大小调的关系，可以实现色彩变化。副属和弦在（非主和弦的）和弦前面加 5 级大三和弦（或属七和弦），构成局部的 V7-&amp;gt;I 的完全终止式，称为副属和弦（secondary dominant）。常见的例子： C-G 加入副属和弦 D：C-D7-G； C-Dm 加入副属和弦 A7：C-A7-Dm； C-F 加入副属和弦 C7：C-C7-F（常见于乡村音乐）。 注意，G7-&amp;gt;C 和 E7-&amp;gt;Am 的解决是到主和弦的解决，严格来说不算是副属和弦。如果在副属和弦前面再加入 5 级和弦，就会构成一段小的爵士乐 251 进行，这样还可能引入更多调外和弦。不再赘述。注意：大三和弦的 II 级和弦用小七，小三和弦的 II 级和弦用半减七，即 ii7-V7-I、iidim7-V7-i。三全音代理在属七和弦中，3 和 b7音是三全音，是属七和弦的不协和音响效果的主要来源，因此可以该和弦替换成拥有相同三全音的属七和弦，称为三全音代理（tritone substitution）和弦。例如：G7 和弦拥有三全音 7 与 4，分别将其看做 3 音和 b7 音，如果将 4 与 7（反过来）看做 3 音和 b7 音的话，就对应 Db 和弦。三全音代理和弦与副属和弦一样用于构成局部的 V7-&amp;gt;I 的完全终止式，将其中 V7 换为三全音代理和弦即 bII7，得到的 bII7-&amp;gt;I 这种升高半音的解决也是一种解决。高低半音的和弦还可以在和弦前接一小段高半音或低半音的和弦，形成半音的解决效果。这种方法不能滥用，偶尔用一次产生惊喜的效果足矣。那不勒斯和弦那不勒斯和弦（Neapolitan chord）是指 bII 级大三和弦，C 调是 Db。它常用第一转位 Db/F，即 4-b6-b2，因此又叫那不勒斯六和弦。那不勒斯和弦的和声功能是“桥”，可以与下属和弦 F、Dm 相互替代。例如在 1451 中，F 换为 Db/F。" }, { "title": "音阶的分类方法", "url": "/posts/scales/", "categories": "音乐", "tags": "乐理", "date": "2021-08-10 00:00:00 +0800", "snippet": "本文介绍音阶，主要是推导如何系统地分类各种音阶。构成一个音阶有几个要素： 主音（tonic）：即音阶的起点音，决定了音阶的调（key）； 相邻音的排列方式：音阶是从主音开始，按照一定的间隔排列方式，向上推演得到的。主音决定绝对音高，排列方式决定色彩、和声功能（相对音高）。以下不讨论主音（可以都假设主音为 C），而讨论有哪些排列方式。音阶一般要保证周期性，周期通常是一个八度，以下均作此假设（违反此假设的只有极其现代的自由爵士等类型的音乐）。按照十二平均律，一个八度之间有 12 个最小间隔（半音），所以音阶相当于把 12 拆成一些小间隔的和，根据拆解的方式不同得到各种音阶。根据排列组合知识，可以看成插隔板问题——在 12 棵树之间插入 \\(n-1\\) 块隔板有多少组合方式，\\(n\\) 音音阶就有多少个。计算公式为： 五音音阶有 \\(C_{11}^4=330\\) 个； 六音音阶有 \\(C_{11}^5=462\\) 个； 七音音阶有 \\(C_{11}^6=462\\) 个； 八音音阶有 \\(C_{11}^7=330\\) 个； 九音音阶有 \\(C_{11}^8=165\\) 个； 十二音音阶有 \\(C_{11}^{11}=1\\) 个。在众多的音阶中，有的比较常见，有名有姓，也有背后适用的音乐风格与故事；有的则不常见。有名有姓的音阶请参考：https://en.wikipedia.org/wiki/List_of_musical_scales_and_modes。以下我讨论的是如何系统地来分类这些音阶，每个类中往往既有常见的音阶，也有不常见的；有的类常见的音阶多，有的类少。五音以上的音阶排列方式的限制条件受此视频启发，我们对排列方式加如下几条限制： 间隔只包含 1（半音，H），2（全音，W），3（全音加半音，WH），3 以上跨度太大不考虑； 一些均匀性限制（为了使音阶好听）： 两个 H 不相邻； W, WH 不相邻； WH, WH 不相邻。 注意第一个音和最后一个音也算相邻，是循环意义上的相邻。可以证明，这样得到的音阶都是七音或六音的。接下来，可以将符合条件的七音或六音音阶按照一套方法（见下一节：调式）划分为几个族。以下每节介绍按照族来介绍：七音音阶共 4 个族，还有 2 族六音音阶和 1 族八音音阶。本章最后面证明列举了符合上述条件的所有情况。自然大调族自然大调的排列方式是 W-W-H-W-W-W-H，由此可以轮换衍生出 7 个不同的音阶，称为调式（mode），指通过轮换得到的一族音阶。调式的英文也很直观，就是数论中的模。这个排列方式循环周期是 7，自然大调音阶作为代表元。以下每一族音阶不仅列出惯用名称与相邻音的排列方式，也列出 C 调音阶、 A 调音阶（前者全列，后者只对偏小调的列）与黑键少的常用的调的音阶。 调式 惯用名 相邻音的排列方式 Ionian Aeolian 黑键最少的调 1 自然大调，Ionian 调式 W-W-H-W-W-W-H - #3, #6, #7 C 2 Dorian 调式 W-H-W-W-W-H-W b3,b7 #6 D 3 Phrygian 调式 H-W-W-W-H-W-W b2,b3,b6,b7 b2 E 4 Lydian 调式 W-W-W-H-W-W-H #4 #3, #4, #6, #7 F 5 Mixolydian 调式 W-W-H-W-W-H-W b7 #3, #6 G 6 自然小调, Aeolian 调式 W-H-W-W-H-W-W b3,b6,b7 - A 7 Locrian 调式 H-W-W-H-W-W-W b2,b3,b5,b6,b7 b2, b5 B 尽量烂熟于心，才能灵活掌握下面的。 可以发现，C 调 Ionian、D 调 Dorian、E 调 Phrygian …等音阶虽然音是一样的（白键），但主音不同，因此色彩不一样。因此对于音阶来说，主音也是很重要的。旋律小调族旋律大调的排列方式循环周期是 7，衍生出以下 7 个音阶。旋律小调与自然大调只差一个 3 音，因此衍生出的 7 个音阶与上表对应位置的音阶也只差某个音。 调式 惯用名 相邻音的排列方式 Ionian Aeolian 变音少的调式 备注 1 旋律小调 W-H-W-W-W-W-H b3 #6, #7 Ionian b3, Aeolian #6 #7   2   H-W-W-W-W-H-W b2,b3,b7 b2, #4 Dorian b2, Phrygian #6   3 增 Lydian 音阶 W-W-W-W-H-W-H #4,#5 #3, #4, #5, #6, #7 Lydian #5 注意，起点变成了 b3 而不是 3 4 泛音阶，原声音阶 W-W-W-H-W-H-W #4, b7 #3, #4, #6 Lydian b7   5 旋律大调 W-W-H-W-H-W-W b6, b7 #3 Ionian b6 b7, Aeolian #3, Mixolydian b6   6 半减音阶 W-H-W-H-W-W-W b3, b5, b6, b7 b3 Aeolian b3, Locrian #2 叫半减音阶原因是主音七和弦为半减七和弦 7 变形（Altered）音阶，超级 Locrian 音阶 H-W-H-W-W-W-W b2,b3,b4(3),b5,b6,b7 b2,b4,b5 Locrian b4 叫超级 Locrian 是 因为 Locrian 把自然大调降了5个音，而它把 6 个音全降了 和声小调族和声小调的排列方式循环周期是 7，衍生出以下 7 个音阶。 调式 惯用名 相邻音的排列方式 Ionian Aeolian 变音少的调式 备注   1 自然增大调 W-W-H-WH-H-W-H #5 #3, #5, #6, #7 Ionian #5     2   W-H-WH-H-W-H-W 1-2-b3-#4-5-6-7-1 #4, #6 Dorian #4     3 大 Phrygian 音阶 H-WH-H-W-H-W-W b2, b6, b7 b2, #3 Phrygian #3 也称 Phrygian Dominant   4   WH-H-W-H-W-W-H #2, #4 #2, #3, #4, #6, #7 Lydian #2     5     H-W-H-W-W-H-WH b2, b3, b4(3), b5, b6, bb7(6) b2, b4, b5, b7 Locrian b4 b7（Altered bb7） 注意，起点变成了 #5 而不是 5 6 和声小调 W-H-W-W-H-WH-H b3, b6 #7 Aeolian #7     7   H-W-W-H-WH-H-W b2,b3,b5,b7 b2, b5, #6 Locrian #6     和声大调族和声大调的排列方式循环周期是 7，衍生出以下 7 个音阶。 调式 惯用名 相邻音的排列方式 Ionian Aeolian 变音少的调式 备注 1 和声大调 W-W-H-W-H-WH-H b6 #3, #7 Ionian b6   2   W-H-W-H-WH-H-W b3,b5, b7 b5, #6 Dorian b5   3   H-W-H-WH-H-W-W b2, b3, b4(3), b6, b7 b2, b4 Phrygian b4   4   W-H-WH-H-W-W-H b3,#4 #4, #6, #7 Lydian b3   5   H-WH-H-W-W-H-W b2, b7 b2, #3, #6 Mixolydian b2   6   WH-H-W-W-H-W-H #2, #4, #5 #2, #3, #4, #5, #6, #7 Lydian #2 #5（增 Lydian #2） 注意，起点变成了 b3 而不是 3 7   H-W-W-H-W-H-WH b2,b3,b5,b6,bb7(6) b2, b5, b7 Locrian b7   这里讨论一下大调与小调。大调与小调就是为一些特殊的音阶起的名字，狭义上指自然大调、自然小调，广义上通常称开头（下四音列）是 W-W-H 的为大调、W-H-W 的为小调。 关系大小调是指大调音阶与小调音阶在同一个调式循环里的（即上面的同一族里的）。可以看到自然大小调、旋律大小调是关系的，而和声大小调不是关系的。另外，下面五声音阶、Blues 音阶也有关系的大小调。非七音音阶以下音阶不是七音音阶，而是六音、八音等音阶。第一个是全音音阶，是六音音阶，循环周期为 1。该音阶的特色风格是梦幻、仙境的效果。 相邻音的排列方式 C 调 常用的调与黑键个数 W-W-W-W-W-W 1-2-3-#4-#5-#6-1 都一样 第二个是增音阶，是六音音阶，循环周期为 2： 相邻音的排列方式 C 调 常用的调与黑键个数 WH-H-WH-H-WH-H 1-#2-3-5-#5-7-1 都一样 H-WH-H-WH-H-WH 1-#1-3-4-#5-6-1 都一样 第三个是减音阶，是八音音阶，循环周期为 2： 相邻音的排列方式 C 调 常用的调与黑键个数 W-H-W-H-W-H-W-H 1-2-b3-4-b5-b6-6-7-1 都一样 H-W-H-W-H-W-H-W 1-b2-b3-3-b4-5-6-b7-1 都一样 分析完备性我们对数字 12 的拆解方式作分类。先看只有 W, H 出现的。H 必须是偶数个： \\(12 = 2\\times 6\\)：即全音音阶； \\(12 = 2\\times 5+ 1\\times 2\\)：H 最近间隔 2 个、1 个，即自然大调族、旋律小调族； \\(12 = 2\\times 4 + 1 \\times 4\\)：W 和 H 只能插空排列，即减音阶；再看包含 WH 的。WH 和 H 的个数之和为偶数： \\(12 = 2\\times 4 + 1\\times 1 + 3\\times 1\\)：WH 和 H 相邻，另外一边只能和 WH 相邻，不满足条件； \\(12 = 2\\times 3 + 1\\times 3 + 3\\times 1\\)：一共 3 个 H，不能相邻，中间要用 W 和 唯一的 1个 WH 隔开。根据先用 W 还是 WH 隔开，得到和声小调、和声大调族； \\(12 = 2\\times 2 + 1\\times 2 + 3\\times 2\\)：WH 太多了，H 太少，不可能满足 WH 与 H 不相邻的条件； \\(12 = 1\\times 3 + 3 \\times 3\\)：WH 和 H 只能间隔排列，即增音阶。放宽限制如果放宽如上的限制，可以得到更多其他的音阶，但大部分不是很实用。例如一些特殊的七音音阶。那不勒斯小调族放宽了 W 与 W、H 与 H 不相邻的限制： 调式 惯用名 相邻音的排列方式 Ionian Aeolian 变音少的调式 备注   1   WH-H-H-W-W-W-H #2 #2, #3, #6, #7 Ionian ♯2     2 终极 Locrian 音阶 H-H-W-W-W-H-WH b2, bb3(2), b4(3), b5, b6, bb7(6) b2,b3(2),b4,b5,b7 Locrian b3 b4 b7     3 那不勒斯小调 H-W-W-W-H-WH-H b2,b3,b6 b2, #7 Phrygian #7（和声 Phrygian）     4   W-W-W-H-WH-H-H #4, #6 #3, #4, ##6(7), #7 Lydian #6     5 增 Mixolydian 音阶 W-W-H-WH-H-H-W #5, b7 b3,#5, b6 Mixolydian #5     6   W-H-WH-H-H-W-W b3, #4, b6, b7 #4 Aeolian #4     7 大 Locrian 音阶 H-WH-H-H-W-W-W b2, b5, b6, b7 b2, #3, b5 Locrian #3   也称 Locrian Dominant 双和声大调族放宽了两个 H 不相邻的限制： 调式 惯用名 相邻音的排列方式 Ionian Aeolian 变音少的调式 备注 1   WH-H-H-WH-H-W-H #2, #5   Ionian #2 #5   2   H-H-WH-H-W-H-WH b2,bb3(2),b5,b6,bb7(6) b2, b3, b5,b7 Locrian b3 b7   3 双和声大调 H-WH-H-W-H-WH-H b2, b6 b2, #3, #7 Phrygian #3 #7（和声大 Phrygian）   4   WH-H-W-H-WH-H-H #2, #4, #6 #2,#3,#4,##6(7),#7 Lydian #2 #6   5   H-W-H-WH-H-H-WH b2,b3,b4(3),b6,bb7(6) b2, b2,b4(3), b7 Phrygian b4 b7 也称终极 Phrygian 音阶 6 双和声小调 W-H-WH-H-H-WH-H b3,#4,b6 #4, #7 Aeolian #4 #7   7   H-WH-H-H-WH-H-W b2, b5, b7 b2, #3, b5, #6 Locrian #3 #6（大 Locrian #6）   那不勒斯大调族放宽了 W 与 W、H 与 H 不相邻的限制： 调式 惯用名 相邻音的排列方式 Ionian Aeolian 变音少的调式 备注   1 那不勒斯大调 H-W-W-W-W-W-H b2,b3 b2, #6, #7 Phrygian #6 #7（旋律 Phrygian）     2 Leading Whole-tone W-W-W-W-W-H-H #4,#5,#6 #3, #4, #5, ##6(7),#7 Lydian #5 #6（增 Lydian #6）     3   W-W-W-W-H-H-W #4,#5,b7 #3,#4,#5,#6 Lydian #5 b7（增 Lydian b7），Mixolydian #4 #5（增 Mixolydian #4）     4   W-W-W-H-H-W-W #4, b6, b7   Lydian Dominant ♭6     5   W-W-H-H-W-W-W b5,b6,b7 #3, b5 Locrian #2 #3 （也称大 Locrian 音阶？） 阿拉伯音乐   6   W-H-H-W-W-W-W b3,b4(3),b5,b6,b7 b4,b5   Half-Diminished ♭4 (or Altered Dominant ♯2)   7   H-H-W-W-W-W-W b2,bb3(2),b4(3),b5,b6,b7     Altered Dominant bb3   波斯音阶族放宽了 H 与 H、W 与 WH 不相邻的限制： 调式 惯用名 相邻音的排列方式 Ionian Aeolian 变音少的调式 备注   1   WH-H-H-W-WH-H-H #2, #6   Ionian #2 #6     2   H-H-W-WH-H-H-WH b2, bb3(2), b4(3), b6, bb7(6)   Ultraphrygian bb3     3 印度陀地音阶 H-W-WH-H-H-WH-H b2, b3, #4, b6         4   W-WH-H-H-WH-H-H #3(4), #4, #6   Lydian #3 #6     5   WH-H-H-WH-H-H-W #2, #5, b7     Mixolydian #2 #5（增 Mixolydian #2）   6   H-H-WH-H-H-W-WH b2, bb3(2), b5, bb6(5), bb7(6)   Chromatic Hypophrygian Inverse     7 波斯音阶 H-WH-H-H-W-WH-H b2, b5, b6 b2, #3, b5, #7 Locrian #3 #7     其他以下音阶族，放宽了 H 与 H、W 与 WH 不相邻的限制： 调式 惯用名 相邻音的排列方式 Ionian Aeolian 变音少的调式 备注         H-WH-H-H-WH-W-H 1-b2-3-4-b5-6-7-1               WH-H-H-WH-W-H-H 1-#2-3-4-#5-#6-7-1               H-H-WH-W-H-H-WH 1-b2-2(bb3)-4-5-b6-6(bb7)-1             印度坡尔维音阶 H-WH-W-H-H-WH-H 1-b2-3-#4-5-b6-7-1               WH-W-H-H-WH-H-H 1-#2-4(#3)-#4-5-#6-7-1               W-H-H-WH-H-H-WH 1-2-b3-#4-5-b6-6(bb7)-1               H-H-WH-H-H-WH-W 1-b2-2(bb3)-4-b5-5(bb6)-b7-1           以下音阶族，放宽了 H 与 H、W 与 WH、W 与 W 不相邻的限制： 调式 惯用名 相邻音的排列方式 Ionian Aeolian 变音少的调式 备注         H-H-WH-W-W-W-H 1-b2-2(bb3)-4-5-6-7-1             神秘音阶 H-WH-W-W-W-H-H 1-b2–3-#4-#5-#6-7-1               WH-W-W-W-H-H-H 1-#2-4(#3)-5(##4)-6(##5)-#6-7-1               W-W-W-H-H-H-WH 1-2-3-#4-5-b6-6(bb7)-1               W-W-H-H-H-WH-W 1-2-3-4-b5-5(bb6)-b7-1               W-H-H-H-WH-W-W 1-2-b3-3(b4)-b5-b6-b7-1               H-H-H-WH-W-W-W 1-b2-2(bb3)-b3(bb4)-b5-b6-b7-1           除此之外，还有大部分的六音音阶（包括 Blues 音阶、三全音音阶、普罗米修斯音阶等，详见维基百科：六音音阶），甚至八音音阶、八音以上的音阶乃至十二音的半音音阶。五音音阶如果继续放宽上面限制的第一条，即允许跨度更大的音程（3,4 等间隔），可以使得音阶中的音更少，例如本章要介绍的五音音阶。当然，可以像上面那种分类方式一样作完备的分类，但是组合方式是在是太多了，而且组合出来的情况多数都不常用，所以本章只挑几个常用的讲解。以下把间隔 3 记作 m（小三度），4 记作 M（大三度）。值得一提的是，这些常用的一般可以看成上面常用的七音音阶去掉两个音，使得它的属性和对应的七音音阶很像。经典五声调式自然大调音阶去掉 4 音 与 7 音，得到大调五声音阶，循环周期为 5，衍生出以下 5 个音阶。这种调式是中国传统调式，中国古代叫宫、商、角、徴、羽；日本受中国影响，称这类调式为雅乐调式，包括吕调式、律调式。 惯用名 排列方式 C 调 A 调 常用的调与黑键个数 其他名称 大调五声音阶 W-W-m-W-m 1-2-3-5-6-1   C 调（0个） 宫调，(日本）吕（ryo）调式（吕旋法）   W-m-W-m-W 1-2-4-5-b7-1   D 调（0个）：2-3-5-6-1-2 商调   m-W-m-W-W 1-b3-4-b6-b7-1   E 调（0个）：3-5-6-1-2-3 角调   W-m-W-W-m 1-2-4-5-6-1   G 调（0个）：5-6-1-2-3-5，C 调（0个） 徴调，(日本）律（ritsu）调式（律旋法） 小调五声音阶 m-W-W-m-W 1-3-4-5-b7-1 6-1-2-3-5-6 A 调（0个） 羽调 蓝调音阶将五声调式中两个相邻的 W 中的第二个拆解成两个 H，相当于在该位置中间加了一个音（这个音就是蓝调音乐的特色音），这样五音音阶变为六音音阶，就是 蓝调音阶。按说上面五种都可以得到对应的蓝调音阶，但是常用的只有这两种。 惯用名 排列方式 C 调 A 调 常用的调与黑键个数 备注 小调蓝调音阶 m-W-H-H-m-W 1-b3-4-b5-5-b7-1 6-1-2-b3-3-5-6 A 调（0个） （C 调）特色音位于 #4 (b5) 位置 大调蓝调音阶 W-H-H-m-W-m 1-2-b3-3-5-6-1   C 调（0个） （C 调）特色音位于 #2 (b3) 位置 蓝调音阶虽然是六声音阶，但不符合上一章的规则（W 与 WH 相邻了），而且它是由五声衍生的，所以放在这里讲。日本调式自然大调音阶拿掉 2 音与 5 音，得到日本的俗乐调式，循环周期为 5，衍生出以下 5 个音阶。 惯用名 排列方式 C 调 A 调 常用的调与黑键个数 平（hira）调子 W-H-M-H-M 1-2-b3-5-b6-1 6-7-1-3-4-6 A 调（0个） 岩户(iwato）调子 H-M-H-M-W 1-b2-4-b5-b7-1   B 调（0个）：7-1-3-4-6-7 阳旋法，田舍节（inaka bushi）调子 M-H-M-W-H 1-3-4-6-7-1   C 调（0个） 本云井（hon kumoi）调子 H-M-W-H-M 1-b2-4-5-b6-1   E 调（0个）：3-4-6-7-1-3 阴旋法，都节（miyako bushi）调子 M-W-H-M-H 1-3-#4-5-7-1   F 调（0个）：4-6-7-1-3-4 以上出现的“旋法”本身的意思是日本筝乐器的调音方式。东南亚调式自然大调音阶拿掉 2 音与 6 音，也得到一个循环周期为 5 的调式，衍生出以下 5 个音阶。这些音阶很有东南亚风味。 惯用名 排列方式 C 调 常用的调与黑键个数 (日本）琉球旋法 M-H-W-M-H 1-3-4-5-7-1 C 调（0个）   H-W-M-H-M 1-b2-b3-5-b6-1 E 调（0个）：3-4-5-7-1-3   W-M-H-M-H 1-2-#4-5-7-1 F 调（0个）：4-5-7-1-3-4   M-H-M-H-W 1-3-4-6-b7-1 G 调（0个）：5-7-1-3-4-5   H-M-H-W-M 1-b2-4-b5-b6-1 B 调（0个）：7-1-3-4-5-7 其它音阶其他音阶不在以上分类体系的就不常用了，很多都是Bebop、自由爵士中的。这里做一个列举，值得注意的单独讲解。 音的个数 惯用名 排列方式 C 调 常用的调与黑键个数 备注 4 三全音音阶（四声音阶） m-m-m-m 1-b3-#4-6-1     " }, { "title": "深度学习训练实践经验", "url": "/posts/deep_learning_training_practical_experience/", "categories": "科研", "tags": "学习笔记, 技术", "date": "2021-04-20 00:00:00 +0800", "snippet": "深度学习并不是搭了模型喂了数据就保证出货的，训练过程的各种细节需要仔细处理，通过加一些 trick，才能训得好的模型，有很多经验上的东西。本文是《学习笔记：深度学习训练理论知识》的实践篇，按照该笔记的顺序组织。一、激活函数激活函数一般的选择经验 ：ReLU &amp;gt; ReLU 变种 &amp;gt; Tanh &amp;gt; Sigmoid。 无特别情况一般选 ReLU 即可； ReLU 虽然有 “挂掉” 的风险，但在实际中也就挂 10-20%，通常无大碍； 使用 ReLU 时要注意调好配置，例如选用小的 bias 初始化（为让这些直线尽量穿过 data cloud）、学习率不要过大（防止不稳定进入 “挂掉” 状态）； ReLU 变种比较花哨，多是试验性质的，实践中用的也不多，可以试试； Tanh 偶尔也可以试试，不要太指望； Sigmoid 不要用，太古老了。二、数据预处理数据标准化几乎是每个深度学习项目必须要做的。注意图像数据通常只作零中心化，不作归一化，因为它的每一维（像素）范围都差不多。分布偏移纠正非常冷门，且有作弊的嫌疑，了解深度学习中可能存在这样的问题即可，除了特定的科研一般不用。三、网络结构集成学习通常用在实际项目或比赛（如 Kaggle）中，通常能给结果带来固定的提高（如几个百分点） 。科研上因为研究具体的模型，不会去做模型集成，但有人也会用一些涉及集成的小 trick 提高效果。四、参数初始化 不可使用全零初始化； 随机初始化是一种简单的方式，若想简单处理可以选用，不过要注意选好标准差； 一般使用 Xavier 初始化就能达到不错的效果。实践中 “预训练 + 微调” 训练策略非常常用，因为一些大公司已经在超大型数据集上训练过大型网络，取得了惊人的效果，我们一般人没有这么强的计算资源，无法从头训练，还想分享到这些成果，就要 “预训练 + 微调” 了。无论是 CV 还是 NLP 任务，都有很多大型网络的预训练权重可供下载，例如： ResNet 在 ImageNet 数据集（约 1400 万张图像）； BERT 的几个版本：BERT-base 在 BookCorpus 数据集（约 8 亿字）；BERT-large 在英文维基百科（约 25 亿字）；等等。由此，通常设计深度模型的思路：用现有流行大模型作 backbone，作为通用的特征提取器，使用预训练权重，其后接剩下的部分负责具体的任务，由自己设计。在微调时更新不能剧烈（即微调的字面意思），因为通常模型在预训练时已足够收敛，预训练数据往往比微调多。例如，一般设置预训练时 1/10 的学习率。五、优化器Adam 算法是最常用的、也是很多深度学习框架默认的优化器（Adam 论文引用有 10w 次！）。推荐的超参数：\\(\\rho=0.9, d=0.999, \\eta=10^{-3}, 5\\times 10^{-4}\\)。如果要单独使用的话，AdaGrad 与 RMSProp 相比倾向于不用前者。最原始的 SGD 也不推荐使用。学习率调整可当作独立的 trick 施加在超参数 \\(\\eta\\) 上，这些 trick 常用于 SGD，在 Adam 等已有学习率衰减机制的优化器上不常用。实践中它的用法一般是先不加调整，观察效果，根据效果设计适当的调整机制。注意，学习率不可设得太低，它对训练时间的影响比其他任何因素都厉害。对于二阶算法 L-BFGS，实践发现它更适合于 full-batch 的深度学习。少数特定的深度学习项目需要 full-batch，平时见到的通常是 mini-batch 的，所以该算法也不常用。六、损失函数无特别需求一般选用 L2 正则化，即 weight decay。L1 正则化会给模型带来稀疏性，较少使用。Smooth L1 损失出现于 Faster R-CNN 中。七、超参数优化验证集一般划训练集的 20-30%。手动粗选的实践经验即调参经验。以下汇总一些 tips： 参数更新幅度的比值一般在 0.001 附近，过大或过小都可能有问题； （待更新）对于搜索算法，无需过多地担心超参数之间不独立的情况，每次独立地调少量的超参数是问题不大的。一般一次独立地调 1, 2 或 3 个参数，不宜太多，否则将极大加重搜索算法的负担。过拟合问题可以看到，解决欠拟合、过拟合一般从模型复杂度下手。在实际项目中，通常的做法是设计多个不同复杂度的模型，分别在验证集上作出学习曲线进行比较，从中选择最接近“正常”学习曲线对应的模型，称为模型选择。在实际项目中，通常需要跑很多天，epoch 是比较慢的，一般是时刻盯着 Tensorboard 画出的 loss 图，根据以上三种情况的特点判断欠拟合与过拟合。怎么把握 loss 多大才算充分下降、epoch 要忍几轮这个度？就是靠经验和多次尝试。不可一开始就急着往模型上加防止过拟合的 trick。要在需要的时候、出现过拟合问题时一点点往上加。另外，有些过拟合技巧不可同时应用，例如 Dropout 和 Batch Normalization。目前大部分网络使用的 Batch Normalization 防止过拟合，而且使用这个就够了。如果它不够用，再考虑正则化等手段。深度学习框架中一般都有发现临界时刻、实现自动早停的 API，无需人手工指定最大 epoch 数、用肉眼观察。" }, { "title": "学习笔记：深度学习训练理论知识", "url": "/posts/studynotes_deep-learning-training/", "categories": "科研", "tags": "学习笔记, 机器学习, 机器学习", "date": "2021-04-20 00:00:00 +0800", "snippet": "本文统一整理深度学习的训练细节的理论知识，算是给以后的科研实践打基础。具体的实践经验见另一篇笔记：《深度学习训练实践经验》。本文按训练流程分几个主题，包括激活函数的选择、数据预处理、参数初始化、优化器的选择、超参数优化等等。我将在每个主题中介绍该主题的理论，介绍其中的各种方法、trick 及其优缺点，最后介绍实际项目中的实践经验。另外过拟合、欠拟合是深度学习的大问题，将单独抽出一章来讲解其理论并介绍应对过拟合的方法。本文主要参考计算机视觉课程 Stanford CS231n以及书《动手学深度学习》的第 4 章。目录 一、激活函数 Sigmoid, Tanh ReLU 及其变种 Maxout 二、数据预处理 数据标准化 降维 数据增强 分布偏移校正 三、网络结构 Dropout Batch Normalization 集成学习 四、参数初始化 简单的初始化 Xavier 初始化 Kaiming 初始化 迁移学习（预训练 + 微调） 五、优化器 随机梯度下降（SGD） 学习率调整 SGD + Momentum AdaGrad, RMSProp Adam 二阶优化器 六、损失函数 七、超参数优化 手动粗选范围 参数搜索算法 过拟合问题总论 过拟合、欠拟合的判断 解决过拟合、欠拟合 开始之前，先把反向传播的原理1写在这里，这是深度学习训练的核心算法。后面用到的符号与以下公式一致。根据链式法则，反向传播是按层迭代的：损失函数对一层神经元（激活前）\\(a_j^l\\) 的梯度按层向后传播，传播就是不断乘以该层权重以及该层激活函数的导数：\\[\\delta_{j}^{l}=\\sum_{k=1}^{r^{l+1}} \\delta_{k}^{l+1} w_{j k}^{l+1} g^{\\prime}\\left(a_{j}^{l}\\right)=g^{\\prime}\\left(a_{j}^{l}\\right) \\sum_{k=1}^{r^{l+1}} w_{j k}^{l+1} \\delta_{k}^{l+1} \\ \\ \\ \\ (1)\\]得到对第 l 层神经元的梯度后，再乘以第 l-1 层（激活后）\\(o_i^{l-1}\\) 就是对权重 \\(w_{ij}^{l}\\) 的梯度：\\[\\frac{\\partial L}{\\partial w_{i j}^{l}}=\\delta_{j}^{l} o_{i}^{l-1}=g^{\\prime}\\left(a_{j}^{l}\\right) o_{i}^{l-1} \\sum_{k=1}^{r^{l+1}} w_{j k}^{l+1} \\delta_{k}^{l+1} \\ \\ \\ \\ \\ \\ (2)\\]一、激活函数激活函数（activation function）是作用在每个神经元上的函数，神经元激活前的 \\(a_j^l\\) 受其激活为 \\(o_j^l\\)。激活函数不仅构成模型的一部分，而且也非常影响训练过程。不合适的激活函数会大幅降低训练效率，甚至“毙掉”训练过程。常用的激活函数有 Sigmoid、Tanh、ReLU 等，下面将探讨它们对训练过程的影响。Sigmoid, TanhSigmoid 函数是最早使用的、也是用在第一批神经网络的激活函数。优点： 值域为 \\([0,1]\\)，可以解释为概率； 原理类似于真实神经元的激活，有很好的生物学解释。缺点： 很容易产生梯度消失现象（又称饱和现象，下文解释），导致靠输入端层的梯度很小，参数不更新； 值域不以零为中心，可能导致 zig-zag 现象出现，降低参数更新效率； 指数函数计算代价高。梯度消失现象：随着反向传播，梯度与很小的数值累乘最后无限接近 0，使梯度下降参数更新非常缓慢。反向传播时，中间某层某神经元激活函数的导数 \\(g^{\\prime}\\left(a_{j}^{l}\\right)\\) 特别小，就会导致之后在其基础上累乘的梯度也特别小，即产生梯度消失现象。根据公式 (1)，反向传播计算梯度是累积的，网络越深，越靠近输入端权重的梯度消失得越厉害。Sigmoid 很容易梯度消失，因为它的导数很容易小，见其导数图像：只要某层某神经元的输入 \\(a_j^l\\) 离 0 很远，导数就很小（称为饱和）；即使在 0 附近，导数值也只有 0.25 左右，仍然会随着累乘让梯度逐渐消失。对零中心问题的解释：设第 l-1 层选用 Sigmoid 激活函数，由于其值域恒正，无论之前层如何，该层所有神经元的输出 \\(o_i^{l-1} (i = 1,2,\\cdots)\\) 都为正。考虑其后连接的权重的梯度，根据反向传播公式 (2)，考虑 l-1 层所有神经元连接到第 l 层某一个神经元的权重（即 \\(w_{ij}^l\\) 固定 \\(j\\)，考虑所有 \\(i\\)），由上层传到第 l 层这个神经元的信号 \\(\\delta_j^l\\) 都相同，且 \\(o_i^{l-1}\\) 同号，则这些权重的梯度也同号。下图将这些权重画在二维空间中示意。由于梯度同号，权重更新时要么向右上、要么向左下（视 \\(\\delta_j^l\\) 符号而定），要是最优解实际在起点的右下方时（高维空间更可能发生），就会发生图中所谓 zig-zag（锯齿）现象。纵观整个网络，哪一层用 Sigmoid 激活，哪一层后面的权重就像被划为几个大组（\\(w_{ij}^l\\) 每个 \\(j\\) 遍历 \\(i\\) 是一组），组内梯度永远同号。这样事实上限制了参数更新的自由。值域恒正的激活函数比较极端，会导致组内梯度永远同号。更一般的情况中，激活函数值域大于 0 占的情况多一点，导致同号现象不是必然出现，但概率也大，也会降低参数更新效率。就这一点，最理想的激活函数应该是值域以零为中心（zero-centered），让 \\(o_i^{l-1}\\) 正负号出现的机会均等。Tanh 函数就是把 Sigmoid 函数拉伸并平移到原点中心。它是对 Sigmoid 函数的改进，主要解决零中心问题。优点：值域以零为中心。缺点： 梯队消失现象； 指数函数计算代价高。ReLU 及其变种ReLU 函数（Rectified Linear Unit，整流线性单元）第一次出现在 2012 年 AlexNet 中，随后被大量使用。仔细观察 ReLU 及其导数，可以看到其逻辑很简单，可以描述为只有两种状态：激活与未激活，以函数输入 \\(a_j^l\\) 的正负作为唯一判断。对一个 ReLU 神经元： \\(a_j^l\\) 为正意味着被激活，前向传播的输入值、反向传播的梯度都原封不动地放行； \\(a_j^l\\) 为负则意味着未激活，前向传播、反向传播到此全被堵住，无论输入值还是回传的梯度全都变 0。（注：前向传播考虑 \\(g(a_j^l)\\)，前者为 a_j^l，后者为 0；反向传播考虑 \\(g&#39;(a_j^l)\\)，前者为 1，后者为 0）优点： 在大于 0 部分不会出现梯度消失现象； 函数很简单，计算效率非常高。缺点： 在小于 0 部分梯度不仅是消失，而是直接变为 0，有网络部分权重永不更新的危险，即有一些 ReLU “挂掉”； 值域不以零为中心。对 ReLU “挂掉” 的解释：上面看到只要喂给 ReLU 的输入是负的，上一层连接到此神经元的那些权重就都不更新此步了。在一个大网络里这种事会经常发生，是会影响效率的。除此之外有时还有 “挂掉” 的危险。一个 ReLU 神经元 \\(RELU(a_j^l)\\) “挂掉” 是指它永不被激活，导致连接的权重 \\(w_{ij}^l (i=1,2,\\cdots)\\) 永不更新，出现网络的一部分还没训好就结束训练的现象。什么时候会 “挂掉” 呢？以此示意图解释，设上层 l-1 层输出值 \\((o_1^{l-1}, o_2^{l-1},\\cdots)\\) 是二维空间的点，所有训练数据对应的点构成一片 data cloud。一个训练数据 l-1 层输出值 \\(o^{l-1}\\) 经过权重的线性组合得到 \\(a_j^l\\)，要看它大于还是小于 0，就是看它在 cloud 中对应点落在直线的哪边（直线位置是权重决定的）。如果不巧直线正好落在 cloud 外，这样无论拿什么训练数据训练就都无法更新了，即 “挂掉” 永不更新。在参数更新的过程中，难免会不小心训练出这种不巧的权重（例如优化器不太稳定时，如学习率太高），而一旦发生这个意外，后果还是很严重的。 当然，上面假设了 data cloud 不变化，由于图示空间实际是它的表示而不是原始输入，前面的权重也会更新，所以 data cloud 是变化的，有可能在以后 data cloud 会跨过直线，将其盘活。但事实上还是有 “挂掉” 现象的存在，不妨碍我们上面简单地理解。理论分析有论文可参考：Dying ReLU and Initialization: Theory and Numerical ExamplesLeaky ReLU、参数化 ReLU、ELU 等都是 ReLU 大规模应用随后提出的对 ReLU 的改进。优缺点：Leaky ReLU 解决了 ReLU “挂掉” 的问题；参数化 ReLU 就是把 Leaky ReLU 的负斜率变成了可学习的参数，更加灵活；ELU 试图融合 ReLU 和 Leaky ReLU，也解决了 “挂掉” 的问题，但引入的指数函数让计算代价变高。 对它们的优缺点还有很多争议。激活函数是深度学习的一个领域，感兴趣可以阅读相关论文。MaxoutMaxout 是 Goodfellow 于 ICML 2013 提出的一种激活函数。它最大的特点是输入不是一个，而是多个，它的输出是取多个输入的最大值。实际上它是单输入 ReLU 函数的推广，因为 ReLU 就是将输入和常值 0 比较大小。在结构关系上，它也是对神经元的作用，即称呼 “Maxout 神经元”。Maxout 神经元形式上可以看成与普通的神经元一样——尾部连接若干个前一层的神经元。但实质上这些权重是 2 份互相独立的，只是共用了两端连接的神经元。优缺点：继承了 ReLU 的所有优点，缺点是参数量加大了多倍。二、数据预处理这里的数据预处理是指能作用于训练过程的统一范式，而不是实际项目中对真实数据的具体处理流程。对训练过程有作用的预处理方法主要是以下几种。数据标准化数据标准化（normalization），最常见的做法是将数据的每一维减掉该维的均值，并除以标准差。前者称为零中心化，后者称为归一化。零中心化作用： 非零中心化的数据对模型权重的改变更加敏感，导致难以优化，见下图； 当激活函数值域零中心化后，对数据零中心化也可以减少 zig-zag 现象； 让 data cloud 集中于原点附近，在上面 ReLU “挂掉” 的问题中，如果配合初始化 bias 参数为 0，可以在训练之初就避免直线不穿过 cloud，防止 “挂掉”； 最主要原因还是只有零中心化后才可以谈归一化，不减掉均值就除以标准差是没有意义的。归一化作用：让数据的各个特征的值域范围统一起来，使损失函数更加倾向于圆润而不是狭长，有利于优化。狭长的损失函数也容易导致 zig-zag 现象，见下图，在学习率稍大时极容易 overshot。 数据标准化对训练集和测试集都要做，且所用的均值和标准差必须是一样的，统一用训练集的（因为训练时没有测试数据，而测试时有训练数据）。还有很多其他类似标准化的处理手段： min-max 标准化：每一维减掉该维的最小值，并除以极差，可以把数据缩放到 \\([0,1]\\) 区间。极容易受个别数据影响，不常用； 比例归一化：每一维除以该维的和，也可以把数据缩放到 \\([0,1]\\) 区间，不常用。降维有一些手段可以起到降维的作用： 主成分分析（PCA）：相当于把原数据做了线性变换，使其第一维是最主要成分（方差最大），第二维是次主要成分，以此类推。它提前抓取了（线性意义下的）主要特征，某种程度上方便以后的训练。如果愿意的话，可以选取前 \\(k\\) 个主成分，即可降维； PCA 白化（whitening）在 PCA 基础上减掉均值除以标准差。 ZCA 白化 在 PCA 白化基础上作 PCA 的逆线性变换。 线性判别分析（LDA）：相当于把原数据作了线性映射（比原来维数低），使数据在新空间上分得尽量开。数据增强数据增强（data augmentation）是将现有训练数据衍生出各种变换的数据，将其加入训练。它起到增大数据量的作用，因此是减轻过拟合的手段。以图像数据为例，常用的变换有水平/垂直翻转、裁剪缩放、色彩抖动等等甚至很多高级的变换（参考数字图像处理知识）。当然数据增强并不只是简单地寻找各种变换，还发展出了各种学习范式，以下是一个例子，使用在 ResNet 原论文中： 训练阶段：随机以统一尺寸裁剪训练图像，使用裁剪的片段训练； 测试阶段：裁剪测试图像的几个固定位置（例如四个角+中间），输入模型得到结果，再投票或平均。分布偏移校正机器学习假设训练数据和测试数据同分布，即都是从一个联合分布中取出来的：\\(p_s(\\mathbf{x}, y)\\)。但实际数据可能并不满足这一条件，称为分布偏移，这种情况下测试效果肯定不好。分布偏移有以下几种类型，下面是统计学上的理解： 协变量偏移（covariate shift）：\\(p_s(\\mathbf{x}, y) = p(\\mathbf{x}) p(y \\mid \\mathbf{x})\\)，协变量 \\(\\mathbf{x}\\) 的分布 \\(p(\\mathbf{x})\\) 不同，而标签与协变量的关系 \\(p(y \\mid \\mathbf{x})\\) 相同； 标签偏移（label shift）：\\(p_s(\\mathbf{x}, y) = p(y) p(\\mathbf{x} \\mid y)\\)，标签 \\(y\\) 的分布 \\(p(y)\\) 不同，而标签与协变量的关系 \\(p(\\mathbf{x} \\mid y)\\) 相同； 概念偏移（concept shift）：而标签与协变量的关系 \\(p(y \\mid \\mathbf{x})\\) 不同。协变量偏移和标签偏移其实是同一个问题的两种形式，比较常见，例如下面两幅图的情况；概念偏移是最严重的偏移，意味着训练数据和测试数据蕴含的知识不是一码事，例如二分类问题，训练集 1 代表猫、0 代表狗，到了测试集 1 代表黑物体、0 代表白物体，此时标签 1,0 的含义都不一样了，在分类猫狗上训练的模型完全不适用分类黑白物体。有时候分布偏移并无大碍，尤其是非概念偏移，模型还是能正常工作。当模型想尽办法怎么调也效果不好时，就要考虑数据是否偏移过大，并解决分布偏移了。识别是否有分布偏移必须比较训练数据和测试数据，通常是通过统计、观察来判断，至于分布偏移属于哪种类型，根据数据是判断不出来的，只能通过对当前问题的理解、经验来判断。解决分布偏移的方法：如果判断是概念偏移，那没救了，出了从零开始收集新数据别无妙方；如果不是概念偏移，即协变量偏移和标签偏移，《动手学深度学习》书中提供了两种偏移纠正算法：协变量偏移纠正、标签偏移纠正。协变量偏移纠正算法： 构造一个二分类数据集：\\({(\\mathbf{x}_1, -1), \\cdots, (\\mathbf{x}_n,-1), (\\mathbf{u}_1, 1),\\cdots, (\\mathbf{u}_m, 1)}\\)，其中 \\(\\mathbf{x}_i,\\mathbf{u}_j\\) 分别来自训练集、测试集； 训练 Logistic 回归模型 h； 在真实训练时，损失函数对数据加权：\\(min \\frac1n \\sum_{i=1}^n \\beta_i l(f(\\mathbf{x}_i), y_i)\\)，权重值 \\(\\beta_i = \\exp(h(\\mathbf{x}_i))\\) 或 \\(\\min(\\exp(h\\mathbf{x}_i), c)\\)（\\(c\\) 为常数）标签偏移纠正算法是对 \\(y_i\\) 做处理，不再介绍。 注意纠正算法应用在正式训练之前，要用到测试数据的部分信息，可能是一种不被允许的作弊，应慎用。三、网络结构本章介绍对网络结构下手的通用的训练 trick。DropoutDropout 是 Google 于 2014 年提出的 一种层。它将上一层输出 \\(o_1^{l-1}, o_2^{l-1}, \\cdots\\) 以设定好的概率 \\(p\\) 随机置为 0，得到它的输出 \\(o_1^t, o_2^t, \\cdots\\)。注意： 是将一层的激活值 \\(o\\) 置 0，而不是激活前的 \\(a\\)。后者的效果是与被置 0 的 \\(a\\) 相连的权重梯度为 0，完全不更新；而 Dropout 并不是这样的，可理解为它将某些参数的完全不更新现象平均到所有参数了； 丢弃概率 \\(p\\) 是超参数而不是可学习的参数，因此 Dropout 是一个不带参数的层。类似 Dropout 的有 DropConnect，置 0 的不是输出而是一层的权重。它不是单独的层，而是依附于别的层，应称呼 “xx层加了 DropConnect”。它和 Dropout 起的作用类似。在测试阶段，模型必须固定，这种随机的 Dropout 层就要固定下来。简单的做法是不管 Dropout 层，直接原封不动地通过，但这使得测试与训练的模型不一致。折中方案是在测试阶段对 \\(o_i^{l-1}\\) 乘以丢弃概率 \\(p\\)，它等价于在训练阶段乘以 \\(p\\)。应选择后者，要尽量将计算量放到训练时。Dropout 起到的作用： 缓解过拟合：迫使前向传播时不时丢弃一些特征，减少模型对使用所有特征的依赖，更容易学到泛化的知识； 打破对称性陷阱：见 “四、参数初始化” 章节。缺点：会降低训练速度，因为 Dropout 可理解为将某些参数的完全不更新现象平均到了所有参数。Batch NormalizationBatch Normalization 是 2015 年论文 Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift 提出的。它不是数据预处理手段，而是一种带参数的层。在训练过程中，当一个 batch 的数据前向传播到 BN 层，设上一层输出为 \\(\\mathbf{x}_1,\\cdots,\\mathbf{x}_m\\)（m 不是维度而是 batch_size），则每个数据都作标准化\\[\\mathbf{\\hat{x}}_i = \\frac{\\mathbf{x}_i - \\mathbf{\\mu}_{batch}}{\\mathbf{\\sigma}_{batch}}\\]注意是按元素的，其中均值、方差都是对于这个 batch 的，所以叫 Batch Normalization。为了增加一些表达能力，还在其后增加了还原操作：\\[\\mathbf{o}_i = \\gamma \\mathbf{\\hat{x}}_i + \\beta\\]\\(\\gamma, \\beta\\) 即 BN 层可学习的两个参数。BN 层实际上是不稳定的、有随机性的，它不是传统意义上的层，更应该看成是一种训练的 trick，因为它严重依赖训练过程，与训练 batch 的划分方式和 batch_size 都有关。在测试阶段，模型必须固定，不稳定的 BN 层就要固定下来。标准化操作是必须固定不变，且应与训练时的基本统一。这里一种方案是，均值方差计算整个训练集的；若对整个训练集计算代价过大，另一种方案是采用整个训练集均值方差的估计。Batch Normalization 起到的作用： 增加模型训练的鲁棒性，对初始化、学习率更加不敏感，更易于训练； 某种程度上可缓解过拟合，因为作了标准化后相当于去除了数据的一部分数据特有的信息；另外 BN 层的随机性也给模型增加了一些扰动，使其泛化能力更强。 BN 层只有在 batch_size 较大时才能发挥更好的作用，因为 batch 的均值方差更加具有代表性。举例极端情况，如果 batch_size 设为 1，BN 每次都会将这个唯一的训练数据置为常数 \\(\\beta\\)，直接导致无法训练。集成学习集成学习（model ensemble）是对同一个任务训练多个模型，将其结果融合。较复杂的集成学习的方法有很多，如 AdaBoost，Bagging 等等，这是机器学习课程应学的知识，也是一大研究领域，不再讲述。简单的集成学习方法就是同时训练多个模型，结果（分类问题）投票，或取平均，或（分类问题）对计算的排名取平均。集成学习的作用是提高模型的泛化能力，缓解过拟合，但也会带来模型规模的增大和计算代价。四、参数初始化参数初始化（initialization）对训练过程非常重要，涉及到从哪里启动训练的问题，稍有不慎会让整个训练过程朝向不好的方向发展。参数初始化也是深度学习研究中的专门的领域，有成百上千种初始化方法被提出。 以下讨论的参数初始化一般是针对权重的，很少有讨论 bias 的初始化规则的，通常置为 0 即可。简单的初始化以下是几种简单的初始化方式： 全部初始化为零； 随机初始化：所有参数从某分布中随机取样，通常为正态分布、均匀分布等。很多时候这种未经仔细设计的初始化方式会导致数值稳定性问题，主要是梯度消失和梯度爆炸： 初始化一开始就是 0 或非常接近 0，容易梯度消失，因为反向传播公式 (1) 计算梯度需要与权重值累乘； 初始化如果过大，容易梯度爆炸（指随训练过程计算的梯度无限增大），原因同上。上述问题前者导致参数不更新或更新很慢，后者使参数更新非常剧烈不稳定（即数值不稳定）。此外，对一个对称的网络的权重对称地初始化（例如全部初始化为同一个值）易导致对称性陷阱，此时如果反向传播过程也是对称的（例如普通的 SGD 算法），最后训练出来地东西也是对称的，这样就和训练了一个简单得多的网络没区别了，影响网络的表达能力。只要打破一个地方的对称——初始化、优化算法、网络结构，最后训出来的东西也就不对称了。以上两种初始化方式，全零初始化是最蠢的，它不仅导致梯度消失，也容易触发对称性陷阱，是必须弃用的初始化方式。随机初始化应当注意分布范围，避免数值过大或过小，例如正态初始化应适当选取均值与标准差。Xavier 初始化Xavier 初始化方法来源于 Xavier Glorot 和 Bengio 2010 年的论文 Understanding the difficulty of training deep feedforward neural networks。它的核心思想认为随机初始化不应对所有参数选用完全相同的分布，应当在层间有差异，具体来说与该层参数数目有关，理论分析见论文。Xavier 初始化给出了随机初始化应采用的分布参数。对第 l 层参数 \\(w_{ij}^l\\)，\\(n_{in}, n_{out}\\) 即 i,j 的个数： 正态分布应选 \\(N(0, \\frac{2}{n_{in}+n_{out}})\\)； 均匀分布应选 \\(U(-\\sqrt{\\frac{6}{n_{in}+n_{out}}}, \\sqrt{\\frac{6}{n_{in}+n_{out}}})\\)。如上图，根据原论文的实验，各层全一样的正态初始化会导致随着前向传播（l 增大方向）神经元输出值 \\(o_j^l\\) 方差逐层递减、反向传播（l 减小）第 l 层参数 \\(w_{ij}^l\\) 的梯度的方差逐层递减，从而导致各参数更新的同质化。而 Xavier 初始化做的修正可以去除这种趋同的趋势。Kaiming 初始化何恺明于 2015 年论文 Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification 提出 Kaiming 初始化。上述几种初始化方式适用于激活函数为 Sigmoid、Tanh 等函数的基础上的，Kaiming 初始化对 ReLU 函数的情况作了额外的修正，因为它有一半被砍为 0：对选用激活函数为 ReLU 的层 l，参数 \\(w_{ij}^l\\)： 正态分布应选 \\(N(0, \\frac{1}{n_{in}})\\) 或 \\(N(0, \\frac{1}{n_{out}})\\)； 均匀分布应选 \\(U(-\\sqrt{\\frac{3}{n_{in}}},\\sqrt{\\frac{3}{n_{in}}})\\) 或 \\(U(-\\sqrt{\\frac{3}{n_{out}}},\\sqrt{\\frac{3}{n_{out}}})\\)。迁移学习（预训练 + 微调）当模型较大而数据较少时，可以直接从其他人在别的任务上训练（称为预训练）好的参数开始训练（称为微调，fine-tuning），将别的任务的知识迁移到自己的任务，这种训练策略可称为迁移学习。 名词解释：预训练使用的任务称为预训练任务，迁移过来的那个大型网络称为预训练模型；自己的任务是训练的目标，又称为下游任务。例如在 BERT 原论文中，预训练模型指 BERT 头本身，预训练任务是 MLM、NSP，下游任务就是作者跑的各种现实的 NLP 任务。我们只要做好参数初始化，训练过程就能自动将知识迁移过来。通常的做法是将网络分为 2 部分：一部分参数有被预训练过，将其初始化为预训练的权重；另一部分参数则正常地初始化，如随机初始化。前者一般是上层网络；后者是下层，可看成分类器、输出头。显然，留给预训练的参数越少，迁移能力越强，但计算量和要求的数据量也随之增加。在预训练 + 微调模式下，由于微调任务前的预训练阶段通常使用了大型数据，大大提高了模型的泛化能力，会缓解微调任务的过拟合。五、优化器深度学习的优化器有很多，参见综述论文 An overview of gradient descent optimization algorithms。本章介绍最基本的梯度下降法，以及其上的改进：Momentum，AdaGrad，RMSProp，Adam 等。随机梯度下降（SGD）最简单的优化算法是梯度下降（gradient descent），前面加随机二字是指损失函数并不是对所有训练数据求和，而是对采样出的一个 batch 求和，有点像用样本去估计，有一定的“随机性”。一次使用所有训练数据求和的称为 full-batch 梯度下降。参数更新公式为（以下 \\(w\\) 表示任意一个参数）\\[w_{t+1} = w_t - \\eta \\nabla l(w_t)\\]此算法有一个超参数：学习率（learning rate）\\(\\eta\\)，从总体上控制参数更新的步长幅度。缺点： 使得参数更新路线直来直去、呈现出突变的折线，例如当损失函数不够圆润时，容易发生 zig-zag 现象（见“数据标准化”一节第 2 幅图），降低更新效率； 容易陷入局部最小点或鞍点，因为在这些点附近梯度接近 0，更新步伐缓慢。深度学习维度高，在损失函数中更容易看到这种点； 随机性的坏处：batch 的数据可能不具有代表性，batch 损失函数可能不是真实损失的好的估计，导致更新的步伐比较混乱、随机，没有目的性。学习率调整学习率调整指在训练过程中不断地改变学习率，而不是一直保持不变。通常认为学习率应随着训练过程逐渐放缓，即学习率衰减。调整方式例如： 等间隔衰减：\\(\\eta = \\eta_0 - kt\\)； 指数衰减：\\(\\eta = \\eta_0 e^{-kt}\\)； 分数衰减 \\(\\eta = \\eta_0 /(1+kt)\\)； 基于验证集指标：当验证集 loss 下降不显著时，采用某种方式降低学习率； ……SGD + MomentumMomentum 的想法是让参数的更新运动具有惯性，从物理上理解即拥有了动量（momentum）属性。参数更新公式为\\(w_{t+1} = w_t - \\eta v_t\\)\\(v_{t+1} = \\rho v_t + \\nabla l(w_t)\\)\\(\\mathbf{v_0} = 0\\)消去中间变量可得下式，可以看到参考的梯度不只是当前位置的，还会参考上次、上上次、……一直到第一次位置的，之前梯度的影响力 \\(\\rho^t\\) 随时间指数衰减。\\[w_{t+1} = w_t - \\eta (\\nabla l(w_t) + \\rho \\nabla l(w_{t-1}) + \\rho^2 \\nabla l(w_{t-2}) + \\cdots + \\rho^{t} \\nabla l(w_0)\\]此算法的超参数：学习率 \\(\\eta\\)，衰减速率 \\(\\rho \\in [0,1]\\)，\\(\\rho=0\\) 时就是 SGD。优点： 使参数更新路线更平滑； 解决了 SGD 容易卡在局部最小点和鞍点的问题，当参数更新路过这些点时，惯性更容易推着它越过这种点，而不是立刻被这些点吸引住。缺点： 惯性使得参数更新容易越过（overshot）最优值，多兜圈子（想象卫星降落星球，不是垂直下落，而是反复绕圈接近）。下面的 Nesterov Momentum 缓解了这个问题。Nesterov Momentum 对 Momentum 作了如下修正，它参考不是在参数当前位置 \\(w_t\\) 的梯度，而是每次越过一点到达一个预判位置 \\(w_t + \\rho v_t\\) 的梯度。\\(v_{t+1} = \\rho v_t + \\nabla l(w_t + \\rho v_t)\\)\\(w_{t+1} = w_t - \\eta v_t, v_0 = 0\\)AdaGrad, RMSPropAdaGrad（Adaptive Gradient）是 Jon Duchi 于 2010 年提出的一种学习率衰减的方法，它的特点是自适应，学习率衰减不只由时间（迭代次数）决定：\\(w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{G_t}+\\epsilon} \\nabla l(w_t)\\)\\(G_{t+1} = G_{t} + (\\nabla l(w_{t+1}))^2, G_0 = 0\\)\\(\\epsilon\\) 是一个很小的常数，防止除以 0。可以看到其调节机制是：一个累计变量 \\(G\\) 不断累加梯度值的平方，随着训练过程的进行逐渐增大，而它在学习率分母下（注意这里把 \\(\\frac{\\eta}{\\sqrt{G_t}+\\epsilon}\\) 理解为学习率），意味着学习率越来越小，训练速度放缓。且梯度越大，累加到 \\(G\\) 越快，训练速度放缓地越快。优点： 自适应的学习率可以让参数在快要接近最优解时自动放缓更新速度，进入更精细地搜索，防止越过最优解。因为接近最优解之前往往有较大的梯度值让参数冲向最优解，\\(G\\) 会累加很多；缺点： 自动放缓的自适应机制是双刃剑，一旦更新到局部最小值和鞍点，就很容易陷进去，且比 SGD 更难从中拔出。RMSProp 是对 AdaGrad 的推广，为 \\(G\\) 的累加引入了指数衰减机制。\\(w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{G_t}+\\epsilon} \\nabla l(w_t)\\)\\(G_{t+1} = d G_{t} + (\\nabla l(w_{t+1}))^2, G_0 = 0\\)此算法的有一个超参数：衰减速率为 \\(d \\in [0,1]\\)，\\(d=0\\) 时为 SGD，\\(d=1\\)时为 AdaGrad。优点： 当陷入局部最小值和鞍点时，经过足够长的时间\\(G\\) 会逐渐下降，学习率重新变大，更容易拔出。缺点： 衰减机制也会导致 \\(G\\) 的累加被拖慢，一直不上去，减慢学习速度。Adam以上两类优化算法是对 SGD 的改进：Momentum 系列方法可看作改进了梯度，RMSProp 系列可看作改进了学习率，这两个改进互不冲突，Adam 提出于 ICLR 2015，结合了以上二者，并作了一些其他的修正：\\(w_{t+1} = w_t - \\frac{\\eta}{\\sqrt{\\tilde{G}_t}+\\epsilon} \\tilde{v}_t\\)\\(v_{t+1} = \\rho v_t + (1-\\rho) \\nabla l(w_t), v_0 = 0\\)\\(G_{t+1} = d G_{t} + (1-d)(\\nabla l(w_{t+1}))^2, G_0 = 0\\)\\(\\tilde{v}_{t+1} = \\frac{v_{t+1}}{1 - \\rho^t}, \\tilde{G}_{t+1} = \\frac{G_{t+1}}{1 - d^t}\\)原 Momentum 中间变量 \\(v\\) 称为第一动量，RMSProp 部分累加的 \\(G\\) 称为第二动量。做的修正见最后一行公式，能防止一开始学习率过大导致的不稳定，原理详见论文。另外一个小区别是 \\(v,G\\) 的累加公式多了 \\(1-\\rho, 1-d\\) 的系数，本质上区别不大。Adam 合并了 Momentum 和 RMSProp 系列良好的性质。下图是各优化器效果对比图，来自 CS231n 课程：二阶优化器深度学习的优化算法主要是基于梯度的，即一阶算法，都需要经过反向传播计算每个参数的梯度，根据梯度方向和大小作参数更新。以上全部为一阶算法。二阶算法要用到函数的二阶信息，即 Hessian 矩阵。深度学习由于参数 \\(N\\) 很多，矩阵规模 \\(O(N^2)\\) 很大，计算代价更是高（要用矩阵求逆，复杂度为 \\(O(N^3)\\)），一般无法应用到深度学习中，牛顿法是一个例子。少数降低计算代价的二阶算法可以应用到深度学习中，如 L-BFGS（Limited memory BFGS），它将牛顿法中求矩阵逆改为求近似的 BGFS 算法（属于拟牛顿法），同时也不一次求整个矩阵逆的近似（即 limited memory），极大降低了计算代价。原理不再介绍。六、损失函数这里重点考虑的不是加在模型输出与真实值之间的那个原始损失函数的选取。一般来说，分类问题用交叉熵损失，回归问题用平方损失就够了。本节讨论加在损失函数后面的正则项。很多科研工作就是设计了针对某场景的正则项，对该场景起到了特定的作用，我们也不讨论。只讨论一些可当作通用的深度学习 trick 的正则化方法，对于任何模型都可以施加的 trick。本章统一记号如下，其中 \\(\\lambda &amp;gt; 0\\) 为正则化超参数，用于调整正则化的强度；\\(\\theta\\) 为模型参数。\\[L(\\theta) = \\frac1N \\sum_{i=1}^N L_i (f(\\mathbf{x}_i; \\theta), y_i) + \\lambda R(\\theta)\\]以下讨论的所有正则化方法都是为了缓解过拟合，详见本文最后一章。L2 正则化又称权重衰减（weight decay）、岭回归。\\[R(\\theta) = \\frac12 \\sum_{w} w^2\\]注意，通常只对权重、不对 bias 作正则化。 可以证明，在损失函数中加入 L2 正则项等价于在更新参数的梯度下降公式中加入下式最后一项（其中 \\(\\eta\\) 为学习率，\\(\\lambda\\) 为 L2 正则化系数）：\\(w := w - \\eta g - \\eta \\lambda w\\)这一项可以与前面的 \\(w\\) 合并：\\((1-\\eta\\lambda) w\\)，形式上看就是让权重衰减了一下再梯度下降，这也是 L2 正则化被称为权重衰减的原因。L1 正则化又称 Lasso（套索）算法，它将对权重 2 范数的约束换为了 1 范数。\\[R(\\theta) = \\sum_{w} \\mid w\\mid\\]它也限制了所有权重的大小，但使用 1 范数倾向于让一些权重直接置 0（稀疏化），起到了特征选择的作用。还有一些更花哨的 L1 正则化，如 Adaptive Lasso, Group Lasso，可参考专门讲 Lasso 的《Statistics for High-Dimensional Data: Methods Theory and Applications》一书。L1 和 L2 相加的正则化称为弹性网络（Elastic Net），其中 \\(\\beta\\) 为平衡超参数。\\[R(\\theta) = \\sum_{w} (\\beta w^2 + \\mid w\\mid)\\]还有平滑（smooth）L1 损失，像是 L1 和 L2 的分段组合。\\[R(\\theta) = sum_{w} sl(w), sl(w) = \\frac12 w^2, \\mid w \\mid &amp;lt;1, \\mid w\\mid-\\frac, \\mid w\\mid&amp;gt;1\\]七、超参数优化深度学习有很多超参数（hyperparameters）： 网络参数：神经元数量，卷积核大小，卷积层步长与填充值等； 优化参数：学习率，batch_size，优化器的其他参数； 正则化超参数：正则项系数、Dropout 的丢弃概率；网络参数关系到模型大小，通常是最初设定好的。batch_size 也是根据计算资源能开大则开大。除此之外，最重要的超参数是学习率。我们需要选择合适的超参数，使模型在这组超参数下训练效果最好，此过程称为超参数优化。超参数优化遵循交叉验证（cross-validation）的科学实验原则：参考的测试集不应为真正的测试集，而是验证集（validation set），它通常是从训练集中分离出的一部分。训练效果主要参考 loss 和准确率等指标随迭代的曲线，所谓训练效果好有以下几个判断标准： loss 有比较显著、稳定的下降；或者看参数更新幅度的比值是否过大或过小； 收敛时 loss 足够小，指标足够好； 训练集和验证集上指标差距不大（否则为过拟合）。超参数优化的通常策略是由粗到细（coarse to fine）： 先手动地超参数优化用于粗选范围，即用肉眼观察曲线，监视训练过程，边看边调参； 再由自动的算法在该范围内精细搜索，算法规定了 “好坏” 判断机制和搜索下一组待试验的超参数值的方法。下面分别介绍它们。手动粗选范围手动选择不必多作介绍，这就是传说中的深度学习 “调参”，需要人有很多训练深度学习模型的经验，从业者有时也被戏称为 “调参师”。手动选择最简单的做法是每选一组超参数就从头开始跑一下。一个更省时间的做法是在同一个训练过程中使用各种超参数（例如每个 epoch 都换一组超参数），仅观察 1 个 epoch 下 loss 的下降情况，但这样也要承担不准的风险。手动选择只需寻找各超参数的大体的合适范围，剩下精细搜索人很难胜任，可以交给自动的搜索算法完成。参数搜索算法遍历所有的超参数组合计算代价是巨大的，通常使用搜索算法，重点放在如何下一组待试验的超参数值上。常用的有： 网格搜索（grid search）：将超参数范围按一定间隔划分，只考虑网格上的点。搜索策略有很多，如深度优先、广度优先以及一些启发式算法等（参考算法课程）； 随机搜索（random search）：每个超参数指定其范围内的一个分布，每次从分布中抽样。据说比网格搜索更高效、精细。Random Search for Hyper-Parameter Optimization过拟合问题总论本章将统一地详细讨论模型的过拟合问题，这是深度学习训练的一个重要议题。欠拟合（Underfitting）是模型没有充分探求训练集的规律，导致训练集和测试集效果都差。过拟合（Overfitting）是过分探求了训练集上的规律，导致虽然训练集效果好，但测试集效果差，模型泛化能力差。这两种情况都不是训练想得到的。过拟合、欠拟合的判断首先给出在训练过程中判断模型是否过拟合或欠拟合。对于一个给定的模型，判断过拟合与否主要是通过学习曲线（learning curve），见下图，此图的横坐标是训练的轮数。下图是正常状态下画出的。还有两种不正常状态：我称为总是欠拟合、总是过拟合。 请注意，这里的 “测试集 loss” 指验证集。测试数据是只能用于最终的测试，不可以辅助模型训练。以上分别对应三种情况： 总是欠拟合：两个 loss 都非常大，降不下去； 正常：两个 loss 都能充分下降，呈现出前面学习曲线的模样，有一个临界点：之前的是欠拟合，之后就慢慢变成过拟合了； 总是过拟合：训练 loss 能充分下降，但测试 loss 总是较大；而且之后训练很多 epoch 也不见得拉低二者的差距。解决过拟合、欠拟合在训练时判断出欠拟合与过拟合后，就该解决了。如果学习曲线出现以上“总是欠拟合”、“总是过拟合”，就将学习曲线变“正常”。欠拟合和过拟合与模型复杂度、训练数据量有关。它们的关系如下图：模型复杂度越高，越容易“总是过拟合”。反之，模型复杂度太低，则容易“总是欠拟合”。这里的模型复杂度是相对于训练数据量而言的，即越复杂的模型需要越大的数据量，若不匹配则会出现这两种情况。解决“总是欠拟合”，只需提高模型复杂度，设计更复杂的模型即可，例如增加神经元等方法。这一点很容易做到，欠拟合的情况是容易处理的，因此也无需专门防止欠拟合的方法。 通过减少训练数据量不能解决欠拟合，一个“偷懒”的模型效果不可能好！而且这也是很蠢的行为，有数据为何不用呢？只需稍一动手把模型搞复杂点即可。解决“总是过拟合”，一个思路是收集更多的数据，提高数据量，但实际很难做到，训练数据量一般是固定的。另一思路就是降低模型复杂度，当然可以砍掉一部分神经元，但通常我们不想把辛辛苦苦设计的模型直接砍掉，这样太生硬，就想出一些花招，包括： 正则化（weight decay 等）； Dropout； Batch Normalization； 数据增强； 集成学习； 预训练 + 微调； 用“偏差-方差”理论来解释，这张图有两个维度：偏差（bias）和方差（variance），理想状态是低偏差和低方差。数据量越大，方差越小；模型越复杂，偏差越小。“总是欠拟合”对应高偏差，“总是过拟合”对应高方差和低偏差。 学习曲线“正常”化之后，还是需要找到临界时刻。临界时刻即测试 loss 开始升高时，继续训练会导致过拟合，令训练停在临界时刻的策略称为早停（early stopping）。 https://brilliant.org/wiki/backpropagation/ &amp;#8617; " }, { "title": "配置 Python 环境备忘", "url": "/posts/Python_configuration_memo/", "categories": "科研", "tags": "日常管理, 技术", "date": "2021-04-19 00:00:00 +0800", "snippet": "众所周知，Python 环境配置是一个大坑，很多人就莫名其妙地在电脑里装了大大小小十几个 Python 环境也不知道。看看这张梗图就明白了。我也经历过这样的事情。本文就简要记录一下自己的配置历史，并留一份当前的环境状态作为备忘。一、个人电脑（Mac 系统）吐槽刚买回手里的这台 MacBook 后就迫不及待去官网把 Python 装了，用 pip 装了几个包一跑程序没问题，感觉美滋滋，就不再管了。后来跑深度学习代码开始用上了 Conda，那段时间是真切地感受到坑了。安装完 Miniconda 后，在 VSCode 里发现了好几个 Python 解释器，便参考网上的教程一点点地卸载，没有记错的话，当时只保留系统了自带的 Python 2.7 以及 Miniconda。过了段时间突然一看，发现系统自带的 Python 2.7 找不到了，吓我一跳，查了下发现是 Mac 系统更新到 Monterey 12.3 版本将自带的 Python 2 删除了。那么是更新成 Python 3 了，还是直接就删除了呢？网上各有各的说法。反正我的电脑上出现了一个让我疑惑的 Python 3.8.9，在 /usr/bin/python3/，它特别神奇，在系统里都找不到 Python.framework 框架，它还能正常运行，而且卸载也卸载不掉。看了这篇博客，我吓尿了，还是留着吧，放在那里不用就好。在我自己的电脑上，我还对一些扩展功能有刚需，如 Jupyter Notebook，IPython 等。它们本质上是 Python 的包，不需要单独安装，只要 pip intsall 就可以用了。Python 环境备忘 一个疑似系统自带的 Python 3.8.9，位于 /usr/bin/，勿使用，当祖宗供着； Miniconda3，位于 ~/Miniconda3/，供平时使用； 环境变量中 python3 包括 Miniconda 中的 Python 3 与那个 Python 3.8.9（前者优先），python 是 Miniconda 中的 Python 2。无论敲 python3 还是 python，进入的都是 Miniconda 里的 Python 环境； 环境变量中的 pip, pip3 也是一样。无论敲 pip3 还是 pip，进入的都是 Miniconda 里的 Pip 环境。Conda 环境备忘Mac 的定位是只做学习机或者跑一些简单的程序，不跑大型项目（例如 Mac 没有 Nvidia 显卡，装不了 CUDA，无法跑大型深度学习项目）。 base：当作基本环境，只作临时使用，只在此环境中安装必要的通用的包，如 Jupyter Notebook，IPython，在创建新环境时都统一复制一份此环境； dl_study：学习、测试深度学习代码用，在 base 的基础上安装深度学习的包； spyder：爬虫程序的环境，在 base 的基础上安装爬虫相关包，如 requests, BeautifulSoup； 对于要发布的项目环境，从头开始创建一个即可（不需要复制 base）。二、个人电脑（Windows 系统）我的 Windows 游戏本是备用机，装有 GTX 960 显卡，可以跑深度学习项目。但我不把此当作主力机器，它只是偶尔测试一下代码用，主要还是用 Mac 远程连接服务器。Conda 环境备忘本电脑只装一个 Miniconda 即可。同样地，base 环境中安装必要的通用的包，如 Jupyter Notebook，IPython，在创建新环境时都统一复制一份此环境。base 以外的其他环境针对项目作临时用，不长期使用。三、服务器（Linux 系统）服务器只跑大型项目，和 Windows 游戏本一样。Conda 需要自行安装，无法共用其他账户的。一般用 Linux 命令行的命令来安装。Conda 环境备忘只装一个 Miniconda 即可。同样地，base 环境中安装必要的通用的包，如 IPython，在创建新环境时都统一复制一份此环境（Jupyter Notebook 没必要装）。base 以外的其他环境针对项目。目前我有两台服务器可用（组内、学院），都把 base 环境配置好，再根据项目需要安装项目环境。" }, { "title": "深度学习科研方法论（非知识或技术）", "url": "/posts/non-technical_knowledge_in_deep_learning_research/", "categories": "科研", "tags": "", "date": "2021-04-09 00:00:00 +0800", "snippet": "本文总结一下深度学习科研里的一些常识，指的是非具体知识或技术的方法论性质的东西，涉及科研流程、论文发表等大方面。我重点介绍深度学习科研特有的，当然也有一些是所有专业科研都会涉及的。（感觉我整理癖有点强啊，连这都要整理……不过也好了，闲着没事整理一下还是对自己有好处，虽然自己还没有什么科研成果 Q_Q）希望这篇总结可以帮助到即将进入深度学习这行科研的朋友，或者纯粹对我们这行科研好奇的朋友。所有学科的学术科研都离不开论文。本文以论文为主要对象，顺着科研工作的一整套流程来总结： 第一章介绍论文发表在哪儿，这是首先应该了解的； 第二章介绍找论文的方法，自己的科研问题都是通过阅读前沿论文产生的； 第三章介绍实际的科研流程，在深度学习领域就是提出算法并做实验。这些都作为一篇论文的主体部分； 第四章介绍完成一个工作后，如何写论文。这里会介绍深度学习论文通常的格式与内容； 第五章介绍如何发论文，涉及论文投稿和审稿的常识，以及后续事项如参加会议等；最后一章会说一些我了解的深度学习圈内的八卦，大家就看个乐子好了。深度学习这一行讲求分享精神，免费、共享、开源的东西比其他理工科要多（据我了解，很多其他专业的论文都要从学校买下的数据库里找）。希望在看完后大家能对此有所体会。一、论文发表在哪儿论文是科研人员总结科研成果、与他人交流的主要载体，也是衡量学术成就的重要指标。论文和报纸、杂志里面的文章性质一样，都是投稿到一个出版物中，由审稿人（一般是领域内的专家）审核，通过后在出版物中出版。有的出版物在圈里是公认的、口碑好、论文质量高，有的则不被人所知、论文也比较水。投稿前者难，代表着科研水平高；后者容易，水平也低。所以每个专业的圈子都有公认的出版物“好坏”标准，论文发表的出版物“好坏”就是衡量其科研水平的一个指标。每个专业的论文发表都有自己的特点。深度学习目前的论文主要发表在各种会议、期刊中。会议（conference）是指学术会议，顾名思义就是研究者讨论学术成果的会议，在某个地方集中开好几天的那种，当然这种大型会议可不是组里平时的讨论班，随便交流一下想法思路的，而是需要有会议认可的学术成果才能参加，即在开会之前的某个时间向其提交论文，通过后才有参加资格，组织者也会把通过的论文整理出版出来。所以，会议可以看成一种出版物。期刊（journal）就是专门刊载某个领域学术内容的杂志，和普通的杂志道理是一样的，不多解释。会议和期刊的区别： 期刊对论文的字数、工作的完善程度要求更高（投了会议的论文可以继续完善再去投期刊）； 期刊的审稿慢，从投稿到出版的周期长； 期刊论文数量少，会议论文数量巨大，每个会议每年一般会有上千篇论文； 期刊不需要开会； 深度学习论文喜欢投会议（这是历史原因慢慢形成的环境。因为深度学习这行变化太迅速，而会议发表比较快）；深度学习的领域有很多，会议、期刊也有太多太多。衡量“好坏”的标准有： CCF（中国计算机学会）推荐的国际学术会议与期刊目录（请看人工智能、数据挖掘等相关的部分）把会议和期刊分为 A、B、C 三个等级，A 代表“好”，C 代表“差”。国内顶尖高校的计算机专业院系认可这个评价体系，并将其作为研究生毕业或老师评职称的参考标准； 被哪个数据库（存放论文的数据库）收录也是一种评价标准，它平行于 CCF 的评价体系，是理工科科研通用的标准。常见数据库有 SCI（科学引文索引）、EI（工程索引）、核心期刊等，SCI 代表着最好、最权威（注意只收录期刊论文），EI 次之（期刊和会议都有收录），核心期刊是国内大学或院所定的，只收录中文期刊。由于很多不收录会议论文，所以计算机专业一般不按照这个标准来，但也有方向比较多的院系将其作为参考标准（例如我在的数学学院）。它们内部也有自己的细分等级： SCI 为其收录的论文发明了一个评价指标：影响因子（是用引用频率算出来的），越高越“好”，根据影响因子的排名将论文分成四个区（一区、二区、三区、四区），一区的影响因子最高； EI 将其论文分成两个档次：核心与非核心； 制定核心期刊标准的机构有很多，对理工科，认可度最高的是北大核心期刊（由北大图书馆制定），其次是中科院的中国科学引文数据库、中国科技核心期刊等。 以上评价最“好”的一批会议或期刊通常被称为顶会、顶刊。深度学习赫赫有名的顶会有 CVPR、NIPS、AAAI、IJCAI、ICML、ICLR、ICCV、ECCV、ACL、EMNLP 等，顶刊有 IEEE TPAMI、JMLR 等。此外，深度学习领域的中文期刊或会议比较水，比较受认可的期刊也就计算机学报、软件学报、自动化学报这几个，大部分人也不会考虑中文论文，这一点可能与很多文科专业不太一样。有的会议有自己细分的评价标准，以下是几个例子： 分 workshop 和非 workshop：workshop 意思是小型的分享会，因此发论文在会议 workshop 分会的难度比正式会议小很多，含金量也低； 分 oral、spotlights、posters：含金量依次降低，分别对应开会时的口头报告、小的口头报告、海报演示（见第五章），自然可见其重要程度。预印本深度学习这一行在正式发表论文前，通常习惯把论文草稿先上传到网上的预印本平台。这样的目的是： 分享给大家阅读并提出更改建议； 自己提出的想法先占坑，防止晚一步被别人抢发论文（预印本发出来之后）；深度学习领域都是使用 ArXiv 这个预印本平台。找论文时要看是不是预印本；如果是的话，再看是作者还没有正式发表，还是已经正式发表了，预印本只是之前的版本。正式发表的文章一般比预印本完整、正式，比如有完整的附录、第一页脚注处也有很多论文发表信息。预印本毕竟是草稿。二、找论文：寻找科研问题科研获得最新知识的来源一般都是最新发表的论文，深度学习的大部分会议和期刊都是年度的，包含了一年来各路研究者的最新成果，通常在会议或期刊名后面加上年份，如 CVPR 2021。论文在网上一般都可以检索到，以下介绍在网上找论文的方法： 找某一篇特定的论文或包含某关键词的论文：去 Google Scholar 或 Google（不建议使用百度）上搜题目或关键词。它们能把世界上几乎所有的论文搜出来（包括预印本 ArXiv 上的），并且给出其引用量、信息、下载链接（如果不付费的话）。中文论文去中国知网、万方数据等国内网站搜更好。如上所述，SCI、EI 的数据库 Web of Science、Engineering Village一般是不用的； 如果只是想涉猎最新的论文，可以关注感兴趣的会议出版的论文集和期刊的网站，每年都有固定的时间发布论文集（proceedings，记住这个单词），可以 Google 搜索“会议名 + 年份 + paper list 或 proceedings”找到论文集网站。当然，论文很多不可能这样遍历，也可以在论文集网站上使用关键词等检索。以下列一些有名的网站，它们可以搜到更具体会议的论文： Open Review：收录了大量会议的开放评审（关于开放评审，请见）的论文，包括完整的 ICLR 会议论文，以及很多如 NIPS 等会议的论文； CVF open access：由计算机视觉基金会（CVF）资助的数据库，收录视觉相关的三个会议 CVPR、ICCV、WACV 的论文； PMLR：Proceedings of Machine Learning Research，主要收录 ICML、AISTATS、COLT 等会议的论文； 很多会议/期刊会在自己的官方网站中公布论文集（注意不是 Accepted Paper List，而是 Proceeding)：NIPS、AAAI、ICML、JMLR、IEEE TPAMI；还有的会委托其他出版社出版：KSEM； 想随意地探索深度学习最新的进展，可以浏览一些第三方的博客、公众号或社区，会有最新的论文列表推荐，如 Paper With Code、CSDN、微信公众号 PaperWeekly 等； 对一个问题或领域深入研究，入门的话建议找综述论文，一般可以检索“领域 + survey”找到；可以关注一些更具体的博客； 对于具体的工作，应以一篇论文为锚点，找其引用的文章（如用了谁的方法、“相关工作”部分）来看。 中国知网、万方数据等虽然是论文数据库，但它们更像搜索引擎，而不能像 SCI、EI、核心期刊一样当做评价指标。三、准备论文：提出算法与实验深度学习的研究问题大部分是完成某个任务，科研就是为该任务提出模型或算法，并进行代码实验验证。深度学习的任务如图像分类、机器翻译等，有太多太多，这里不详细讨论，具体技术细节可以参考我的这篇笔记。对每种任务，会有前人提出他们的模型或算法（称为 benchmark），发表了论文并获得认可，我们的目标是提出自己的，在效果上超过他们，如果比别人的好，就可以发论文了。（如果超过了该任务上最好的模型或算法（称为 SOTA，state of the art 的缩写），那发出来的论文水平也就相当高了）为了证明自己的效果更好，深度学习领域都是在数据集（dataset）上，用自己和别人的模型或算法做代码实验（此时被比较的别人的模型称为 baseline），比较验证结果（通常用定义好的评价指标（metrics））。为了公平，每种任务都有大家公认的数据集，大家的实验也都在这些数据集上做。（这里额外为完全不懂计算机的朋友解释一下，这里的模型或算法并不是一个摸得到的物体，而是我们敲到电脑里的代码对应的程序，做的实验都是敲敲键盘在电脑上完成的，而没有像生物、化学那样真正的实验室。数据集可以理解为实验对象如小白鼠，输入到程序中，实验结果就是输出的评价指标）深度学习由于常常使用大型数据集，所以实验周期也不是想象地那么短。真正深度学习科研的程序不是几秒、几分钟就出结果的，往往需要几天、几周，甚至几个月。这里就涉及到算力这个话题，做代码实验使用的是计算机的计算性能，算力就是指一个研究组计算机设备（常称为计算资源）的计算性能。有的组经费充足，算力强大，实验周期短，就可以加快科研进度，更快地发论文；有的组计算资源少，做实验就慢。深度学习主要使用显卡这个计算机部件，因此圈内大家讨论、吹牛的也是谁的显卡厉害。在这个阶段，除了看别人的论文内容外，网上很多还有代码资源可以利用： 深度学习领域的研究者通常会把代码完整地发布到网上（称为开源），一般就放在 GitHub 上（最大的代码托管与分享网站）； 一些网站将深度学习各任务的数据集、benchmark 及相应的代码分类汇总起来，典型的是 Paper With Code，非常实用。四、写论文：论文的格式与内容上一章看到了深度学习科研的流程，非常固定，所以深度学习的论文，用我们老师的话说，和八股文差不多，意思是遵循固定的写作格式，比较死板，所以写起来、读起来都是有章法的。下面就按组织顺序讨论这种论文的格式与内容。开头和结尾是非主体部分，和其他专业基本一样： 题目：深度学习比较卷，论文题目经常比较花里胡哨； 作者：按照贡献程度分为第一作者、第二作者、第三作者…，（允许出现共同一作），比较特殊的是通讯作者，一般是作者的导师或指导论文的专家，作为负责人。通讯作者可以是第一作者（一般不能是第二作者等），也可以没有。 摘要：与科研流程一致； 引言； 相关工作； 最后一章结论； 参考文献：有很多，但并不是所有都和文章有重要的关联。引言等部分中的参考文献一般关联不大，重点是方法部分、相关工作中的，才是直接继承的工作。主体部分有深度学习自己的特色： 问题定义：会给出深度学习任务的数学定义，目的是讲清楚问题，并为下文统一数学记号。对于一些大家都知道的任务，这一部分往往省略； 模型与算法部分：给出作者提出具体的模型或算法的描述，通常会画一张图来示意，有时候会在里面做一些简单的数学证明，可以合在一章，也有的拆成很多章； 实验部分： 第一小节一般是实验设置：介绍使用的数据集、baseline、训练细节（计算资源、超参数等）；（注：上文说过 baseline 就是被比较的别人已有的模型或算法，用来证明自己的有效果，类似于其他理工科实验的对照组） 第二小节一般是主要结果的呈现与分析。结果通常呈现在一张表中，一维是各种模型，另一维是各种评价指标。通常会把每个指标效果最好的数加粗；该节正文中会写一些对该结果的分析； 后面的小节是其他次要的实验，可能会发现一些有趣的事情； 最后一节一般是消融实验（ablation study）。对于模型中有多个独立的机制或模块的，第二小节合起来的结果不足以说明每个机制或模块都是有效的（例如：有一个模块是很有效的，但另一个模块没有用甚至拖后腿，但合起来有效，这样会误导人以为两个模块都有效果），需要单独验证每个模块的效果，就是消融实验（也和对照试验原理相似）； 附录：包含正文里放不开、次要的长图、长段证明、长结果等等。 由于这种固定的格式，看深度学习论文不需要从头一字不落看到尾，需要找到有助于自己科研需要的论文的重点。关于如何看论文、速读论文这种心得性质的东西就不在这里聊了。五、发论文：投稿、审稿、开会我们在发论文时并不只有某一个会议/期刊。很多会议/期刊的领域都有重叠，所以可以选择的有很多。这些年度的会议/期刊散布在一年的各个时间，一般人的做法是盯着这些时间节点来合理规划自己的科研工作节奏，在恰当的时间投稿恰当的会议/期刊。有很多网站会定期整理和更新最新的会议/期刊时间节点，很方便，这里列几个：CCF推荐会议截止时间、WikiCFP。以下是发会议论文的流程： 作者投稿； 审稿人审稿（一般在投稿结束后1-2月出结果）； 如果没有中，那就准备投其他的；如果中了，就需要去参加会议。看起来很简单，但具体事务是比较繁琐的。下面的几节不会涉及具体事务，但会详细讲解必须知道的常识。投稿发论文需要遵守相应会议/期刊的规则，通常会在官网上通常会有 CFP（Call For Papers） 版块发布，值得关注的信息包括： 各种截止时间：注册时间（提交摘要）、提交时间（提交论文）、其他材料提交时间（一般晚提交时间 7 天）； 论文要求：长度要求（页数），格式要求（一般都会提供 LaTeX 模板）； 审稿方式（见下节），这关系到是否需要给出作者列表（双盲审不能给）； … 有些会议为了组织方便要求作者在提交时间前注册稿件并提交一份论文的摘要，这就是上面的注册时间。审稿会议的审稿很像考试，但其结果只有两个：录用/不录用。审稿是由一批审稿人完成的，每篇文章一般会分配几个审稿人。审稿方式有以下几种： 单盲审：审稿人知道作者的身份，但是作者不知道审稿人的身份； 双盲审：审稿人不知道作者的身份，作者也不知道审稿人的身份； 开放式评审（Open Review）：审稿人知道作者的身份，作者也知道审稿人的身份。ICLR除了 ICLR 以外，一些会议也强制规定开放式同行评审，例如 CVPR。深度学习论文的开放式评审结果一般都可以在 Open Review 网站 中查到。审稿结果：大部分审稿只有一轮审稿。深度学习领域的审稿结果一般有 5 个分数等级： Strong Accept； Weak Accept； Borderline； Weak Reject； Strong Reject；每位审稿人打分，并写下审稿意见。最后根据各审稿人的打分，由上一级的组织者决定是否录用。（注意，录用之后过一段时间才会出版，录用和出版不是一回事。）有两轮审稿的，中间多了给作者申辩（rebuttal），类似于上诉，一般只有 7 天时间。第二轮审稿时审稿人会结合 rebuttal 重新评判分数。开会会议一般要求每篇论文必须有一个作者要参加（线上会议除外），费用一般可以由所在学校或机构报销。（待更新）发期刊论文的流程（简单介绍，待更新）圈内八卦深度学习圈的八卦信息汇总于此。（待更新）大牛这里列举深度学习领域大牛，简单介绍其工作、贡献。（待更新） David Marr： 深度学习三巨头 Yoshua Bengio： Geoffrey Hinton： Yann LeCun： Ian Goodfellow： 何恺明：" }, { "title": "Linux 学习笔记：从科研使用需求出发", "url": "/posts/studynotes_Linux/", "categories": "科研", "tags": "学习笔记, 技术", "date": "2021-02-02 00:00:00 +0800", "snippet": "经典外行人疑惑下面这些问题大概是外行人的经典疑惑，也是我在接触深度学习领域前长期悬而未决的问题。但很多事情用下面这几个 QA 就说清楚了。 Q：为什么要学习使用 Linux 系统？ A：因为要用研究组里的服务器，服务器一般是 Linux 系统。 Q：服务器是什么？ A：就是一台配置高得很的电脑，它干起活来效率很高。 Q：你用服务器做什么？ A：做深度学习领域的大型实验。 Q：自己电脑不行吗，为什么要用服务器？ A：众所周知，深度学习实验主要使用部署在服务器上的 GPU 计算资源，个人电脑跑不动。 Q：Linux 系统不是和 Windows、Mac 一回事，都是操作系统吗，怎么还要单独学习？ A：服务器上的 Linux 主要是命令行控制的，而不是图形界面。这可不是普通的点点鼠标的人性化电脑。所以，学习 Linux 只要满足科研使用需求即可，了解这个操作系统的基本逻辑，学一些必要的命令。没必要漫无目的地抱着一本厚厚的技术书从头到尾啃下来。国内比较知名的书可能是《鸟哥的 Linux 私房菜》，但这本书着实和看小说一样，废话太多太磨叽。我选择参考的是 The Linux Documentation Project 上的 《Introduction to Linux》，比较精要也比较权威。我的科研使用需求，就是用服务器做深度学习的实验，对于 Linux 系统，只需学会这些即可： 逻辑层面：基本概念，多用户系统的逻辑、命令系统的逻辑、文件系统逻辑； 操作层面：平时常用的服务器终端命令。一、Linux 系统的逻辑基本概念Linux 的那些能讲出一本书的历史实在是没必要看，只需要搞明白发行版这个概念。首先要适应 Linux 是“开源”的这件事，与 Windows、Mac 都不一样，它们完全受自家公司控制，也就只有一套版本号例如 WinXP -&amp;gt; Win7 -&amp;gt; Win10，Mac 的 Catalina -&amp;gt; Big Sur -&amp;gt; Monterey。Linux 系统的核心从诞生之日起就是开源的（不属于某个个人或公司，任何人都可以贡献并使用），于是就有各路人套壳包装变成一套有头有脸的操作系统。前者那个由 Linux 社区维护更新的核心就叫 Linux 核心（kernel），后者由其他公司或社区包装好的叫发行版（Distribution）。核心顾名思义就是操作系统最必要的功能，发行版除核心外还包括一些重要软件、工具、系统的安装包等，让这个核心更能用。我们当然无法直接使用核心，用的都是某个发行版。所以给电脑装 Linux 就是找一个喜欢的发行版，下载好系统镜像，和平时装系统一样一步步安装即可： Ubuntu Red Hat Debian CentOS自然地，一个 Linux 系统有两套版本号：一套是 Linux 核心的版本号（在此查看），一套是发行版的版本号（在各发行版的官网查看）。账户系统Linux 系统的重要特性是多账户系统，主要是因为使用 Linux 的服务器通常是供多人使用的，必须设计多账户机制以防使用混乱（实际上 Windows、Mac 也有，但很少使用）。Linux 账户分为： 管理员（root）账户：负责管理机器的，有很多更高的关于机器的权限 普通账户：普通使用者的账户，人很多时可以划到不同群组进行更细致的管理以我们研究组的服务器为例，管理员目前是大师兄，负责管理维修这台服务器；普通用户就是其他研究生，每个人都有一个号，是向管理员申请的普通账户。登录服务器账户并操作的方式一般为远程的，即在另一台电脑上进入服务器的命令行。北大的情况是，只要连接校园网，即可以通过 SSH 登录在校园网环境中的服务器。由于服务器的使用性质，机器都是常年开着机的，一般都是以登入登出的方式使用的，开关机会影响他人使用。因此只有管理员开关机的权限。命令系统除了图形界面（点鼠标的），操作系统都可以通过在命令行输入命令控制电脑。这个命令行本身也是一个程序，叫做壳（Shell）：Windows 为 cmd（或更高级的 PowerShell），Linux 为 Bash（ Mac 使用了更高级的 Zsh，兼容 Bash）。壳总是会生成提示字符串，示意在其后输入指令，它包含当前账户、文件路径等信息，并以一个提示符与输入的命令分隔：cmd 为 &amp;gt;，Bash 为 $，Zsh 为 %。提示字符串格式为（以 Bash 为例）：账户名 @ 机器名 : 当前路径 $（提示符）一个指令的格式为：指令名和若干个选项及其参数（各部分以空格隔开）。选项是本指令的细化设置，格式为 -单个字母 ，或 –单词（前者是后者的缩写）。选项通常包含一些参数需要填写，就放在选项的后面。指令有两种类型，一种是像上面这样敲完指令与选项、参数立即执行，执行后回到命令行提示符；另一种是敲完指令名回车，进入到某个环境，例如命令行里的文本编辑器、python 等。除了系统自带的指令，可执行文件也可以作为指令执行：进入此文件的目录，输入文件名敲回车即可；或者直接打出文件的路径敲回车。 注意：执行文件与打开/编辑文件不是一个概念。执行必须是可执行文件。在 Linux 系统下，文件是否可执行是以文件权限区分的，可以修改。除了在命令行敲命令外，还可以像写程序一样组合一系列指令为脚本，运行脚本文件等效于在命令行依次敲击里面的命令。脚本文件是文本文件（通常以 sh 扩展名区分），需要将其权限设为可执行。（因此，Shell 下执行文本文件都视其为自己的脚本，不管其扩展名是什么） 壳作为一个程序，也有自己的配置，如环境变量。这些配置信息通常放在一个配置文件中，常见的如 .bash_profile，.bashrc，.zshrc 等。文件系统操作系统的文件系统基本没什么区别，了解一些 Linux 特有的东西即可：一是路径格式，根目录为 “/”，绝对路径即从根目录向下，也是用 / 分开的，最后一个 / 可带可不带；相对路径的记号总结如下： ./ ：表示当前目录 ../ ：表示上一级目录 -/ ： 表示上次的目录 ~/ ：表示当前用户（提示字符串里写了）目录，用户目录见下文 ~用户名/ ：表示此用户名的用户目录二是文件目录的组织，都有哪些文件夹，里面都放什么文件。 Linux 不涉及磁盘分区，根目录里就包含了所有文件； 用户目录就是一个账户自己的目录，有了一个账户后，当然要有自己的一块小空间！所有用户的用户目录都在根目录中的 /home ，用户目录的名称就是账户名； 根目录中的 /root 是管理员的用户目录。但管理员日常使用不会用这个身份，使用这个目录，一般会用一个普通账户； 根目录中的其他目录一般都是系统或全局的文件，普通用户不必也没有权限管理它们，就别管了。三是文件权限，这是多账户避免不了讨论的问题。Linux 所有文件和文件夹都有权限，形式化为长度10的字符串： 第1位表示文件类型：只需知道 d 表示文件夹， - 表示普通文件，l 表示链接即可； 后面的位都可看作开关，- 表示权限关闭，字母表示权限打开： 第2-4位分别表示拥有者的读（r）、写（w）、执行（x）权限； 第5-7位分别表示同群组的用户的读（r）、写（w）、执行（x）权限； 第8-10位分别表示其他人的读（r）、写（w）、执行（x）权限； 应注意，Linux 不是以后缀区分可执行文件的，而是由文件权限中的执行权限（x）决定的。二、Linux 必备操作以下涉及的操作都不需要管理员权限。关于管理员要用的操作，还是等我成为组里服务器的管理员时再说吧！账户操作 操作 命令 远程登入账户 ssh 用户名@服务器地址 （在其他电脑） 退出账户 exit 查看当前在线用户 who 或 w 查看用户登录日志 last 系统级信息查看 操作 命令 任务管理器 top 浏览操作 操作 命令 使用场景 查看当前路径 pwd 想复制一个绝对路径出来 更改路径 cd + 路径名 浏览各目录，最常用操作 列出当前路径下的文件 ls ll（详细） ls -a（显示隐藏文件）   （在命令行）查看文件 cat + 文件名   （使用默认软件）打开文件/文件夹（仅 Mac） open + 文件名/路径名 MacOS 中想要转到图形化界面操作时 查找文件/文件夹 find + 文件名/路径名（支持正则表达式）   文件整体修改操作 操作 命令 创建文件 touch + 文件名 创建文件夹 mkdir + 文件夹名 移动 mv + 文件路径 + 目标路径 复制 cp + 文件路径 + 目标路径 重命名 mv + 文件名 + 新文件名 删除文件 rm + 文件路径（支持正则表达式） 删除文件夹 rm -r + 文件夹路径 跨设备复制文件 （在待移动文件设备上）scp + 文件路径 + 目标路径 注意：服务器路径前加 username@servername:   下载链接文件 wget + 文件链接 文件内部编辑操作可以通过 open 命令在外部软件中打开然后编辑，这里讨论在命令行内部编辑文件。方式一：echo 命令 echo + 字符串 &amp;gt; 文件路径：写入字符串到文件； echo + 字符串 &amp;gt;&amp;gt; 文件路径：追加字符串到文件。这种方式通常适合在空文件中写入一些简单的东西，或者在配置文件中加一条规则。方式二：使用 Linux 命令行里的文本编辑器是 Vim输入 vim + 文件路径 以进入 Vim 环境编辑此文件。具体使用方法详见我的这篇 Vim 学习笔记。如果从未接触过，可能会非常不习惯，这种方式还是只适合临时的少量的文本修改。如果嫌麻烦，还是写好了移动到服务器上吧！最好是有一个图形界面的东西辅助我们！很多 Linux 发行版是有图形界面的，为何不用呢？道理很简单，服务器那台机器就是个大箱子，连显示屏都没有，管理员连装都不想装，没必要！而且都是远程访问它的，要想远程看到系统的图形界面，岂不是还要传输图形界面的画面？想想 Windows 远程桌面有多难用就知道了！但是我这里说的图形界面不是这个！在远程控制服务器的电脑上装一些软件如 VSCode，可以借它与服务器交互信息，让它在你的电脑本地给你整理出来一个图形界面（而不是直接把服务器安装的图形界面图像传输过来）。VSCode 中安装 “Remote - SSH” 扩展后，边栏会出现一台小电脑按钮。在 SSH TARGETS 处点击加号，即远程 SSH 连接到服务器。连接成功后点击边栏的文件按钮，就可以打开服务器上的文件夹了。此时的 VSCode 就变成了一个图形界面的文件管理器，移动、复制、删除、查看、运行等文件操作均可通过鼠标点击实现，省下了在命令行里输命令的麻烦。有些机构或公司会有自己的服务器客户端（例如我们学院的公用服务器），效果类似 VSCode 的图形界面，具体怎么用参考相应的帮助。程序运行 sh 程序：bash xx.sh查看显卡：nvidia-smi三、Linux 进阶操作这些操作可能是非必需的，但能在效率上锦上添花。命令行快捷键 Tab：命令自动补全； Ctrl + C：强行结束命令 / 从程序环境中回到命令行提示符； Shift + PageUp / PageDown：翻页（这个操作无法用滚轮实现）其他尚未更新。" }, { "title": "数学课程手写笔记总结", "url": "/posts/math_classes_summary_notes/", "categories": "其他", "tags": "学习笔记, 数学", "date": "2020-08-01 00:00:00 +0800", "snippet": "本文汇总了我学习或教学数学课程时曾经做的总结性笔记，大家有需要的话可以参考。我是数学专业出身，本科阶段有复习专业课时写总结的习惯，当时是手写在草稿纸上然后扫描得到的，如有不清楚请谅解。第一个是当助教时做的复习提纲，也放进来了。目录 高等数学D：复习提纲 高等数学D：串讲教案 数学分析：曲线积分与曲面积分总结 数学分析：无穷级数总结 数学分析：函数项级数总结 概率论：大数定律与中心极限定理总结 数值计算方法：理论总结 数值计算方法：算法总结 实变函数 常微风方程：判断微分方程(零解)稳定性的方法总结 常微分方程：关于解对初值和参数连续可微的理解 偏微分方程：分离变量法过程高等数学D：复习提纲 我担任北大《高等数学D》课程助教（该课程是为文科、医科专业开设的高数课程），此复习提纲是我教授习题课过程中整理所得。使用的教材是《大学文科数学简明教程（上册）》（姚孟臣编著），对应前五章内容。高等数学D：串讲教案 我担任北大《高等数学D》课程助教时的期末串讲教案，该活动由北大数院青协组织：https://mp.weixin.qq.com/s/IkYvpJ4ULV6do7VgTFa1hQ。注意，教案中有一些很重要的错误，我懒得改了…希望你能发现。数学分析：曲线积分与曲面积分总结 使用的教材是《数学分析教程》（李忠、方丽萍编著），对应第九章内容。数学分析：无穷级数总结 使用的教材是《数学分析教程》（李忠、方丽萍编著），对应第十章内容。数学分析：函数项级数总结 使用的教材是《数学分析教程》（李忠、方丽萍编著），对应第十一章内容。概率论：大数定律与中心极限定理总结 使用的教材是《概率论教程》（李贤平），对应第五章内容。数值计算方法：理论总结 使用的教材是《数值计算方法》（）。数值计算方法：算法总结 使用的教材是《数值计算方法》（）。实变函数 使用的教材是《实变函数与泛函分析》（），对应第二章内容。常微风方程：判断微分方程(零解)稳定性的方法总结 使用的教材是《常微分方程》（）。常微分方程：关于解对初值和参数连续可微的理解 使用的教材是《常微分方程》（）。偏微分方程：分离变量法过程" }, { "title": "Conda 学习笔记", "url": "/posts/studynotes_conda/", "categories": "科研", "tags": "学习笔记, 技术, Python", "date": "2020-04-12 00:00:00 +0800", "snippet": "Conda 几乎是深度学习领域必备的工具，以后做科研一定会用到。今天就系统学习一下。Conda 官方文档：https://docs.conda.io一、Conda相关概念我觉得很多初学者分不清 Conda、Anaconda 等之间的区别，这里就先把概念整理明白吧。Conda 是一个包管理工具，最主要的功能是方便管理一个计算机里安装的不同环境。使用的场景举例： 写某代码需要尝试不同版本的 Python 或库； 某代码需要使用新版本的 Python 或库，但不想更新掉旧版本，因为自己有些别的代码是依赖旧版本的； 有的大佬身兼数职，既搞深度学习，又搞前端开发，他想独立地管理这两个不太相关的领域的库； 运行陌生的代码，想单独找个环境，用完即删除；等等。对于人工智能领域的程序员，Conda 几乎是必备的，因为大部分代码是用 Python 写的，而且代码并不是从头实现，需要用到各种不同的人、公司开发的库，且这些库版本迭代非常快，如 TensorFlow、PyTorch 等，这时包管理就显得非常重要了。Anaconda 和 Miniconda 简言之是安装 Python + Conda 环境的两种方式，即只要装了它，就相当于把 Python 和 Conda 都装好了。Miniconda 是最精简版本，几乎只有 Conda；Anaconda 更像懒人包，把大量常用的库预装好了（类似于《上古卷轴5》一些贴吧大神做的 Mod 整合包），包管理还有图形界面，运作更加商业化。一般来说，就装 Miniconda 即可，更不容易被花里胡哨的东西迷惑双眼。需要用到什么包自己手动安装，而不是用 Anaconda 懒人包里的东西，这样对代码的理解可能更深。在 Linux/Mac 系统里，Python 和 Conda 都是在终端运行的应用程序，即可以在终端敲 python conda，后面跟一系列子命令运行的。二、安装后安装过程不再叙述，基本就是“下一步、下一步、下一步、完成”。还要强调，装了 Anaconda / Miniconda 就是装了 Python，没必要单独去官网装一个 Python 了。如果安装前就有一个 Python，官方文档说了，卸不卸载随意，只要能分的清。以 Miniconda 在 Mac 系统为例，安装完毕后，所有 Python （包括解释器 python.app ）和 Conda 的文件全部默认都在用户目录 ~/Miniconda3/ 里面，管理的包也都在这里面。这个文件夹的文件是怎么组织的参见文档。安装完毕后，Conda 会创建一个默认的环境 base，Mac 系统的命令行左边会出现一个 (base) 前缀，表示当前处在此环境中。conda 和 python 都已配置到环境变量中。如何在指定的环境运行 Python 代码？ 在终端中，激活此环境（让左边的括号变成此环境，见下文），运行 py 文件； 在 IDE 如 VSCode 中，找一找，图形界面里总有地方可以选择（指定了环境的）解释器的，选择后运行即可。 请注意，Conda 中多个环境可共用一个 Python 解释器，因此运行 Python 代码需要指定环境而不是解释器。三、Conda 必备操作Conda 作为一个包管理工具，最主要的逻辑就是一个两层的关系：上一层为环境，下一层为该环境里安装的包。所有必备操作（指实现此软件核心功能必须有的操作）都是围绕这两层进行的： 对环境的操作 检索：conda info --envs 创建：conda create --name NAME python=x.x 复制：conda create --name NAME --clone XXX 切换：conda activate NAME (conda deactivate 等效于 conda activate base) 删除：conda remove --name NAME --all 对包的操作 检索：conda list 安装：conda install PACKAGE_NAME 卸载：conda remove PACKAGE_NAME 四、Conda 进阶操作这些操作可能是非必需的，但能在效率上锦上添花。 对环境的操作 复制：在 conda create 命令加选项 --clone NAME 其他全局操作 增加下载源：conda config --add channels 加 URL。在默认的境外下载源抽风时使用，推荐国内清华大学 TUNA 开源镜像。 其他待更新。" }, { "title": "Python 学习笔记", "url": "/posts/studynotes_Python/", "categories": "科研", "tags": "学习笔记, 技术, Python", "date": "2019-05-05 00:00:00 +0800", "snippet": "这是很早之前学 Python 记的一些东西，经过我重新整理发布在个人网站上。我本科正式学习的是 C 和 C++ 语言，所以学 Python 的时候有点走马观花，会用就行，不够扎实。本文我将更关注 Python 基础知识中自己不太熟悉或未理解到位的部分，关注编程语言的逻辑，完善自己的知识体系。我不打算像网上的教程那样举很多生动形象的例子，也不打算拆成一大串系列文章。此笔记是较抽象的高度总结性的文章，旨在把事情言简意赅地说明白，定义清楚，再加上一些对难点的深层次的理解，就够了；不适合当作顺序阅读的从入门到精通的教程，内容组织方式不是按照学习顺序或难易顺序，更适合当作概念手册。本笔记内容仅限原生的 Python，参考较多的资料是廖雪峰的 Python 教程。其他 Python 的库在另外的笔记中（Numpy、Pandas 学习笔记，《动手学深度学习》读书笔记系列）；也不再介绍安装、配置、界面等杂事（在 Conda 学习笔记 等笔记中会涉及）；具体代码的细节我放在自己整理的速查手册上。目录 一、杂七杂八的事情 运行 Python 程序 解释器的类型 在命令行传参 变量命名规范 二、基本数据类型与数据结构 Python 变量赋值机制 可变类型内部机制 例：Python 复制可变变量的坑 数值类型 序列类型 集合、字典 迭代器、生成器 直观概念 用法 设计迭代器 一、从头自定义 二、从内置类型构造 三、生成器 其他 Python 预定义的数据结构 三、控制流 四、函数 变量的命名空间与作用域 参数传递方式 参数列表 返回值 函数也是类！ 匿名函数 闭包 装饰器 五、面向对象编程 封装 继承与多态 super()函数 获取对象信息的内置函数与方法 六、Python 模块 模块的组织 模块的使用 模块文件模版 标准注释 文档信息 类与函数定义区 运行区 七、输入输出 input() 与 print() 函数 格式化字符串 八、遇到 bug？ 异常 常见异常 自定义异常 异常处理：try 语句 抛出异常：assert, raise 语句 上下文管理器：with 语句 调试 测试 附录：其他 Python 标准库与内置函数一、杂七杂八的事情运行 Python 程序Python 程序就是一个扩展名为 py 的文本文件，里面存放的是编程人员写的遵守 Python 语法的代码。不同的 IDE 都有一些图形界面可以运行 Python 程序，例如通常有一个“运行”按钮。无论是什么形式，所有运行 Python 程序的方式本质上都是执行了命令行中的如下命令（关于 Linux 命令的知识见我的 Linux 学习笔记）：??/??/python ??/??/prog.py前者指定了 Python 解释器，后者即要运行的 Python 程序。解释器本身就是一个程序，它可以作为命令直接执行，而要运行的 Python 程序相当于这个命令的参数。 一般 Python 安装时会加入系统的环境变量，只需要： python ??/??/prog.py 解释器的类型解释器除了安装 Python 时自带的（称为 CPython，因为是用 C 语言开发的），还有很多其他功能更高级的，如 IPython、Python、PyPy、IronPython 等。我们需要了解 IPython，因为是 Jupyter 公司开发的，常用的 Jupyter Notebook 用的就是自家的解释器。可以从提示符上区分：CPython 的提示符是 &amp;gt;&amp;gt;&amp;gt;，IPython 的提示符是 In [序号]: 。IPython 特色功能是 Tab 自动补全命令、历史记录等，还额外扩展了一些 Python 语法： !{Shell命令}：可以通过 Python 执行 Shell 命令； {变量}?：查看变量的相关信息； {函数}?,{函数}??：查看函数的文档、源代码； _{序号}, _i{序号}：查看第 {序号} 次输出结果、语句内容； 常用的魔法函数（IPython 预定义好的一些命令，以 % 开头） %lsmagic：打印可用的魔法函数列表； %time：测试一句代码（跟在它后面）运行时间； %matplotlib inline：要求 Matplotlib 绘图模式是内嵌（inline）模式，即将绘图直接显示在当前命令行中而不是单独的窗口。 在命令行传参Python 可以在解释器的执行命令中向程序里传参，在一些场景下非常方便。允许可以传入无限个参数，以空格隔开：??/??/python ??/??/prog.py arg1 arg2 ...在程序内这些参数通过 sys.argv 来接收（需要 import sys 模块）。它是一个字符串列表，存放了解释器的执行命令的所有参数。以上为例，sys.argv 的内容为 [&#39;??/??/prog.py&#39;,&#39;arg1&#39;,&#39;arg2&#39;,...]。 Python 内置的 argparse 模块提供了更高级的功能，请参考我 Python 命令行解析参数的笔记和 argparse 模块的文档。变量命名规范编程语言一般有一些约定俗成的习惯，如变量命名规范，下针对 Python 具体情况总结： 模块名/包名/文件名：单词全小写，加下划线； 类名：单词首字母大写，不分隔（驼峰命名法）； 变量名 普通变量（包括实例）：单词全小写，加下划线； 全局变量（当作常量）：单词全大写，加下划线； 函数名/方法名：单词全小写，加下划线。除此之外，还有一些其他 Python 规定的命名法，用以表示特殊含义（如私有、保护、特殊变量或方法），见第五、六两章。二、基本数据类型与数据结构文档：https://docspython.org/3/library/stdtypes.html以下是我整理的 Python 的内置类型（包括了各种基本数据类型与数据结构），它们是逻辑上基本的类型，还有一些零散的内置类型是为其他功能服务的，虽然也是内置类型，但不方便划归到主要的分类逻辑，在后面穿插讲解。应当强调一件事情：Python 中任何数据类型都是类，任何变量都是对象（类的实例）。Python 定义了一个基类 object，所有内置类型都是它的子类（是在 Python 安装目录中的源码 py 文件中定义的）。连函数这种东西也是一种类，见“函数也是类！”一节。Python 是动态语言，数据结构非常灵活，这些容器里的元素可以是任何数据类型。例如列表里的元素可以是列表，从而形成列表的嵌套。C 语言里经常学的隐式转换在 Python 也有，规则麻烦，没有必要了解，老老实实地统一数据类型按规矩来，必要时用强制类型转换。强制类型转换函数就是 类型名()，它们是 Python 的内置函数。除此之外，Python 还有个空类型，用 None 表示。在上面的列表中，类型间的层级关系表示相应类的继承关系，也就意味着子类继承了父类的操作，又自己定义了一些操作。明白这种关系很有必要，也方便分类记忆操作。另外，Python 变量类型分可变（mutable）和不可变（immutable），前者可以修改数据结构里面的值，后者只能看作一个整体。在上图中，红色是不可变类型：数值类型、元组、字符串等，蓝色是可变类型：列表、字典、集合等。这个概念非常重要，是理解 Python 变量机制的关键。Python 变量赋值机制在介绍具体数据类型前，先强调 Python 变量的赋值机制，对下文深入理解非常重要。先说结论：Python 所有变量都是一段内存空间的标签，或称引用。以一个简单的赋值语句为例：A = B，深究它其实是非常麻烦的事。先看 C 语言： 设 B 是常量，第一次对 A 赋值是初始化，系统为 A 分配一块内存空间存放 B； A 和这一块存储空间是绑定死的，之后的赋值都是修改这块空间； 设 B 是变量，赋值操作也是把 B 的内容取出来，复制到 A 的这块空间。注：C 语言不存在大小不合适放不进来的问题，因为 A、B 的类型都声明过了，如果不同会报错（不考虑强制类型转换）。再看 Python 语言： 设 B 是常量，第一次对 A 赋值是初始化，和 C 语言一样，系统为 A 分配一块内存空间存放 B； 但 A 不是绑死在这块空间的，之后遇到新的赋值 A = B&#39;(B’ 不等于 B) 时，会为 B’ 分配一块新的空间，然后将 A 绑定到这块空间； 设 B 是变量，赋值操作是将 A 绑定到 B 绑定的那块空间。 Python 内置函数id() 可以查看变量，与 C 语言 取地址 &amp;amp; 运算符类似，以上内容可以用此函数验证。这种灵活的绑定与换绑就是标签的逻辑。可以把常量想象成客观存在的物体，如桌子、凳子、电脑；变量就是一堆贴纸，赋值操作就是拿着贴纸贴来贴去。标签机制也解释了为什么 Python 语言是动态的，即一个变量不需要声明其类型，是在被赋值后才确定类型的。因为它就是个标签啊！因此，Python 中的 del 语句仅仅是删除了变量与数据的绑定，而不是真正释放了内存。Python 是高级语言，垃圾自动回收，不需要显式地销毁对象。可变类型内部机制这里要涉及一下可变类型与不可变类型的区别。它们都是变量，如果把变量看作整体的话，上面的“标签”机制都是适用的。但可变类型还可以修改里面的值，例如考虑列表修改元素 A[i] = B，这时“标签”机制还适用吗？可变类型就是一撮不可变类型和可变类型的集合，如此递归下去，即可变类型就是一撮不可变类型的集合。它存放里面的每个不可变类型都是以变量的形式，所以就是一个盛放标签的容器。这个机制对区分可变与不可变至关重要，也对理解 Python 函数传参方式至关重要。以列表和元组的区别举例，A1 = (1,2,3,4,5) 和 A2 = [1,2,3,4,5] 的区别在于，前者是不可分割的一个整体，后者相当于一个盛了 5 个变量 a,b,c,d,e 的桶，它们分别是贴在整数 1,2,3,4,5 上的标签。因此，上面的回答是肯定的，比如 A2[0] = 0 就是把贴在 1 身上的标签 a 撕下来贴到 0 身上。例：Python 复制可变变量的坑方便性自然会带来一些麻烦。Python 这种灵活的变量赋值机制也会带来麻烦，例如这里要介绍的在复制可变变量时的坑。这也是为什么要了解这些机制的原因，因为如果还按照 C 语言的理解方式的话，写代码会遇到不了解就永远想不通的 bug！以列表为例，假设想把变量 B（是个列表）复制一份给变量 A，并要求这两份完全独立。可以想到很多方式： A = B：这个在 C 语言都知道是错的（不过是因为 A、B 是数组的第一个元素地址）。 Python 中如果这样的话，会把 A 这个标签贴到 B 贴的地方，之后它俩是联动的，一个动另一个也跟着动，因为指的是一个东西； A = B[:] 或 copy() 方法：切片切全部。这个语句原理是：虽然 B 是变量名，但是 B[:] 就是具体的一个列表了。但是问题在于，如果列表里面的元素有可变类型的话，复制这个元素就属于第一种 A = B 的方式了。 copy()方法实现的原理是一样的； 写循环按元素复制：这个可以确保没问题，但是非常麻烦！我之前碰到 bug 后就直接这样做了，有点蠢； A = copy.deepcopy(B)：终极杀招。调用 copy 库中的 deepcopy 函数，会递归地（深度优先）复制列表的每一个角落。上面第 2 条称为浅复制（顾名思义是只真正复制了列表的最外面一层），第 4 条称为深复制。可以看到，在 Python 这些复制上的区别就是列表变量的“标签”机制引入的麻烦。以下介绍 Python 各类型，分成几组来讲。数值类型从逻辑上讲，编程语言基本的数值类型简化成整数、浮点数、复数就够了。Python 和这种简单的逻辑是一致的，也是只有这些基本的数值类型，不需要像 C 语言等底层语言那样还要区分整数的位、符号、浮点数的精度等（例如：long、int32、float64），也不用考虑溢出等问题，按照其逻辑意义放心使用就行，非常省心。数值类型的操作一般通过运算符定义。常用的运算包括： 四则运算：加、减、乘、除、余数、幂…… 比较运算：大于、小于、不等于…… 布尔数：与、或、非…… 整数的按位运算 复数：取实部、虚部、共轭…… 增强赋值：二元运算符 + =这些运算都是学编程/数学最基本的东西，不可能忘记。可能有些符号上的不同，现查就可以了，不必花工夫记忆。序列类型序列类型包括基本的列表和元组，还有一些数据类型是它们的子类： 字符串（string）：只能存字符的元组； 数字范围（range）：只能存等差整数列的元组，一般用于 for 循环； 字节串（bytes）：只能存 ASCII 字符的元组； 可变字节串（bytearray）：只能存 ASCII 字符的列表。这才是这些数据类型的本质，并不是单独的、全新的类型。这样也就理解了诸如为什么字符串的操作和元组类似的问题，因为它就是个元组。非数值类型（包括序列类型和下面的集合、字典等）通常由 Python 内置函数或该类方法定义，也有些常用操作重载到符号。可变和不可变类型共有的是一些访问操作，例如切片、取最值、拼接（注意拼接这类操作会返回新的对象而不是原地操作）；可变类型特有的是修改操作，例如添加、删除、替换。这种方式方便记忆。集合、字典我把集合和字典放在一起说，因为它们都模拟了数学概念。这些数学概念本身就是从现实世界中抽象出来的，所以对应的数据结构在编程中也很实用。集合可以理解成是操作受限的线性表，而且是无序的。受限的操作就是集合的交、并、补等运算。另外，集合也分可变（set）和不可变（frozenset），是两种不一样的数据类型。之前一直对字典的概念理解不到位，平时也慎用字典。就是字典这个词语太迷惑人了，它就是数学上的映射（或称函数），即给定只不过定义域是有限的集合。当然字典是无序的，因为映射的概念并没有给定义域赋予序关系。字典的操作完全可以拿数学上对映射的操作来对应，例如 list 方法就是取定义域等等。迭代器、生成器这部分是高阶内容，需要花大篇幅讲解。直观概念迭代器是一个用于遍历，并能记住遍历位置的类。可以将其看作一个售货机，一开始里面装满了商品，每次投币它就会吐一个商品，吐完了机器就会报错（也可以设计永远吐不完的迭代器）。迭代器每次投币后会吐什么东西，什么时候吐完，都是编程人员自己可以设计的。可能会想，之前介绍的序列类型不是已经实现这种功能了吗？其实 Python 的序列等类型可以直接构造迭代器，但还有很多其他的方式（见“设计迭代器”小节）。通过其他方式设计的迭代器有很多额外的优点：如不需要事先计算好所有要遍历的元素（例：深度学习把一批批数据放在叫 Dataloader 的迭代器中），有的可以实现无限遍历下去（例：自然数），等等。应当注意，迭代器是一次性的，实例创建了之后，就开始计投币，无法撤销。因为迭代器就是用来遍历的，别无他用，吐完了的迭代器就废了没用了，没法重置。因为这种一次性的特性，Python 设计者干脆设计了一种工厂，专门生产（即创建实例）这些一次性的迭代器，用完了没关系再生产一个就得了。这个工厂就是所谓的可迭代类。用法当一个对象 A 属于可迭代类（严格定义见下一小节）时，调用 iter(A) 返回一个迭代器对象（相当于工厂生产了一个一次性的迭代器）。设它为 I，每调用一次 next(I)，就相当于投一次币，吐出来的商品就是 next() 函数的返回值。多次调用 next(I) 后，可能会抛出异常 StopIteration，可能永远不会（迭代器的设计者决定）。一种简便的方式是：用 for 循环。以下 for 循环代码for x in A: ... 等价于I = iter(A)while True: try: x = next(A) ... except StopIteration: break这样就不用多次手动调用 next() 函数，还能自动检查是否碰到了 StopIteration（即是否吐完），防止程序报错。 上面的等价代码就是 Python 中 for 循环的本质，它只是用 while 循环定义的一种简便写法，并不是一种新的循环语句。in 后面必须是可迭代类型。 Python 内置函数 zip() 可以将多个可迭代类的对象打包在一起，构成一个以元组为元素的可迭代对象。此函数常用于 for 循环中有多个变量同时循环： for x, y in zip(A, B): ... A、B 长度不一致时，取最短者。设计迭代器一、从头自定义Python 在类中提供了两个特殊方法 __iter__(), __next__() 。这两个方法定义了 Python 的内置函数 iter(),next() 作用在此类实例上的效果。这两个方法有特殊要求： 必须只能有一个参数 self； iter() 只能返回 Iterator 对象。一般的设计模式是： __iter__() 定义一些实例属性，用于存放基础数据，通常返回 self 自己； __next__()定义向下迭代的规则。以数列为例，会在前者存放初值，后者存放递推公式。 Python 根据方法的定义情况自动判断： __iter__(), __next__() 都定义了的是 Iterator 类； 只定义了 __iter__() 的是 Iterable 类，即可迭代类。任意一个类（不需要显式继承 Iterator 等类），只要把这两个方法定义了（称实现迭代器），它就是可以像上小节的 A 那样用了。按照这个定义，__iter__() 返回 self 自己没有问题，因为 self 定义好了两个方法，本身就是 Iterator 类。 二、从内置类型构造Python 本身的多数能当数据“容器”的类型，包括：列表（list）、元组（tuple）、字典（dict）、集合（set）、字符串（str），已经定义好了 __iter__()，但没有定义 __next__()，所以是可迭代类。Python 定义好的 __iter__() 都不是返回自己，而是一个新的类的对象（如 list_iterator 类，这个类定义了 __iter__() 和 __next__() 所以是 Iterator 类）。所以这些 Python 内置类型直接用 iter() 包裹一下就是个能用的迭代器，其迭代方式就是依次访问容器里的第 1,2,… 个元素。 这里只是讨论内置类型的原理而已。实际使用上根本用不上迭代器这个概念，因为平时对其的遍历就是 for 循环，它让语法中连 iter() 函数都不会出现。三、生成器Python 另外提供简捷的语法实现迭代器。以下两种方式是生成器（generator），它是 Iterator 的子类。这个类没有什么特殊的，暂时理解为普通的迭代器即可。语法如下： 将函数的 return 改为 yield，函数就变成了生成器。这种生成器函数会 yield 很多次（通常把 yield 语句包在循环中），其迭代方式就是依次访问第 1,2,… yield 出的东西。 有一种快速构造列表的语法，形式如 ` [f(i) for i in A] `。如果把外面的中括号换成小括号，就是构造生成器（并不是元组），这种方式有人称为列表生成式。其他 Python 预定义的数据结构 枚举（enum）：与 C 语言中的枚举类似（Python 3.4 版本新功能）； 文件（file）：Python 中文件也是类，打开的文件都在会创建一个 file 类的实例。此类定义的主要操作有打开、关闭、读取、写入等等。 Python 标准库 collections 中有其他更高级更花哨的数据结构，例如双向列表、有序字典、计数器等等。有需要可使用。三、控制流编程语言一共有三种控制流：顺序、条件、循环。每种语句都有我不太熟悉的用法。 空语句： pass。某些时候为了保持程序结构的完整性，必须填一个语句，但它什么也不干； del 语句：除了可以删除变量与数据的绑定，还可以删除容器类型的元素，参见速查手册； 条件选择语句：在 C 语言中是 switch 语句，但 Python 只能用 if...elif...else 实现。（新版 Python 3.10 新引入了类似 switch 语句的 match 语句，应慎用）； 循环语句后加 else：这是个语法糖，在循环正常执行完毕（指没有通过 break 跳出）后会自动跳到 else 语句，适用于检验是否循环被 break （不用 else 的实现方法是加 flag）。四、函数我之前只是知道最基本的函数定义，并知道 Python 是动态语言：函数非常灵活，不需要规定参数和返回值的类型。本章系统介绍 Python 的函数，还是有很多知识可以学的。函数语法如下：def function_name(arguments): &#39;&#39;&#39; documentation &#39;&#39;&#39; return expression 冷知识：函数开头三个引号的注释能当作函数的文档（通过命令查询后显示出来的），而 # 开头的注释起不到这个作用。变量的命名空间与作用域Python 变量的命名空间与作用域和 C 语言是差不多的（暂不考虑下文中函数中嵌套函数的情况）： 内置变量：Python 自带的东西； 全局变量：在函数外定义，对当前文件（模块）生效； 局部变量：在函数内定义，只对此函数生效。Python 中没有定义外部变量的机制。一般实现起来是用一个配置文件（模块）统一定义各种常量，在要用到的文件中 import 该模块。Python 的命名空间规则（和 C 语言一样）类似文件系统命名规则，全局变量和局部变量可以重名。如果想在函数内使用重名的全局变量，应加条声明语句：global 变量名 或 nonlocal 变量名。参数传递方式C 语言中函数有两种参数传递方式，搞不清楚这两种方式，就容易出现写函数时不小心把实参修改了这种情况： 值传递：相当于在函数内定义里一个局部变量（形参），初始化为传进来的（实参）值，形参动实参不跟着动 引用传递：形参是实参的引用，形参动实参跟着动C 语言是通过在函数定义处参数前加上特殊符号来区分的。例如加 *, &amp;amp; 可实现引用传递，不加就是值传递。数组都是引用传递。Python 的参数传递规则理解起来很简单：传进来参数后，它做的事情就相当于在函数开头加了句 形参 = 实参。那么它是什么类型的传参方式呢？实际上是兼而有之，严格意义我们不能说值传递还是引用传递，我们应该说传不可变对象和传可变对象。现假设形参动了，考虑实参会不会跟着动。根据 Python 变量的赋值规则（“标签”机制），如果是整体地动形参，则只是换绑了形参的标签，不会影响实参，此时是值传递；如果形参是修改可变类型的内部，则只是把里面变量换绑了，而形参作为可变类型包含的变量没有变，此时是引用传递。现在看来，Python 的参数传递规则是根据变量可变类型自动决定的。如果想自己决定怎么办？把变量类型改一改即可，比如把整数用中括号包裹后再传进去。参数列表Python 函数的参数列表最多可以有 5 部分组成：位置参数、默认参数、可变参数、命名关键字参数、关键字参数。规定： 每一部分都是可有可无； 参数列表必须按照这个顺序写，否则会报语法错误。传参有两种写法：一种前面带参数名=，另一种不带直接传。def func(arg, default_arg=0, *args, arg2, **kw):以这个 5 部分都包含的函数为例，这 5 个参数分别是： 位置参数：放在最前面，以位置来标识参数； 默认参数：带了默认值的参数； 可变参数：可以包含多个元素，只能写一个； 命名关键字参数：必须以前面带参数名=的形式传的参数，如上例必须以 关键字参数：同上，但参数名不受限制，只能写一个（效果类似可变参数，可以理解为“可变的必须带关键字的参数”）。 有一点应说明，命名关键字参数和普通的位置参数写法是一样的，如果它们之间的默认参数和可变参数都没有，怎么区分呢？Python 额外规定了语法：中间用一个*隔开，它不是参数，只起到分隔符的作用。{ :.prompt-info }传参时 Python 背后解析参数的算法是贪心的、有优先级的，即先处理前面的，吃剩下的再施舍给后面的： 先按照位置识别位置参数，所有位置参数必须一一传入，否则报错； 再识别默认参数，识别到了就传给它该有的值，识别不到就设为默认值； 再识别可变参数 *args： 只考虑剩下的不带参数名=的（不可以直接传args=或*args=），统一传到 args 的元组里； 还能识别前面带 * 的实参，此实参必须是元组（列表也行），可以直接传入 args 元组，只识别一次； 再识别两种关键字参数。此时剩下的一定都是带参数名=的（如果有不带的会报错）： 先检查前几个和命名关键字参数是否一致，不匹配直接报错； 剩下的全部识别为关键字参数 **kw ： 统一传到 kw 的字典里（键对应参数名，值对应参数值）； 类似地，也能识别前面带 ** 的实参，此实参必须是字典，可以直接传入 kw，只识别一次。 如何表示任意函数？因为没有规定参数数量，就要用到两个可变的东西——可变参数、关键字参数：def func(*args, **kw): args、kw 是习惯写法，用别的名字也可以，只要前面加上了适当的星号。返回值Python 函数可以有多个返回值，以逗号隔开。return 语句和外部调用里的返回值个数和顺序应一致，否则会报错。 Python 函数返回多个值时，有的我们不需要，可以用临时的替代符号代替，例如 _：```pythondef func(…): … return A, B_, B = func(…)函数也是类！ 本节涉及很多函数式编程（Functional Programming）的理念，不打算详细讲其原理，因为 Python 本身是面向对象编程（Object-Oriented Programming），只是有一点函数式编程的特性罢了。之前提到函数也是一种类（定义在 Python 安装目录下的某个源码 py 文件中），也继承自 object 基类，也有通用的属性与方法如__name__，当然也有很多它特有的。所以函数名也是变量。是变量就可以赋值、当作参数传入、返回值返回等等。 注意，C 语言也可以把函数当参数或返回值传入传出，但原理不一样。人家是地址。匿名函数匿名函数就是换了个皮肤的缩写版，与函数是同一个东西。设输入为 x，输出为 f(x)，则写成 lambda x: f(x)。（f(x) 代表 x 的表达式）为什么放在这里讲匿名函数，因为匿名函数这种简写形式通常就是为了把函数当参数传递的，就不用在外面额外定义了。也就是下面要讲的：高阶函数是指输入输出中有函数对象的函数。以下讨论高阶函数的用法。 Python 标准库 functools 有很多处理高阶函数的工具。本文不打算讲解这些工具，只讲基本概念。闭包函数外面再套一层函数把它包裹起来，叫闭包。任给一个函数def func(*args, **kw): # 一些用到 args, kw 的语句 return r其闭包为def closure_func(*args, **kw): def func(): # 一些用到 args, kw 的语句 return r return func将函数写成闭包形式的作用是通过运行闭包 func = closure_func(*args, **kw) 暂时记住函数和它的参数（都打包在了 func 变量里面），但先不运行（一般出于节省计算资源考虑）；在要运行的地方直接调用 r = func() 会更方便，不需要传参数。装饰器Python 中有在函数前加 @开头的语法：@decdef func(*args, **kw):称 dec 是 func 的装饰器。加了装饰器后的 func，在调用之前会先 func = dec(func)，再执行调用 func(...):。即先用装饰器用装饰一下原来的函数（这时的 func 已经不再是原来的 func），再使用 func。这对装饰器函数 dec 的内容有要求，否则在调用 func 时会报错。至少它的输入输出都必须是函数。至于怎么装饰则是用户定义的了。例：实现在调用函数 func 前打印“正在调用 func 函数” 的日志def log(func): def wrapper(*args, **kw): print(&quot;正在调用&quot; + func.__name__ + &quot;函数&quot;) return func(*args, **kw) return wrapperPython 也有一些内置函数（下文会涉及到），就是专门用来当作装饰器的，直接拿来去修饰函数即可。五、面向对象编程Python 是面向对象的语言。之前提过在 Python 中，万物皆对象。因此这一部分应该属于最基本、底层的知识。面向对象的基本机制是封装、继承与多态，以下分别介绍。封装形式上，类是一段封装了各种变量和函数的代码块。以下是模板：class Class(object): &#39;&#39;&#39; documentation &#39;&#39;&#39; A = 1 def __init__(self, B): self.B = B def method(self): C = 2 return self.B类里面出现的变量分为三种： 实例属性：类的实例自己的属性，通过实例.属性调用，例如 B。不可以写在方法外面，总是以实例.属性的形态出现，例如必须写self.B； 类属性：整个类自身的属性，通过类.属性调用。例如 A。写在方法外面，和函数并列； 普通的局部变量：写在方法里面的其他普通的变量。例如 C。类里面的函数叫做方法，分几种： 类中出现的函数本质上不是函数（参见“函数也是类！”小节，在外面定义的函数本质是一个类）。而且在 Python 里是先有了类的概念，才定义了函数。所以是先有了方法，才有了外面的函数。 实例方法：普通的 def 定义出的都是实例方法，它至少有一个参数，第一个参数是实例本身（习惯起名为 self），在调用时省略； 类方法：用装饰器 @classmethod 修饰，至少有一个参数，第一个参数是类本身（习惯起名为 cls），在调用时省略； 静态方法：用装饰器 @staticmethod 修饰，没有对参数的限制。在属性或方法的名称上加下划线也会起作用，有的可以起到语法上的作用，有的只是约定俗成的提示： __init__()：构造方法，在类创建实例时自动执行，一般在这里统一定义实例属性； __名字__：有特殊用途的属性或方法，通常另作他用，去完成一些扩展的功能；有很多是在基类 object 定义的； __名字：私有（private）属性或方法，外部代码无法直接调用，调用会报错；（注意：__名字__ 不算私有属性或方法） _名字：受保护（protected）属性或方法，提示最好不要调用它，但调用并不会报错。以下列举了一些特殊属性或方法，所有类都可以使用它们： 类的信息： __name__：类属性，返回类的名称； __class__：实例属性，返回实例所属的类； __dict__：作类属性，返回所有类属性和方法；作实例属性，返回所有实例属性； 定制类：规定 Python 内置函数在该类上的行为 __len__()：规定 len() 函数； __str__(), __repr__()：规定 print() 函数（优先使用后者）； __call__()：使该类的实例可以像函数一样被调用（callable），此“函数”与__call__() 方法一致； __iter__(), __next__()：规定 iter(),next() 函数（见“迭代器、生成器”一节）； __getitem__(), __setitem__(), __delitem__()：使该类的实例可以像列表那样用中括号下标取、赋值、删除元素； __getattr__()：规定在调用该类没有的属性和方法时的行为； __slots__()：规定类允许添加的实例属性； 定制运算符：类似于 __add__(), __and__() 这种用运算符名称的特殊方法，可以使该类的实例使用运算符合法，具体行为在这些特殊方法中定义。支持的运算符可以现查。继承与多态Python 继承与多态的规则如下： 继承的语法是把继承的父类名写在类名后的括号里，不写则默认继承基类 object； 子类会拥有父类的所有属性和方法，可看作把父类定义的所有代码复制了过来； 子类不仅是子类，也是父类（多态）； 子类可以定义新的属性或方法； 方法重写：子类里写与父类名称相同的属性或方法，这样会覆盖掉； 不允许方法重载，即有相同的名称，但是参数列表不相同；（同理也不允许函数重载） 允许多重继承，在类名后的括号里以逗号隔开。多重继承使得继承的树形结构变成前馈网络状结构。Python 所有的类都继承自一个共同的基类 object（注意是小写），就像森林的所有的根结点有一个共同的父节点一样。这个基类会定义一些共有的、通用的属性或方法，如上文提到的特殊属性与方法。 从以上介绍的内容看， Python 似乎与其他面向对象的语言（如 C++）没啥区别，都是面向对象的那一套理论的实现。实际它和 C++ 的主要区别在于 Python 是动态语言，实例的属性和方法都是可以临时添加或删除的，而 C++ 等语言则必须在类定义时声明。{ :.prompt-info }super()函数考虑一个问题：每个类一般都要写构造方法 __init__() 来定义一些实例属性，但是子类和父类的构造函数是重名的，那岂不是把父类的重写了？想要继承父类__init__() 中定义的实例属性怎么办？做法：不需要手动把代码复制过来，只需在子类的调用 super().__init__(...) 即可。（注：... 中是父类方法的参数，不需要带 self）super() 函数原理很复杂，不需要搞明白，只需知道其效果是：向上找到最近的匹配点后面方法名字和参数的父类，然后调用该方法。获取对象信息的内置函数与方法下面列举与类有关的接口，有的是 Python 的内置函数，有的是类自带的方法： type(A)：返回变量 A 的类型。本质上由于所有变量都是对象，它返回的实际上是对应的类名； isinstance(A, Class)：判断对象 A 是不是类 Class 的实例； dir(A)：获取对象 A 的所有属性与方法； help(A)：查看对象的帮助文档； callable(A)：判断 A 是不是可调用对象（参见函数和 __call__() 方法）。除此之外，一些 IDE 也在这些事情提供了一些方便，方便程序员看代码：如 Ctrl 点按函数、类或模块直接打开源代码，以及上文提到的 Jupyter Notebook 的 ? 等扩展命令。六、Python 模块模块的组织在 Python 中，一个 py 文件就称之为一个模块。最简单的方式就是在一个目录下写很多并列的 py 文件，这样就有了很多模块。但很多时候，模块间有逻辑上的层次关系。由于模块就是文件，所以可用文件系统的层级关系（即文件夹）实现，但必须手动规定，Python 不会把所有的子文件夹视作自己人。手动规定的方式是在目录中添加 __init_.py。Python 只认里面有 __init__.py 文件的文件夹。这和文件系统还不太一样，文件系统是分文件（叶子结点）和文件夹（内部结点）的，文件夹只起到组织结构的作用，没有内容。而 Python 模块的内部结点是有内容的，就是写在 __init__.py 这个文件里，所以也是个模块。模块的层级关系表示方法也是以点表示：父模块名.子模块名。到了叶子结点后，再往下的点表示的就是模块里的变量、类或函数了。像这样的一整套多层的模块通常称为包（package）。Python 内置的标准库以及 pip 安装的第三方库就是包的形式。例如下图是 PyTorch 包结构：因此想要把自己的代码做成工具形式，就最好打包成包。（即使可能比较简单，如一个目录下多个并列的 py 文件，也最好向此目录添加一个 __init__.py 文件）模块的使用使用模块主要是通过各种含 import 关键字的语句。这里会有一个疑问，模块不就是 py 文件吗，直接运行不就得了？例如要运行模块 A，A 依赖模块 B，可不可以不在 A 的代码中 import B，而是先运行 B 再运行 A 呢？一个原因是在终端里执行完一个 py 文件，程序就真正结束了，内存得到释放（VSCode 运行 Python 代码也是在内置的终端里跑的）。而有些环境配备了更强大的功能，例如： Spyder：每跑完一个 py 文件后，它的工作区会记录下此程序的所有变量，相当于跑完程序没有去释放内存。释放内存需要手动去 clear 工作区。 Jupyter Notebook：直接把一个个代码块（严格来说不是模块，因为一整个 ipynb 格式的笔记本才是一个文件）当作进入 IPython 提示符后执行的一块块表达式，虽然逻辑不太一样，但也有起到那样的效果。即使可以用更高级的环境解决，也涉及很多方便性的因素： 有很多模块时，并不想去跑去逐个运行一遍，只想运行一下主程序； 标准库、第三方库都放在隐藏位置，找出来也很麻烦。但这些都不是本质的。最重要的事情是：模块也是一种封装机制，它把各种函数、类和变量采用类似于 class 的方式封装起来（调用时前加模块名.以区分)，而不是把各种变量混在一起，只当作按照不同顺序执行的代码块。这体现在各种 import 语句的效果上，见下： import Package：模块当作一个大“类”被定义出来，调用模块中的类或函数也和类差不多，都是加点：模块名.类或函数名。注意： 可以一次 import 多个，以逗号隔开 可以为模块取别名，在后面加 as 别名 from Package import ...：（沿用上文说法）若 Package 是内部结点模块，可 import 子模块；若是叶子结点模块，则可 import 模块里的变量、类或函数。 实际上，from Package import * 就起到了“直接运行”的效果。但实际上这个语句并不常用，可见平时大家都是在利用模块的封装功能的。在使用时，Python 的寻找顺序为：先从当前目录下找，再去 Python 的安装目录里找，都找不到的话就报错 ModuleError。模块文件模版一个标准的模块的模版如下：#!/usr/bin/env python3# -*- coding: encoding -*-&#39; documentation &#39;__author__ = &#39;...&#39;import ...def ...:class ...:if __name__ == &#39;__main__&#39;: ...标准注释最开始几行的格式化的注释是标准注释，并不是单纯的文本，解释器其实是可以识别注释中的某些格式、提取出一些配置信息的。第一行由井号和叹号组合的叫 Shebang（或 Hashbang），是类 Unix 操作系统的特有的东西。在代码文件中出现这一行，操作系统会将 #! 后的东西作为解释器来执行。（这一行平时是不加的，因为通常解释器都由 IDE 或 Conda 环境指定好了。）代码的字符编码也用格式化的注释定义，见第二行。encoding 是编码标准，如 utf-8，cp1252 等。如果不加这行注释，则表示默认为 utf-8。Python 作为文本文件，它自己本身是定义过编码的（例：Windows 记事本在保存文件时有选择编码的选项），这是文件本身的属性，定义在整个文件的前 2 个字节。上面注释的定义并不是指这个编码，它规定的是 Python 解释器如何读取解析这个代码文件，通常对输入输出中的符号起作用。我不打算仔细研究这些东西，直接上懒人包：怕遇到乱码就直接 utf-8，把文本文件和第一行注释都设置好，应该就不会出问题了。文档信息Python 模块视第一个字符串为模块的文档，类似于函数的文档。注意它不是写在注释里的，而是字符串。作者信息由特殊变量 __author__ 字符串定义。类与函数定义区正文开始一般会定义一些类或函数。在模块中的变量、函数、类也可以加下划线代表一些信息，和类是相似的道理： __名__：模块特殊的变量、函数、类； __名：表示模块私有的，模块外无法直接调用，调用会报错； _名字：表示模块受保护（protected）的，提示最好不要调用它，但调用不会报错。运行区定义完类与函数后，就是运行程序的主要代码，我想不出好名字就叫运行区好了。这部分代码通常由 if __name__ == &#39;__main__&#39;: 包裹。__name__是模块。只需知道加上这行起到的效果： 如果直接运行此模块，则执行其中的语句； 如果 import 进别的模块，则运行别的模块时不会执行其中的语句。 此语句不充当程序入口的作用。和 C 语言里的 void main() {...} 意义不同！考虑以下场景：当一个项目有好多 py 文件时，如果此程序是主程序，那加不加这句都无所谓；如果不是主程序，通常里面都是些定义的类与函数，也没有真正执行的代码，那if __name__ == &#39;__main__&#39;:里面的代码是做什么的？答：测试此模块用。只要运行此模块，就会执行测试代码；而真正运行主程序时不会执行它。七、输入输出Python 输入输出有很多方式，这里不讨论由第三方库实现的各种数据的输入输出，只讨论最基本的： 标准输入输出：从键盘读取，在 console 打印输出； 文件输入输出：读取或写入文件。文件输入输出和 C 语言形式上类似，比较麻烦，我太不常用就不写了。本章只讲标准输入输出。input() 与 print() 函数标准输入输出都是通过几个内置函数实现的。输入主要靠 input() 函数，效果为：程序在此处暂停，等待用户输入字符串，敲下回车后字符串传入其返回值。input 函数接受一个字符串参数作为提示语。输入主要靠 print() 函数。print 函数非常灵活，可以接任意类型的任意多个参数，输出结果以空格隔开。除字符串外，Python 为所有的类都规定了一套默认的打印格式，在 __print__()和__repr__() 方法中定义，可以自行修改。 注意，代码中不能像在提示符下那样变量直接敲回车查看信息。所以直接把 变量 摆在代码里的语句是没有效果的，必须写 print(变量)。 之前的习惯没必要：我喜欢把所有要打印的东西用 str() 函数强制转换成字符串，用加号运算符拼接后再打印。直接用逗号隔开即可。格式化字符串格式化字符串是指意义是把动的变量统一放后面，不动的字符堆在前面，看起来更美观。字符串的格式化规则都是在 str 类中 format() 方法定义的。用法：在字符串处要填空的位置以{}表示，在方法的参数中填入数据。大括号里面可以： 什么也不填：顺序传入参数；（即 format() 方法的可变参数，也可以传入一整个 *元组） 自然数：按照数字标示的参数位置传入； 变量名：应传入带 变量名= 的参数；（即 format() 方法内的关键字参数，也可以传入一整个 **字典） 数据格式：以 % 开头，不需记忆，因为与 C 语言的表示方法一致。这样 print() 函数的使用方法是 print(&#39;...{}...{\\%f}...&#39;.format(A,B))。还有一种更简洁的写法：print(f&#39;...{A}...{B:f}...&#39;)，字符串前加 f 表示格式化的字符串。八、遇到 bug？本章统一讲解一切有关 bug 和遇到 bug 的时候做的事情。异常Python 中的报的错称为异常（exception）。异常本质上是一大类特殊的类，都是一个基类 BaseException 的子类。常见异常Python 预定义了各种各样的内置异常，都可在文档中查到：https://docspython.org/3/library/exceptions.html 前面有很多地方涉及到了各种异常。这里我只讨论一些经常遇到的，也是为找 bug 提供经验吧。先占个坑，以后再写。自定义异常Python 的内置异常有时候不够用，可以自定义异常。做法：只需定义一个继承异常类的子类。最简单的是直接继承异常的基类，什么事也不做。class MyError(Exception): pass 自定义异常应当继承 BaseException 下的 Exception，而不是BaseException，BaseException 除了 Exception 里普通的异常外，主要是 KeyboardInterrupt 等系统级别的特殊异常。异常处理：try 语句Python 有完善的异常处理机制可供编程人员使用，即 try 语句，可以在代码执行时捕获异常，并采取用户规定的操作。由几部分组成： try: 后面跟要尝试捕获异常的代码； except EXCEPTION_NAME: 后面跟捕获到名为 EXCEPTION_NAME 的异常时要执行的代码，可以有多个 except 从句；（EXCEPTION_NAME 可以多个用逗号连接，不加它则表示所有异常） else: 后面跟没捕获到任何异常后要执行的代码； finally: 后面的语句是在执行完上面所有程序后一定要执行的代码。如果触发不在 EXCEPTION_NAME 里的异常或者没有异常，程序不会中断；而触发不在 EXCEPTION_NAME 里的异常，仍然是会报这个异常的错。这两种情况都会触发最后的 finally 从句。 实际上 try 语句定义了四种从句，是比较混乱的，即有的情况可以有很多种表达方式，会有些语法冗余。但是编程语言不是追求逻辑的完备性，而是为了使用方便。所以不必纠结定义混乱的问题。抛出异常：assert, raise 语句有时候在代码里需要手动触发异常，自己自定义异常也是为了抛出的。抛出异常有专门的语句，平时 Python 报错抛出的异常在 Python 的源码就是用了这些语句： raise EXCEPTION_NAME：抛出名为 EXCEPTION_NAME 的异常； assert EXPRESSION：EXPRESSION 是一个逻辑表达式，如果为 True 则通过，否则抛出 AssertionError 异常。它等价于 if not EXPRESSION: raise AssertionError assert 语句的用处： 预防未知的错误，在写代码时我们脑海中预判了一些情况下可能出现的问题（例如 0除以0），虽然这些情况可能不会发生，但也可以加条 assert 这些情况 在前面以防万一； 有些机器不满足程序运行的条件，在最前面加条 assert 可以让程序直接报错，而不必等待程序运行后崩溃； 调试：见下节。上下文管理器：with 语句with 语句一般就是用于异常处理的，放到这里来讲。with 语句的控制流如下。以下代码的执行顺序为：with Class(...) as var: ... 创建一个 Class 类的实例； 调用该实例的__enter__() 方法，返回值赋值给as 后面的变量 var； 执行主体部分的语句； 调用该实例的__exit__()方法。可见本质上就是在一段代码前后加上两段代码。但并不是三段代码简单地顺序拼接，它们之间和一个类的实例息息相关。__enter__()和__exit__()方法需要自己定义（称实现上下文管理器），也必须遵循一些规则：例如 __exit__() 必须规定三个有关异常的参数 exc_type, exc_val, exc_tb。可见它适用于对资源进行访问的场合，例如文件操作就是包裹在 with 语句中进行的，file 类就是这里的 Class 类，打开关闭文件都由 file 类定义的上下文管理器控制。自己写的代码根本用不到这东西，只需知道哪些地方最好用 with 包裹，遵守现成的规范即可。调试在 Python 中调试代码有以下几种方式： print 语句：在合适的位置 print 变量的值； assert 语句：用 assert 后接的逻辑表达式来验证自己的想法和程序实际运行是否一致，不一致则报 AssertionError； logging 库：专门用于日志的库，其实就是更高级的 print。它可以输出不同样式的信息（DEBUG, INFO, WARNING, ERROR, CRITICAL），也像 assert 一样有开关控制输出哪些信息；不仅可以输出到 console，还能输出到日志文件中； pdb 调试器：在代码中 import pdb，用pdb.set_trace()语句打断点。 在运行代码时命令加选项 -m pdb，代码运行进入调试模式，提示符变成 (Pdb)-&amp;gt;，可在其中输入 pdb 命令控制调试流程： n：单步执行； l：是查看当前运行到的代码位置； p 变量：查看变量； … IDE 里的调试工具：基本都是 pdb 的图形化实现，它不需要在代码中做任何标记，通过按按钮的方式完成打断点、执行，还能即时查看各种变量。这些调试方法没有优劣之分，各自都有优缺点。平时使用时应视具体情况选用合适的调试方式： print 语句可以输出具体信息，assert 则不行。有时候我们需要具体信息，而有时候会看起来太乱； assert 语句可以全局开关。在运行代码时加参数 -O 可以关闭所有 assert（相当于把所有 assert 语句删掉）。用 print 语句如果不想调试了必须删除或注释掉； logging 库更适合耗时较长的大型项目中管理日志，对于小代码实在没有必要； 在 IDE 里调试的问题是每次都需要手动打断点，不能长久保存断点信息，也无法输出成文本。更适合临时地查看程序执行的逻辑。Python 官方教程里也介绍了什么时候用什么工具最好：https://docs.python.org/zh-cn/3/howto/logging.html#logging-basic-tutorial测试前面提到在模块的 if __name__ == &#39;__main__&#39;:可以用来做测试。Python 中也有专门的 unittest 模块用来做单元测试，我用不到，就不作介绍了。附录：其他 Python 标准库与内置函数Python 标准库与内置函数在上面各章节都有所涉及。其余的暂时列举在这里，只概述其大致用途，使用时现查，不作过多介绍。官方文档：https://docspython.org/3/library/ sys 模块：可以查看当前系统或解释器级别的信息，例如上面见到了 sys.argv； os 模块：让 Python 能类似于 Shell 命令那样处理文件和目录； time, date, datetime 模块：处理时间信息，有专门的时间类表示，也可以整数表示（1970.01.01 后度过的秒数）； time.sleep(secs)：可以让程序暂停 secs 秒，比较常用； math 模块：提供常用数学函数，处理数学计算。cmath 模块用以处理复数； re 模块：使用正则表达式处理文本匹配等问题； argparse 模块：为命令行向解释器传参提供了更高级的功能（见笔记）。 json/pickle 模块：都用于保存、加载 Python 变量，前者存的是文本文件（json 格式），后者是二进制文件。文本文件更通用，更易读，但支持类型少，例如不支持 Python 类实例的保存；二进制文件不可读，但支持所有 Python 类型。" }, { "title": "日语学习笔记：《标准日本语》按课单词总结", "url": "/posts/studynotes_Standard-Japanese_vocabulary/", "categories": "有趣的事情", "tags": "学习笔记, 语言", "date": "2017-02-16 00:00:00 +0800", "snippet": "本文将《标准日本语》中的单词按课整理，按日语学习笔记：语法简要总结提到的词性分类，每个词性按照意义分作小类。这里负责语言积累性的部分，不涉及只起语法作用的附属词（助词、助动词）。每个词会讲解值得注意的用法和难以直观理解的构词法。由其他词类自然转用的词类，我会与原词类放在一起，例如动作性名词与三类动词、能构成二类形容词的名词与二类形容词。对用言我会讲解固定搭配。之后我会为我认为比较难懂的单词补充课本第一次出现的例句。名词一、国家与地名等专有名词国家名 通常用～国（こく），西方国家一般直接用片假名。国家可以直接说国(くに）（I-3）。国家的人一般在后面加人（じん）。语言一般加語（ご）。例外情况： 英語（えいご）：英语（##### I-11） I-1 中国（ちゅうごく）：中国 音便：国，こく浊化为ごく 日本（にほん/にっぽん）：日本 构词：日（にち变に），太阳；本（ほん），本源。全称日本国（にほんこく） にほん更常用；にっぽん更正式，更有气势，常用于正式的国号、比赛喊加油等场合 韓国（かんこく）：韩国 アメリカ（America)：美国 フランス（France）：法国I-11 スペイン（Spain）：西班牙 外国（がいこく）：外国I-16 オーストラリア（Austrilia）：澳大利亚地名I-2 汕頭（スワトウ）：汕头 ロンドン（London）：伦敦I-3 上海（シャンハイ）：上海 東京（とうきょう）：东京I-4 横浜（よこはま）：横滨 构词：横（よこ），旁边；浜（はま），海滨。旁边的海滨 名古屋（なごや）：名古屋 大阪（おおさか）：大阪 构词：阪（さか），同坂（さか），坡。很大的坡 I-5 神戸（こうべ）：神户 构词：神（こう），神社；戸（へ变べ），房子。神户即神社所辖的民户（日本封建时代皇族、地方官员和神社都有封户） I-6 北京（ペキン）：北京 広島（ひろしま）：广岛 构词：大岛。广岛建成于三角洲，三角洲像一个大岛屿 京都（きょうと）：京都 北海道（ほっかいどう）：北海道 箱根（はこね）：箱根 构词：箱（はこ），箱子；根（ね），嶺（みね）的省略音，山顶。形状像箱子的山顶 銀座（ぎんざ）：银座 构词：座（ざ），货币铸造厂。银币铸造厂 渋谷（しぶや）：涩谷 构词：渋（しぶ），茶色、铁锈色；谷川（やかわ），溪流。涩谷川。涩谷川因含铁量较多，河底为涩涩的铁锈色而得名，涩谷是建设在涩谷川上的 新宿（しんじゅく）：新宿 构词：宿（じゅく），宿场，旧时日本的驿站（相当于服务区）。新建设的宿场（江户时代设立） I-9 天安門（てんあんもん）：天安门 万里の長城（ばんりのちょうじょう）：万里长城I-10 奈良（なら）：奈良 构词：古称平城京，吴语读音书面可简写做“那罗”或者“宁乐”，后演变成读音相同的奈(な)良(ら) 富士山（ふじさん）：富士山 构词：fuji，阿伊努语（日本少数民族阿伊努族的语言），意为火 I-12 チョモランマ：珠穆朗玛峰二、与人身份有关的 日本姓名称为名前（なまえ）（I-8），包括姓氏与名字。日本姓氏 以下是日本姓氏，我参考维基百科上的日本姓氏列表标注了姓氏的人口排名，方便理解姓氏的常见程度。I-1 森（もり），27 林（はやし），19 小野（おの），52 吉田（よしだ），12 田中（たなか），4 中村（なかむら），8I-2 長島（ながしま），328I-6 佐藤（さとう），1 日本人起名时有一些常见的日本名字，列举如下： 太郎（たろう）：长男（I-1） 中国姓氏中国姓氏一般是汉字音读，列举于此；其他外国姓氏一般是片假名，不再列举。I-1 李（り） 王（おう） 張（ちょう）I-8 陳（ちん）称呼、称谓、关系I-1 父（ちち）：父亲 称呼自己的父亲 ～さん：称呼别人 〜ちゃん：称呼小孩、亲密的朋友 〜君（くん）：仅称呼男性（不可用于长辈）I-2 家族（かぞく）：家人，家属 不是广义的家族 母（はは）：母亲 称呼自己的母亲 お母さん（おかあさん）：母亲 称呼别人的母亲 人（ひと）：人，用于构成代词，如あの人 方（かた）：人（礼貌语），用于构成代词，如あの方 称呼长辈、上司 I-3 お〜：名词前（一般是训读）加お会变得更礼貌（不是所有名词都可以加） 子供（こども）：孩子，小孩 构词：～ども是名词后缀表示复数（但在本词中已经失去了复数的意义），假借字供（ども） 兄弟（きょうだい）：兄弟姐妹 不只包括男性（兄弟），还包括女性（姐妹） 仅指亲兄弟姐妹，不包括表的、堂的 称呼自己的兄弟姐妹，称呼别人的用ご兄弟 両親（りょうしん）：父母 称呼自己的父母，称呼别人的用ご両親 妹（いもうと）：妹妹 称呼自己的妹妹，称呼别人的用妹さん 男（おとこ）：男（单纯表示性别） 女（おんな）：女（单纯表示性别）I-4 ご〜：名词前（一般是音读）加ご表示礼貌（不是所有名词都可以加）I-6 友達（ともだち）：朋友 构词：～たち是名词后缀表示复数（但在本词中已经失去了复数的意义），假借字達（たち）。音便：だ变た 弟（おとうと）：弟弟 称呼自己的弟弟，称呼别人的用弟さん I-8 お兄さん（おにいさん）：哥哥 称呼别人的哥哥 ～様（さま）：称呼别人，多用于书面语（如写信时）或称呼天皇等贵族I-9 女性（じょせい）：女性 お客様（きゃくさま）：来宾 构词：様（さま），接在人名、称呼下表示敬意。对客人的敬意称呼 I-12 兄（あに）：哥哥 称呼自己的哥哥 I-16 皆さん（みなさん）：大家 子（こ）：孩子 子（こ）与子供（こども）形式上互为单复数 与子供（こども）的用法区别？ 职业、身份I-1 学生（がくせい）：（大）学生 先生（せんせい）：老师 留学生（りゅうがくせい）：留学生 教授（きょうじゅ）：教授 社員（しゃいん）：职员 表示具体某个公司的人员 会社員（かいしゃいん）：公司职员 表示广义的职业 店員（てんいん）：店员 研修生（けんしゅうせい）：研修生 課長（かちょう）：科长 注意日本称公司或机关的科为课 社長（しゃちょう）：总经理I-2 会社（かいしゃ）：公司I-4 生徒（せいと）：学生 生徒（せいと）一般指初中、高中的学生；学生（がくせい）一般指大学生。 I-9 歌舞伎（かぶき）：歌舞伎（日本的一种戏剧）I-10 観光客（かんこうきゃく）：游客 作家（さっか）：作家 部長（ぶちょう）：部长I-16 旅行会社（りょこうがいしゃ）：旅行社 航空会社（こうくうがいしゃ）：航空公司 営業部（えいぎょうぶ）：营业部 アイティー産業（IT さんぎょう）：IT 产业 建築家（けんちくか）：建筑师三、物品一般物品I-2 本（ほん）：书 かばん：包 构词：即汉字鞄（かばん） ノート（note）：笔记本 是笔记本，不是笔记 鉛筆（えんぴつ）：铅笔 傘（かさ）：伞 靴（くつ）：鞋 新聞（しんぶん）：报纸 是报纸，不是新闻。新闻是 ニュース 雑誌（ざっし）：杂志 辞書（じしょ）：词典 カメラ（camera）：照相机 テレビ（television）：电视机 パソコン（personal computer）：个人电脑 ラジオ（radio）：收音机 電話（でんわ）：电话 机（つくえ）：桌子 椅子（いす）：椅子 鍵（かぎ）：钥匙 还有锁的意思，是由钥匙的含义衍生的。锁本身的单词为錠（じょう） 時計（とけい）：钟，表 注意是“时计”不是“时针” 手帳（てちょう）：记事本 写真（しゃしん）：照片 中文语境下一般指模特的“写真”，是日语传入的词；原日语就是广义的照片。 土産（みやげ）：礼物 原意为土特产，引申为礼物 名産品（めいさんひん）：特产 シルク（silk）：丝绸 ハンカチ（hankerchief）：手绢I-3 エスカレーター（escalator）：自动扶梯 服（ふく）：衣服 コート（coat）：大衣，风衣 デジカメ（digital camera）：数码相机 地図（ちず）：地图I-4 冷蔵庫（れいぞうこ）：冰箱 不是冷库的意思 壁（かべ）：墙壁 スイッチ（switch）：开关 本棚（ほんだな）：书架 ベッド（bed）：床 箱（はこ）：盒子，箱子 眼鏡（めがね）：眼镜 ビデオ（video）：录像机 サッカーボール（soccer ball）：足球 ビール（beer）：啤酒 ウイスキー（whiskey）：威士忌I-7 テニス（tennis）：网球 サッカー（soccer）：足球 野球（やきゅう）：棒球 野，场地。在室外场地打的球。在室外球类运动中棒球最先传入日本，所以最先被叫做野球。 申込書（もうしこみしょ）：申请书 申す，申请；込む，进入？；二者的连用形复合作名词（申込，申し込み，申込み），再与名词（書）复合作名词。 手紙（てがみ）：信 不是上厕所用的卫生纸的意思 シーディー（CD）：CDI-8 プレゼント（present）：礼物 チケット（ticket)：票 パンフレット（pamphlet）：小册子 記念品（きねんひん）：纪念品 注意是“記”，不是“紀” スケジュール表（schedule ひょう）：日程表 写真集（しゃしんしゅう）：摄影集 花（はな）：花 金（かね）：钱 ボールペン（ballpen）：圆珠笔 宿題（しゅくだい）：作业 航空便（こうくうびん）：航空邮件 速達（そくたつ）：快递 ファックス（fax）：传真 メール（mail）：邮件 電話番号（でんわばんごう）：电话号码 新聞紙（しんぶんし）：报纸 新聞紙比新聞更强调实体的纸 紙飛行機（かみひこうき）：纸飞机I-9 温泉（おんせん）：温泉 湯（ゆ）：热水，开水 不是食物汤的意思 仅指热水，不能说 冷たい湯 水（みず）：凉水 仅指凉水，不能说 熱い水 浴衣（ゆかた）：浴衣（一种和服） 薬（くすり）：药 医生开药：薬を出す 紙（かみ）：纸 グラス（glass）：玻璃杯 〜用（よう）：～用的 接在名词后面表示使用者或用途，如子供用，ワイン用のグラス I-10 人形（にんぎょう）：人形（日本的一种人偶） 彫刻（ちょうこく）：雕刻 道具（どうぐ）：工具 含义更广，不是舞台或游戏中的道具 物（もの）：物品，东西I-11 ピアノ（piano）：钢琴 絵（え）：画 ゴルフ（golf）：高尔夫球 コンピューター（computer）：计算机 窓（まど）：窗户 寄木細工（よせぎざいく）：木片儿拼花工艺品（箱根地区的传统工艺品） 寄せる，把东西收集到一起。连用形与其他名词复合作名词 I-13 荷物（にもつ）：包裹，行李，货物 葉書（はがき）：明信片 書く的连用形与名词复合作名词。音便：かく浊化为がく 切手（きって）：邮票 切る的连用形与名词复合作名词。是切符手形（きっぷてがた）的缩写，本意是商品交换券。 引き出し（ひきだし）：抽屉 引く，拉；出す，出来。两个动词的连用形复合作名词 アルバム（album）：相册 写真集指的是出版物，アルバム指家庭一般的相册 タバコ（tobacco）：烟 ボーリング（bowling）：保龄球 髪（かみ）：头发I-14 メモ（memo）：备忘录 ドア（door）：门I-15 ベンチ（bench）：长椅 携帯電話（けいたいでんわ）：手机 クーラー（cooler）：空调I-16 デザイン（design）：设计图，图案 ネクタイ（necktie）：领带 財布（さいふ）：钱包 革（かわ）：皮革 布（ぬの）：布匹 水筒（すいとう）：水壶 アイディーカード（ID card）：身份证 看板（かんばん）：牌子 クリスマスツリー（Christmas tree）：圣诞树 〜製（せい）：〜制品食物I-7 コーヒー（coffee）：咖啡 コーラ（cola）：可乐 茶（ちゃ）：茶 ワイン（wine）：葡萄酒 パン：面包 葡萄牙外来语（pan） ケーキ（cake）：蛋糕 粥（かゆ）：粥 昼ご飯（ひるごはん）：午饭 弁当（べんとう）：盒饭 源于中国南宋时期的俗语“便当”一词，意为方便、容易，传入日本后指代盒饭，用字也发生变化 蕎麦（そば）：荞麦面 指荞麦面条，日本人常吃，一般不指荞麦本身 うどん：面条 指日本的小麦面条，中国称乌冬面。汉字为饂飩（うどん），太难一般不写 親子丼（おやこどん）：滑蛋鸡肉饭 亲子是因为有鸡肉（亲）也有鸡蛋（子）；丼是一种又深又厚的陶瓷大碗，可以在日料店见到 カレー（curry）：咖喱（饭） 卵（たまご）：鸡蛋 玉子也读作たまご，也意为鸡蛋。卵一般指料理前的生鸡蛋，玉子指料理后的熟鸡蛋 チース（cheese）：奶酪 リンゴ：苹果 汉字为林檎（りんご），是中国古代对苹果的称呼 イチゴ：草莓 汉字为苺（いちご） I-8 チョコレート（chocolate）：巧克力 アイスクリーム（ice cream）：冰激凌 小麦粉（こむぎこ）：面粉 箸（はし）：筷子 スプーン（spoon）：勺子I-9 料理（りょうり）：菜肴，饭菜 既可以指桌上的饭菜，也可以指某个国家民族的饮食 四川料理（しせんりょうり）：四川菜 スープ（soup）：汤 北京ダック（Peking Duck）：北京烤鸭 食べ物（たべもの）：食物 食べる的连用形与名词复合作名词 すき焼き（すきやき）：寿喜烧 すき的汉字为鋤，原意是日本农村人把食物放在锄头上烧熟来吃，寿喜烧是音译。焼く的连用形与名词复合作名词 I-10 魚（さかな）：鱼 菓子（かし）：点心I-11 飲み物（のみもの）：饮料 飲む的连用形与名词复合作名词 酒（さけ）：酒 肉（にく）：肉 野菜（やさい）：蔬菜 不是字面野生蔬菜的意思。日语中也有蔬菜一词，但逐渐被野菜代替 果物（くだもの）：水果 ヒマワリ：向日葵 ひ的汉字为日；まわり即周り，转动。 バラ：玫瑰 汉字为薔薇（ばら） I-12 日本料理（にほんりょうり）：日本菜 寿司（すし）：寿司 日本古代写作鮨（すし），寿司是借音字。原意为咸鱼。 ナジ：梨 汉字是梨（なじ） バナナ（banana）：香蕉 ミカン：橘子 汉字是蜜（み）柑（かん） 焼酎（しょうちゅう）：烧酒（日本的蒸馏酒） 日本酒（にほんしゅ）：清酒（日本的酿造酒） 紅茶（こうちゃ）：红茶 ウーロン茶：乌龙茶 汉字为烏龍（うーろん） ジャスミン茶（jasmine ちゃ）：茉莉花茶 緑茶（りょくちゃ）：绿茶 ジュース（juice）：果汁I-13 生ビール（なま beer）：生啤 焼き鳥（やきとり）：烧鸟（日本的鸡肉烧烤） 鳥，指鸡；焼く的连用形与名词复合作名词 唐揚げ（からあげ）：炸鸡 揚げる，油炸。唐指中国，指引进自中国。动词的连用形与名词复合作名词 肉じゃが（にくじゃが）：土豆炖牛肉（日本菜） じゃが是馬鈴薯（じゃがいも）的缩写 I-14 朝ご飯（あさごはん）：早饭 晩ご飯（ばんごはん）：晚饭 バーベキュー（barbecue）：户外烧烤自然界与动植物I-4 猫（ねこ）：猫 犬（いぬ）：狗I-7 パンダ（panda）：熊猫I-9 天気（てんき）：天气 海（うみ）：海 山（やま）：山I-10 紅葉（もみじ/こうよう）：红叶 こうよう表示秋天阔叶林的落叶变红或树叶全都变成红色；もみじ多表示枫树的红叶 シーズン（season）：季节 晴れ（はれ）：晴天 动词晴れる的连用形直接作名词 雨（あめ）：雨 曇り（くもり）：阴天 雪（ゆき）：雪I-12 季節（きせつ）：季节 シーズン可以作广义的“xx季”的意思，而季節只能是春夏秋冬 冬（ふゆ）：冬天 春（はる）：春天I-13 象（ぞう）：大象I-15 火（ひ）：火人体相关I-11 脚（あし）：腿 注意不是脚，脚在日语为足（あし），与脚（あし）同音 I-12 背（せ）：个子，身高I-15 風邪（かぜ）：感冒 熱（ねつ）：发烧I-16 足（あし）：脚 指（ゆび）：手指 目（め）：眼睛 鼻（はな）：鼻子 顔（かお）：脸 頭（あたま）：头，头脑 不仅指头部（head），也指脑子，头脑（mind） 頭がいい：脑子好，聪明 四、时间相关表示时间的名词I-3 今日（きょう）：今天 水曜日（すいようび）：星期三 日本的星期采用西方的星象制，顺序为日月火水木金土（日一二三四五六） 木曜日（もくようび）：星期四I-5 今（いま）：现在 今　何時　ですか：现在几点了？（询问时间）（今为什么不加は？） ～半（はん）：～点半 先週（せんしゅう）：上周 来週（らいしゅう）：下周 再来週（さらいしゅう）：下下周 今週（こんしゅう）：这周 昨日（きのう）：昨天 明日（あした/あす）：明天 明後日（あさって）：后天 一昨日（おととい）：前天 毎朝（まいあさ）：每早晨 毎晩（まいばん）：每晚上 毎週（まいしゅう）：每周 午前（ごぜん）：AM（指 0-12 点） 午後（ごご）：PM（指 12-24 点） 和 AM, PM 一样，不过是加在钟点前面 日曜日（にちようび）：星期日 月曜日（げつようび）：星期一 火曜日（かようび）：星期二 金曜日（きんようび）：星期三 土曜日（どようび）：星期四 今朝（けさ）：今早 今晩（こんばん）：今晚 来年（らんねん）：明年 去年（きょねん）：去年 “毎～” “来～” “今～” 等系列修饰其他时间名词时不加 の，例如 每日七時、来週日曜日 夜（よる）：夜晚 晩（ばん）：晚上 晚（ばん）一般指睡觉前的晚上，夜指天黑的所有时段 朝（あさ）：早晨 〜ごろ：〜（时间）左右 ごろ后面一般不加 に I-6 来月（らいげつ）：下个月 先月（せんげつ）：上个月 夜中（よなか）：半夜，凌晨 昨夜（ゆうべ）：昨晚I-7 午前中（ごぜんちゅう）：上午 午前中指一般的上午，午前是严格的0-12点 I-8 夕方（ゆうがた）：傍晚 方是什么意思？ 昼休み（ひるやすみ）：午休 动词连用形与名词复合作名词 I-13 昼（ひる）：白天，中午节日、场合I-5 展覧会（てんらんかい）：展览会 歓迎会（かんげいかい）：欢迎会 パーティー（party）：派对I-6 コンサート（concert）：音乐会 クリスマス（Christmas）：圣诞节 誕生日（たんじょうび）：生日 子供の日（こどものひ）：儿童节 夏休み（なつやすみ）：暑假 动词连用形与名词复合作名词 I-10 平日（へいじつ）：工作日 日（ひ）：日子I-11 結婚式（けっこんしき）：婚礼 写真展（しゃしんてん）：摄影展 会議（かいぎ）：会议五、地点相关地点I-3 デパート（department store）：百货大楼 食堂（しょくどう）：食堂 郵便局(ゆうびんきょく）：邮局 銀行（ぎんこう）：银行 マンション（mansion）：高级公寓 ホテル（hotel）：宾馆 コンビニ（convenient)：便利店 喫茶店（きっさてん）：咖啡馆 病院（びょういん）：医院 本屋（ほんや）：书店 レストラン（restaurant）：餐馆 レストラン指偏高级的餐厅，食堂指单位、组织的或者大众的快餐店 ビル（building）：大厦 建物（たてもの）：建筑物 売り場（うりば）：柜台 売る，卖；动词连用形与名词复合作名词 トイレ（toilet）：厕所 入り口（いりくち）： 入る，进入；动词连用形与名词复合作名词 事務所（じむしょ）：事务所，办公室 受付（うけつけ）：接待处，前台，传达室 受ける，承接、受理；付ける，连接上；两个动词连用形，意思为受理、接受，引申为接待处 バーゲン会場（bargain かいじょう）：降价大卖场I-4 部屋（へや）：房间 庭（にわ）：院子 家（いえ）：家 居間（いま）：起居室，客厅 場所（ばしょ）：地方，场所 教室（きょうしつ）：教室 図書室（としょしつ）：图书室 公園（こうえん）：公园 花屋（はなや）：花店 売店（ばいてん）：小卖部 駅（えき）：车站 地下鉄（ちかてつ）：地铁I-5 学校（がっこう）：学校 宅（たく）：家（指别人的）I-6 美術館（びじゅつかん）：美术馆 アパート（apartment）：高级公寓 家（うち）：家，家庭 家（うち）带有“我”家的意思，在说“我家”时，家（うち）可加“私の”可不加，而家（いえ）必须加 家（うち）与家（いえ）相比，除了实体的房子，还可以指抽象的家庭、家的情感 プール（pool）：游泳池I-7 蕎麦屋（そばや）：荞麦面馆 動物園（どうぶつえん）：动物园I-8 住所（じゅうしょ）：住址I-10 故郷（こきょう）：故乡 通り（とおり）：大街 动词通る的连用形作名词 町（まち）：街道（日本的行政区划单位） 所（ところ）：场所 与場所（ばしょ）意思一样 店（みせ）：店铺I-11 旅館（りょかん）：旅馆 別荘（べっそう）：别墅I-13 ガレージ（garage）：车库 居酒屋（いざかや）：酒馆（日本特色）I-14 駅前（えきまえ）：车站一带，车站周边 是指车站周围的整个区域，不是字面上车站前面的意思（后者是駅の前） 橋（はし）：桥 角（かど）：拐角 横断歩道（おうだんほどう）：人行横道 交差点（こうさてん）：十字路口 道（みち）：道路 近义词：通り、街（まち） I-15 市役所（しやくしょ）：市政府 風呂（ふろ）：澡堂（日本的泡澡） 本意指洗澡（泡澡），引申为洗澡的地方 お風呂に入る：洗澡 薬局（やっきょく）：药店I-16 展示場（てんじじょう）：展览会场 天井（てんじょう）：天花板方位名词 方位名词与名词组合：名词 + の + 方位名词。I-3 隣（となり）：旁边 离得特别近的周围，任何一个方向都可以 周辺（しゅうへん）：附近 一般都表示大的范围 I-4 上（うえ）：上面 仅指垂直上方的范围，不包括汉语中类似“墙上”的含义 外（そと）：外面 中（なか）：里面，中间 下（した）：下面 前（まえ）：前面 後ろ（うしろ）：后面 近く（ちかく）：附近 形容词近い的连用形作名词 离的比较近的地方，比周辺范围近，但比隣、横等远 I-14 右（みぎ）：右边 左（ひだり）：左边I-16 横（よこ）：旁边 离得特别近的周围，只能是左右、水平方向 交通工具I-2 車（くるま）：车 自転車（じてんしゃ）：自行车 転（てん），即中文汉字“转”的日本写法。 I-4 ジェーアール（JR）：日本铁道（Japanese Railways）I-6 交通機関（こうつうきかん）：交通工具 新幹線（しんかんせん）：新干线（属于 JR） 飛行機（ひこうき）：飞机 フェリー（ferry）：轮渡 電車（でんしゃ）：电车 バス（bus）：公交车 タクシー（taxi）：出租车I-10 自動車（じどうしゃ）：汽车I-14 船便（ふなびん）：海运I-15 ボート（boat）：小船 特指用桨划的小船 六、兴趣爱好类抽象名词I-7 音楽（おんがく）：音乐 映画（えいが）：电影I-11 歌（うた）：歌曲 カラオケ：卡拉OK ロック（rock）：摇滚乐 ポップス（pops）：流行音乐 クラシック（classic）：古典音乐 スポーツ（sports）：运动 水泳（すいえい）：游泳I-13 漫画（まんが）：漫画I-14 太極拳（たいきょくけん）：太极拳七、其他抽象名词I-1 出迎え（でむかえ）：迎接 两个动词出る、迎える的连用形复合作名词 I-4 一人暮らし（ひとりぐらし）：离开家一个人生活 暮らす，生活，度日，不是字面意思 “行将暮年”。动词连用形与名词复合作名词 I-5 仕事（しごと）：工作 休み（やすみ）：休息 动词休む的连用形作名词 I-8 件（けん）：事情I-9 眺め（ながめ）：风景 ニュース（news）：新闻 気持ち（きもち）：心情 気，精神；持つ，保持。动词连用形与名词复合作名词 I-10 作品（さくひん）：作品 修学旅行（しゅうがくりょこう）：修学旅行（日本中小学教育的一部分） 生活（せいかつ）：生活 世界（せかい）：世界 ～中（じゅう）：整个，全（表示地点范围的全部） 例：世界中（全世界），日本中（全日本），学校中（全校） 一（いち/ひと）〜中（じゅう）：在〜一直（表示时间范围的全部） 例：一日中（一整天），一年中（全年） I-11 模様（もよう）：花纹，图案I-12 席（せき）：座位 クラス（class）：班级 種類（しゅるい）：种类I-13 他（ほか）：其他，另外I-14 書類（しょるい）：文件 原稿（げんこう）：草稿 記事（きじ）：报道 電気（でんき）：电，电力I-15 禁煙（きんえん）：禁止吸烟 睡眠（すいみん）：睡眠 睡眠を取る：睡觉 気（き）：精神，意识 気を付ける：注意，打起精神 打ち合わせ（うちあわせ）：事先商量，碰头，接洽 打つ，打；合わせる，会面；两个动词的连用形复合作名词 無理（むり）：勉强 不是字面意思不讲理或没有理由 無理をする：行为超过了体力活经济能力的界限，可能给身体或事态的发展造成负面后果 駐車禁止（ちゅうしゃきんし）：禁止停车 立入禁止（たちいりきんし）：禁止进入 火気厳禁（かきげんきん）：严禁烟火 撮影禁止（さつえいきんし）：禁止拍照I-16 機械（きかい）：机器 製品（せいひん）：产品 形（かたち）：造型，形状 緑（みどり）：绿色 間違い（まちがい）：错误 間，？違う，不一致，违反；动词连用形与名词复合作名词 問題（もんだい）：问题 広告（こうこく）：广告 入場料（にゅうじょうりょう）：门票，入场费 サービス（service）：服务 警備（けいび）：警备，戒备 〜費（ひ）/〜料（りょう）/〜代（だい）：〜费用 日语的xx费有三种表达形式，没有一定的规律，需要自行积累 代词I-1 私（わたし）：我（第一人称代词） あなた：你（第二人称代词） 一般不直接用あなた（会显得不礼貌），而是用～さん，除了不知道对方姓名又必须招呼的时候 あの人（あのひと）：他，那个人（第三人称代词）I-2 これ/それ/あれ/どれ：这/那/那/哪（指示代词） 日语中指示代词不仅有近称（这）和远称（那），还有中称 どれ用于提问在三个以上的事物中选一个，相当于 which；而何（なん）相当于 what これ/それ/あれは　〜です：这/那是～。 ～は　どれ ですか：～是那个？ 何（なん/なに）：什么 可以用于提问任何句子成分 一般情况读なに；在助词の和量词前读なん；在助词で前二者皆可 これは　何 ですか：这是什么？ 誰（だれ）：谁 あの人は　誰 ですか：那个人是谁？ どなた：哪位 是誰（だれ）的敬体，用于尊长和地位较高的人 いくつ：多少岁 相当于何歳（なんさい） おいくつ/何歳　ですか：多大年纪了？ I-3 ここ/そこ/あそこ/どこ：这里/那里/哪里 ここ/そこ/あそこは　〜(地点）です：这里/那里是～(地点）。 ～は　どこですか：～在哪里？ こちら/そちら/あちら/どちら：这里/那里/哪里（以上的敬体） どちら口语为どっち いくら：多少钱（提问价格） ～は　いくらですか：〜多少钱？ I-5 いつ：什么时候 询问具体的时间使用“何〜に”，如何時に、何日に いつ　～（动词连用形）ますか：～在什么时候？（询问动作的时间） いつから/まで　～（动词连用形）ますか：～在什么时候？（询问动作的时间范围） I-11 僕（ぼく）：我 只用于男性 对关系比较密切的人的自称，不能用于长辈或正式场合 数量词数词 数字有很多，这里只列举基本的表示数的汉字和组合成数的规则，以及特殊用法。表示数的汉字I-2 零：れい（汉音），ゼロ（zero） 一：いち（吴音） 二：に（吴音） 三：さん（吴音、汉音） 四：し（吴音、汉音），第二读音：よん（训读） 五：ご（吴音、汉音） 六：ろく（吴音） 七：しち（吴音），第二读音：なな（训读） 八：はち（吴音） 九：く（吴音），第二读音：きゅう（汉音） 十：じゅう（吴音） 百：ひゃく（吴音） 点：てん（音读） 分：ふん（汉音）I-3 千：せん（吴音、汉音） 万：まん（音读惯用音） 億：おく（音读）组合成数的规则以下～代表一到九： 10-19：和中文习惯相同，十～； 20-99：和中文习惯相同，～十～（十位数用第二读音）； 100-9999：和中文习惯大致相同，～千～百～十～（百位数、十位数用第二读音），不同之处： “～0～”读 ～百～ 而不读 ～百零～； 100, 1000 前面不加一，直接：百、千； 10000及以上：和中文习惯大致相同，不同之处： 1000000 前面不加一，直接：百万； 小数：和中文习惯相同，～点（てん）～ 分数：和中文习惯相同，～分（ふん）の～：～分之～发生音便的： 三百（〜びゃく） 六百（ろっぴゃく） 八百（はっぴゃく） 三千（〜ぜん） 八千（はっ〜） 一千万（いっ〜） 八千万（はっ〜）量词 日语的量词和前面的数次搭配使用时，有时会有多种发音的情况，书中《附录 IV：数量词搭配使用表》详细列举了 20 以内的所有情况。这里我只把这些特殊的读音列举出来。 与量词搭配使用时，数词一般都读第二读音。I-1 第（だい）～課（か）：第～课 音便：第一課（〜いっ〜）、第六課（～ろっ〜）、第八課（〜はち/はっ〜）、第十課（〜じゅっ〜） I-2 〜歳（さい）：～岁 特殊读音 二十歳（はたち）： 音便：一歳（いっ〜）、八歳（はっ〜）、十歳（じゅっ〜） I-3 ～階（かい）：～层 音便：一階（いっ〜）、三階（〜がい）、六階（ろっ〜）、八階（はち/はっ〜）、十階（じゅっ〜） 〜円（えん）：〜日元 音便：四円（よ〜） I-5 ～年（ねん）：〜年（表示时刻） 特殊读音： 七年（しち/なな〜） 音便：四年（よ〜） 〜月（がつ）：〜月（表示时刻） 特殊读音 四月（し〜）、七月（しち〜）、九月（く〜）：全部使用第一读音 〜日（にち）：〜日（表示时刻） 特殊读音 一日（ついたち）： 二日（ふつか）、三日（みっか）、五日（いつか）、六日（むいか）、七日（なのか）、八日（ようか）、九日（ここのか）、十日（とおか）：使用数字的其他训读音，日使用训读音か 二十日（はつか）： 音便：四日（よっか）、十四日（じゅうよっか） 〜時（じ）：〜点钟，～时（表示时刻） 特殊读音 七時（しち〜）、九時（く〜）：使用第一读音 音便：四時（よ〜） ～分（ふん）：〜分（表示时刻） 音便：一分（いっぷん）、三分（〜ぷん）、四分（〜ぷん）、六分（ろっぷん）、八分（はっぷん）、十分（じゅっぷん）、何分（〜ぷん） I-12 〜年間（ねんかん）：〜年（表示时间段） 音便：四年（よ〜） I-13 〜か月（げつ）：〜个月（表示时间段） 音便：一か月（いっ〜）、六か月（ろっ〜）、八か月（はち/はっ〜）、十か月（じゅっ〜） 〜週間（しゅうかん）：〜个周（表示时间段） 音便：一週間（いっ〜）、八週間（はっ〜）、十週間（じゅっ〜） 〜時間（じかん）：〜个小时（表示时间段） 特殊读音 七時間（しち/なな〜）、九時間（く〜） 音便：四時間（よ〜） 〜回（かい）：〜次 音便：一回（いっ〜）、六回（ろっ〜）、八回（〜はち/はっ〜）、十回（〜じゅっ〜） ～度（ど）：～次 ～番（ばん）：第～ ～人（にん）：〜个人 特殊读音 一人（ひとり）： 二人（ふたり）： 七人（しち/なな〜）： 音便：四人（よ〜） 〜つ：～个 十及以上不加量词つ 特殊读音 一つ（ひと〜）、二つ（ふた〜）、三つ（みっ〜）、五つ（いつ〜）、六つ（むっ〜）、八つ（やっ〜）、九つ（ここの〜） いくつ： 音便：四つ（よっ〜） 〜個（こ）：～个 音便：一個（いっ〜）、六個（ろっ〜）、八個（はち/はっ〜）、十個（じゅっ〜） 〜冊（さつ）：〜本（书，本子） 音便：一冊（いっ〜）、八冊（はっ〜）、十冊（じゅっ〜） 〜皿（さら）：〜盘 特殊读音 一皿（ひと〜）、二皿（ふた〜） 音便：八皿（はち/はっ〜）、十皿（じゅっ〜） 〜足（そく）：〜双（鞋，袜子） 音便：一足（いっ〜）、八足（はっ〜）、十足（じゅっ〜） 〜台（だい）：〜辆 〜着（ちゃく）：〜件（衣服） 音便：一着（いっ〜）、八着（はっ〜）、十着（じゅっ〜） 〜頭（とう）：〜头（任何大动物） 音便：一頭（いっ〜）、八頭（はっ〜）、十頭（じゅっ〜） 〜杯（はい）：〜杯 音便：一杯（いっぱい）、三杯（〜ばい）、六杯（ろっぱい）、八杯（はっぱい）、十杯（じゅっぱい）、何杯（〜ばい） 〜匹（ひき）：〜匹（任何小动物） 音便：一匹（いっぴき）、三匹（〜びき）、六匹（ろっぴき）、八匹（はっぴき）、十匹（じゅっぴき）、何匹（〜びき） 〜本（ほん）：〜支（任何细长物品） 音便：一本（いっぽん）、三本（〜ぼん）、六本（ろっぽん）、八本（はっぽん）、十本（じゅっぽん）、何本（〜ぼん） 〜枚（まい）：〜枚（任何薄平物品） 〜羽（わ）：〜只（鸟，兔子） 音便：八羽（はっぱ）、十羽（じゅっぱ/じゅうわ） 〜キロ（kilo）：〜千克 音便：六キロ（ろっ〜）、八キロ（はち/はっ〜）、十キロ（じゅっ〜） 动词一类动词与二类动词I-4 ある/いる：（自五/自上一）在 表示事物的存在，ある用于物体，いる用于人或动物 ～（地点）に　〜が　ある/いる：〜在～（地点） ～は　〜（地点）に　ある/いる：〜在～（地点） 表示在某处发生某事（I-11） ～（地点）で　〜（事件）が　ある/いる：〜（事件）在～（地点） 表示有，所有（I-14） 〜は　〜が　ある/いる：〜有～ I-5 休（やす）む：（自五）休息 働（はたら）く：（自五）工作 始（はじ）まる：（自五）开始 終（お）わる：（自五）结束 ～（动作性名词）は　～（时刻）に/から　始まる：～（动作）在/从～（时刻）开始 ～（动作性名词）は　〜（时刻）に/まで　終わる：～（动作）在/到～（时刻）结束 起（お）きる：（自上一）起床 寝（ね）る：（自下一）睡觉I-6 行（い）く：（自五）去（移动动词） 帰（かえ）る：（自五）回来（移动动词） 来（き）る：（カ变，自动词）来（移动动词） ～（地点）へ　行く/帰る：去～/回到～ ～（地点）から ～（地点）まで 行く/帰る：从～去/回到～ ～（地点）から　来る：从～来 ～（交通工具）行く/帰る/来る：乘坐～去/回到/来 I-7 飲（の）む：（他五）喝 買（か）う：（他五）买 撮（と）る：（他五）拍照 書（か）く：（他五）写 読（よ）む：（他五）读 聞（き）く：（他五）听 食（た）べる：（他上一）吃 見（み）る：（他上一）看，seeI-8 上（あ）げる：（他下一）给（授受动词），give 与える、やる的敬体丁宁体 A（人）は　B（人）に　〜（物品）を　あげる：A给B～ もらう：（他五）得到（授受动词），receive 给予对象如果是人，一般用に；如果是组织或团体，一般用から A（人）は　B（人）に/から 〜（物品）を　もらう：A从B得到～ 送（おく）る：（他五）送，寄（授受动词），send A（人）は　B（人）に/から 〜（物品）を　送る：A寄给B～ 会（あ）う：（自五）见，会面，meet ～（人）に 会う：会见～ 作（つく）る：（他五）做，制造，make ～（工具）で ～（东西）を 作る：用～做～ 太（ふと）る：（自五）胖（不是变胖） 出（だ）す：（他五）寄（信） 届（とど）く：（自五）到达，送到 注意是自动词 書（か）く：（他五）画 貸（か）す：（他五）借给，借出（授受动词），lend 注意不是字面意思：贷款 A（人）は　B（人）に 〜（物品）を　貸す：A借给B～ 借（か）りる：（他上一）借（授受动词），borrow 与貸す互为反义词，记忆口诀：“贷出借入” A（人）は　B（人）に 〜（物品）を　借りる：A向B借～ 習（なら）う：（他五）学习，learn 侧重非自学、向他人/老师学习 かける：（他下一）打（电话） ～（人）に/へ 電話を　かける：给～打电话 教（おし）える：（他下一）教I-11 分（わ）かる：（自五）懂，明白，know ～が　分かる：懂〜 迷（まよ）う：（自五）犹豫，难以决定；迷路 迷いますね：还真不好选啊 〜（地点）で　迷う：在〜迷路 出来（でき）る：（自上一）会，能 〜が　出来る：会〜 閉（し）める：（他下一）关门 疲（つか）れる：（自下一）感到累，疲倦I-12 降（ふ）る：（自五）下（雨，雪） ～（雨，雪）が　分かる：下〜（雨，雪） I-13 掛（か）かる：（自五）花费（时间、金钱），cost 注意是自动词 ～が　掛かる：花费〜 咲（さ）く：（自五）开（花） 花が　咲く：开花 泳（およ）ぐ：（自五）游泳 遊（あそ）ぶ：（自五）玩 吸（す）う：（他五）吸（烟） 切（き）る：（他五）剪，切I-14 通（とお）る：（自五）通过，经过（移动动词） ～（地点）を 通る：从～（地点） 经过 注意这里的を不是提示宾语，通る是自动词 急（いそ）ぐ：（自五）急 飛（と）ぶ：（自五）飞 死（し）ぬ：（自五）死 待（ま）つ：（他五）等待 売（う）る：（他五）卖 話（はな）す：（他五）说 说xx语要用助词で表示方式，不能用在を的宾语里 渡（わた）る：（自五）过（桥，河）；穿过（马路）（移动动词） ～（地点）を 渡る：过～（地点） 下（お）ろす：（他五）取出，卸货 選（えら）ぶ：（他五）选择 消（け）す：（他五）关（灯）；消除，去除 歩（ある）く：（自动）步行（移动动词） 曲（ま）がる：（自五）拐弯，曲折 洗（あら）う：（他五）洗 出（で）る：（自下一）离开（移动动词） ～（地点）を 出る：离开～（地点） 出（で）かける：（自下一）外出，出门（移动动词） 出る侧重于描述简单的动作，从门里出来；出かける侧重于描述持续的动作，出远门。 開（あ）ける：（他下一）打开 過（す）ぎる：（自上一）过，经过（移动动词） 通る是具体的方位上的通过；過ぎる既可以表示具体的方位，也可以表示时间和程度上的超过了标准 見（み）せる：（他下一）给…看，出示，show 注意与見る区分 〜（人）　～を 見せる：给〜（人）看〜 点（つ）ける：（他下一）开（灯） 降（お）りる：（自上一）下（车，山）（移动动词） 注意与降る区分 I-15 乗（の）る：（自五）乘坐 〜（交通工具） 乗る：乘坐〜 使（つか）う：（他五）用，使用 座（すわ）る：（自五）坐 〜 座る：坐到〜 入（はい）る：（自五）进入 申（もう）す：（他五）是；说，讲，告诉 作“说”一意时，是言う的自谦语 〜（人）と　申す：我是〜（可用于打电话介绍自己） 取（と）る：（自五）取，取得 歌（うた）う：（他五）唱 伝（つた）える：（他下一）传达，说，转告 止（と）める：（他下一）停止I-16 持（も）つ：（自他五）有，拥有，持有 住（す）む：(自五）住 知（し）る：（他五）知道，了解 对知っていますか的回答不能是 知っていません，而是知りません 直（なお）す：（他五）改，改正 片付（かたづ）ける：（他下一）收拾，整理 是口语化的表达，整理地偏随意一下；整理する更正式，整理地更精致 サ变复合动词（三类动词） サ变复合动词是动作性名词衍生的，我把动作性名词一起混放在了这里，意味着可以通过～する形成サ变复合动词。I-5 試験（しけん）：（他）考试 也有实验的意思，但考试的意思不要忘了 遅刻（ちこく）：（自）迟到 出張（しゅっちょう）：(自）出差 研修（けんしゅう）：（他）进修，培训 旅行（りょこう）：（自）旅行 勉強（べんきょう）する：（自他）学习，study 是“学习”最常用的一个词，自带努力和坚持的意思 I-7 ジョギング（jogging）：（自）慢跑 掃除（そうじ）する：（他）打扫 する：（サ变，自他）做，doI-11 運転（うんてん）：（自他）开车 不是字面意思：运转 散歩（さんぽ）する：（自）散步I-13 修理（しゅうり）：（他）修理I-14 買い物（かいもの）する：（自）买东西 卒業（そつぎょう）する：（自）毕业～を 卒業する：从～毕业 食事（しょくじ）する：（自）吃饭 食事する强调吃饭吃菜到喝汤这一开始到结束这一系列动作的总和，食べる只强调吃这一行为 食事する是自动词，食べる是他动词 整理（せいり）する：（他）整理 コピー（copy）する：（他）复制I-16 操作（そうさ）：（他）操作 設計（せっけい）：（他）设计 練習（れんしゅう）する：（他）练习 結婚（けっこん）する：（自）结婚 安心（あんしん）する：（自）放心，安心形容词一类形容词I-9 辛（から）い：辣的，咸的 有辣的和咸的两种意思，需要区分时用塩辛い、塩っぱい 甘（あま）い：甜的 塩辛（しおから）い**/**塩（しょ）っぱい：咸的 后者只用于口语 酸（す）っぱい：酸的 苦（にが）い：苦的 美味（おい）しい：好吃的 不味（まず）い：不好吃的 熱（あつ）い：热的，烫的（指物体） 冷（つめ）たい：凉的（指物体） 楽（たの）しい：快乐的 面白（おもしろ）い：有趣的 构词法：字面意思，脸变亮了；引申为表示惊喜，有趣的 つまらない：无聊的 构词法：汉字为詰（つ）まらない，动词詰まる的未然形接否定助动词ない，演化为形容词 広（ひろ）い：宽广的 狭（せま）い：狭窄的 大（おお）きい：大的 小（ちい）さい：小的 忙（いそが）しい：忙碌的 いい：好的 有时也表示喜欢（情感形容词） 悪（わる）い：不好的 有时也表示不喜欢，相当于（情感形容词） 〜が　いい/悪いです：〜好/不好；喜欢/不喜欢～” 素晴（すば）らしい：极好的，厉害的 遠（とお）い：远的 近（ちか）い：近的 高（たか）い：高的 低（ひく）い：低的 安（やす）い：便宜的 不是字面意思：安全的 寒（さむ）い：寒冷的（指天气） 暑（あつ）い：炎热的（指天气） 与熱（あつ）い同音，汉字不同 青（あお）い：蓝色的 不是绿色，参考中国古话“青出于蓝而胜于蓝” 白（しろ）い：白色的 新（あたら）しい：新的 古（ふる）い：旧的 難（むずか）しい：难的 易（やさ）しい：容易的 多（おお）い：多的 不能直接作连体修饰语修饰名词，形容名词多要用たくさんの〜 少（すく）ない：少的 不能直接作连体修饰语修饰名词 可愛（かわい）い：可爱的I-10 汚（きたな）い：脏的I-11 怖（こわ）い：害怕（情感形容词） ～は　〜が　怖い　です：〜害怕～” 赤（あか）い：红色的 痛（いた）い：疼的I-12 若（わか）い：年轻的 暖（あたた）かい：暖和的（指天气） 涼（すず）しい：凉爽的（指天气） 速（はや）い：快的I-14 暗（くら）い：黑暗的I-15 温（あたた）かい：温的（指物体）I-16　　　　　　 明（あか）るい：明亮的 長（なが）い：长的 短（みじか）い：短的 軽（かる）い：轻的 優（やさ）しい：温柔的，和蔼的，体贴的 細（ほそ）い：细的 太（ふと）い：胖的 黒（くろ）い：黑的二类形容词 这里一并把可以构成二类形容词的名词混放在了这里，意味着可以通过～だ形成二类形容词。为了方便，所有的二类形容词都省略だ。I-10 綺麗（きれい）：漂亮的 有名（ゆうめい）：有名的 賑やか（にぎやか）：热闹的 静か（しずか）：安静的 暇（ひま）：空闲的 親切（しんせつ）：热情的 好き（すき）：喜欢（情感形容词） 嫌い（きらい）：不喜欢，讨厌（情感形容词） ～は　〜が　好き/嫌いだ：〜喜欢/不喜欢～” 便利（べんり）：方便的 不便（ふべん）：不方便的 元気（げんき）：健康的，有精神的 簡単（かんたん）：简单的 ハンサム（handsome）：帅的I-11 上手（じょうず）：擅长；水平高 下手（へた）：水平低 苦手（にがて）：不擅长 ～が　上手/下手/苦手だ：擅长/不擅长/水平高/水平低〜 I-12 人気（にんき）：受欢迎 大好き（だいすき）：非常喜欢（情感形容词），用法同好きI-14 大変（たいへん）：不得了，严重；够受的 表示事态糟糕、严重，也可以表示人的状态辛苦、不容易 I-15 大丈夫（だいじょうぶ）：没关系 だめ：不可以I-16 ユニーク（unique）：独特的 安全（あんぜん）：安全的 派手（はで）：花哨的，耀眼的 地味（じみ）：朴素的，质朴的 厳重（げんじゅう）：严格的，森严的 不是字面意思：事态严重 真面目（まじめ）：认真的，严肃的 大切（たいせつ）：重要的，珍贵的 複雑（ふくざつ）：复杂的连体词I-2 この/その/あの/どの：这个/那个/那个/哪个I-10 どんな：什么样的（提问形容词） 何の也可以作提问，区别为 何の 提问的是内容和材料，どんな 只用于提问性质 ～は　どんな〜　ですか：〜是什么样的～？ 副词程度副词 程度副词形容谓语（形容词或动词）的程度。I-1 どうも：非常，很I-9 あまり：不太，不很（表示程度不高） 修饰的谓语必须使用否定形式 也是频率副词 とても/大変(たいへん）：很，非常，严重 少し（すこし）/ちょっと：有点儿 全然（ぜんぜん）：一点儿也不 修饰的谓语必须使用否定形式 也是频率副词 I-14 なかなか：相当，很，非常 含有实际情况比自己预想的程度要高的含义 含有评价对方的含义，不能用于上级或长辈，不礼貌 I-16 随分（ずいぶん）：相当，很，非常 内容上无论是好是坏，用于事态的程度大幅度超过了说话人自身或者一般性的评判标准 频率副词 频率副词形容动词动作发生的频率。I-5 いつも：总是I-9 あまり：不经常，不怎么 全然（ぜんぜん）：总是不I-11 時々（ときどき）：有时，时不时 々是叠字符号，与中文相同，重复了前面的字：時 音便：第二个時（とき）要浊化 よく：经常 たまに：偶尔，很少 注意比あまり频率高一点 时间副词I-7 これから：从现在起，今后I-8 もう：已经；马上，就要（##### I-14） “已经”用过去时；“马上”用现在时 さっき：刚才 たった今（たったいま）：刚刚 さっき和たった今的谓语要用过去式 たった今比さっき还要更“刚才”，接近于现在 さっき和たった今都是比较随意的说法，用于口语 前に（まえに）：以前 に有时可以不用 後で（あとで）：之后，过会儿I-16 すぐ：马上，立即其他副词I-6 確（たし）か：好像是，大概；的确 用在句子开头 表示不完全有把握的记忆：我记得是 まっすぐ：径直，笔直 用于形容移动动词的直接性，如直接回家：まっすぐ　帰る 一緒に（いっしょに）：一起 名词接格助词，副词化了 I-7 もう一度（もういちど）：再一次I-9 本当に（ほんとうに）：真的，实在是 ちょうど：正好，恰好I-10 どう：如何 如何（いかが）：如何 いろいろ：各种各样I-11 また：还，再，又 どうして：为什么 回答要用接续助词から结句 どうして　（ですか）：为什么？ I-12 一番（いちばん）：第一，最 必须有比较的范围，不像汉语一样可以用于不明确比较范围的“最”。提示范围的助词为で ずっと：〜得多 やはり/やっばり：果然；最后还是 表示某信息或事态的发展与自己的预测一致，意思是“正如自己预料的那样” 表示经过一番考虑，最终确定了某个想法 やっぱり是やはり较随便的说法 I-13 大体（だいたい）：大约，大概 修饰数量词 とりあえず：暂且 从具有可能性的几种动作或事项中，暂且选择一种先做，暗示其他动作或事项还会随后进行 例如在饭店点菜时，先点一些的时候 I-14 こう/そう/ああ：这样/那样/那样 指示副词，指示状态、动作，相当于これ/それ/あれ的副词形式 不定称为どう I-15 勿論（もちろん）：当然，不用说 常用于回答对许可的询问：〜ても　いいですか，可以简化为：もちろんです 十分（じゅうぶん）：充足地 ゆっくり：好好地，安静地，不着急地 表示劝别人安稳地、从容地、好好地把事情做完，不要着急 I-16 まだ：还，尚 まだ一般要接谓语否定形式，表示还没有达到所询问的内容的程度，まだ本身没有否定的意思 注意まだ和また是两个词 ちゃんと：〜好，好好地 只用于口语，表示把一件事搞好：〜好，例如收拾好 副词转用的名词I-9 たくさん：很多I-12 最近（さいきん）：最近 只能表示过去的时间，不能表示将来 I-16 最新（さいしん）：最新连词I-7 じゃあ/では：那么，so 接过别人的话题来发表自己的看法，或中断谈话时使用 じゃあ比较随便，では正式 I-10 でも：表转折关系，但是，but 一般只用于口语 そして：表并列关系，而且，and ところで：哎，那个 转换话题时使用 I-11 だから/ですから：所以，因此，so 与接续助词から意义相同 ですから是だから的敬体丁宁体 I-14 それから：然后感叹词I-1 はい：是的（应答） いいえ：不是的（应答） あっ：哎，哎呀 吃惊或有所感触时 人们在紧急情况下一般情不自禁发出此声音，自言自语时也用 I-2 えっ：啊 对对方所说的话感到意外或吃惊时 没听懂或没明白对方所说的话时的反问（重复使用不礼貌） わあ：哇 感动或吃惊时 ええ：嗯，是的（应答） 是はい的比较随便的说法 注：在别人叫到自己的名字而应答时，必须用はい，不能用ええ I-3 あのう：请问，不好意思 用于向对方搭话或欲引起对方注意（相当于 Excuse me） 在说到难以启齿的事情，开始一个新话题以及向别人提出请求 I-4 ええと：啊，嗯 被别人问及某事，思考该如何回答时 I-9 あら/あれ：哎呀，咦 感到惊讶或疑惑不解时使用的词语 あら为女性用 ああ：啊（应答） 对对方的提问或建议作出肯定性回答； 仅仅作为搭腔语使用； 与はい和ええ相比，其搭腔的语气比肯定的语气要重 I-10 あれえ：咦 感到吃惊或意外而情不自禁发出的声音，由“あれ”音变而来，读升调 I-11 うーん：嗯……，这个…… 用于表示在考虑怎么回答 I-15 もしもし：喂？ 打电话使用的第一句话 いえ：不是的（应答） いいえ的简化说法 I-16 ほら：你看，瞧 用于提醒别人注意 固定搭配I-1 はい、そうです：是，是这样的。 使用场合：回答一般疑问句 语法分析：はい是叹词，作独立语； いいえ、違います：不，不是的。 使用场合：回答一般疑问句 语法分析：いいえ是叹词，作独立语；动词違う的连用形違い接敬语助动词ます，ます用终止形ます表示谓语结句。 分かりません：不知道 使用场合：回答一般疑问句 语法分析： こんにちは：你好 使用场合： 语法分析： すみません：对不起；请问 使用场合 道歉； 提示对方，请求对方做某事（相当于 Excuse me） 家人或关系密切的朋友之间不怎么使用 变形： どうも　すみません すみませんが、〜（##### I-14）：只有请求的意思，后面接请求对方的事情 语法分析： 初めまして：初次见面。 使用场合：自我介绍完毕后 语法分析：动词初める的连用形接敬语助动词ます，ます的连用形接接续助词て，与后面的谓语构成对等文节。 よろしく　お願いします：请多关照。 使用场合：自我介绍完毕后 变形： どうぞ　よろしく どうぞ　よろしく　お願いします お願いいたします 语法分析：动词お願いする的连用形接敬语助动词ます，ます用终止形ます表示谓语结句。一类形容词よろ（宜）しい的连用形作连用修饰语修饰谓语。 こちらこそ～：我才要（请您～） 使用场合：后接　よろしく　お願いします，作为对方自我介绍后的回应 语法分析： I-2 ありがとう　ございます：非常感谢。 变形： どうも　ありがとう　ございます（加强谢意） どうも（略表谢意） 语法分析：一类形容词ありがたい的连用形ありがたく接补助动词〜ある的丁宁语敬体ござる；ござる的连用形ござい接敬语助动词ます，ます用终止形ます表示谓语结句；发生一类形容词的う音便：ありがたく变为ありがとう。 どうぞ：请 使用场合： 用于给对方物品，或劝对方进餐； 肯定回答对许可的询问：〜ても　いいですか ～ですか：是～ 使用场合： 升调用于进一步确认对方所说的信息 降调将对方所说的内容重复一遍，表示理解了对方所说的内容 I-5 おはよう：早上好I-6 お疲れ様でした：够累的 使用场合：同伴之间体谅、安慰对方使用的词语；公司工作结束后的寒暄用语 语法分析：动词疲れる的连用形疲れ作连体修饰语，修饰様。お疲れ様接断定助动词です，です的连用形でし接过去助动词た，助动词た用终止形た表示谓语结句 お先に失礼します：我先告辞了 使用场合：下班时先离开的对还留在公司里工作的人说的；向长辈或上司道别；进别人的房间 变形： 失礼します 失礼しました 语法分析： 大変ですね：真不容易 使用场合：表示对对方的同情 语法分析：大変だ是二类形容词，意为严重的、重大的。 I-7 ～を　ください：请给我～ 使用场合：买东西或点菜时；既可以用于花钱，也可用于不花钱的索取。不可用于年长的、地位高的人 语法分析： 授受动词くださる（请给）是くれる（给）的敬语，其命令形ください（不接助动词）表示命令语气； 按照规则，くださる是五段活用动词，其命令形应为くだされ，但现代日语用的是ください，原理是： くださる的连用形ください接敬语助动词ます 其命令形くださいませ表示命令语气 省略ませ得到ください ～を作连用修饰语，表示他动词くださる给动作的对象 そうですか：是吗 使用场合： 升调用于对对方所说内容持有疑问：是吗？ 降调表示理解了所听到的新信息：是嘛 そうですね：好啊 使用场合： ね发音不拉长，用于同意对方的提议 ね发音拉长，表示说话人在思考接下来要说的内容（I-11） いってまいります：我走了 使用场合：离开家或公司时，以还要返回为前提 变形： いってきます（较为随便的说法） 语法分析： いってらっしゃい：你走好，你去吧 使用场合：回复我走了：いってまいります/いってきます。含有说话人盼望对方早点回来的心情 语法分析： ただいま：我回来了 使用场合：回到家或单位，回来的人说 语法分析： お帰りなさい：你回来啦 使用场合：回到家或单位，家或单位的人说 语法分析： いらっしゃいませ：欢迎光临 使用场合：顾客进门时店员对顾客的寒暄语 变形： いらっしゃい（较为随便的说法，常用与亲朋好友来家里做客） 语法分析： かしこまりました：我知道了 使用场合：店员对顾客，语气非常郑重 语法分析： お邪魔します：打扰了 使用场合： 语法分析： I-8 ～（名词）+（を）＋　お願いします：～拜托了 使用场合：请求对方做某事 语法分析： 分かりました：明白了；好的 使用场合： 表示理解了对方所做的说明 对对方说的话表示承诺或应答 语法分析： 良かったです：太好了 语法分析： I-9 気持ちがいい：感觉舒服 使用场合： 语法分析：省略了助动词です I-10 ～どうですか：～怎么样？ 使用场合： 提问对方对某状态的意见或感想； 劝诱对方进行某动作，提出某种建议或推荐某物（##### I-11） 变形： どうですか どう ～どうでしたか 如何ですか（礼貌用法，对长辈或上级使用） 语法分析：对作谓语的形容词进行提问 I-11 結構です：不用了 使用场合：表示对〜どうですか提议的委婉拒绝 变形： 今は　結構です 语法分析： （〜が）　気に　入りました：喜欢 使用场合：第一次见到某事物而感到中意，不表示原有的持续性爱好（后者要用好き） 语法分析： I-12 人気が　あります：受欢迎 使用场合： 语法分析： I-13 どのぐらい　かかりますか：大约需要花多长时间/多少钱？ 变形： どれぐらい　かかりますか 语法分析： I-14 〜（动词て形） ください：请做〜 使用场合：请求某人做某事 变形： 〜（动词て形） くださいませんか：更为礼貌的形式 そうして　ください：省略上文的谓语 语法分析：て是接续助词，接续动词连用形。起到与补助动词ください的连接作用。补助动词～てくれる（表示对方给予自己的动作）的敬体是～てくださる，属尊敬语。通常使用命令形～てください，用于表命令的语句。 I-15 いいですよ：可以的 使用场合：肯定回答对许可的询问：〜ても　いいですか 语法分析： かまいません：没关系，不要紧 使用场合：肯定回答对许可的询问：〜ても　いいですか 语法分析： いえ、ちょっと…：不，这个有点… 使用场合：否定回答对许可的询问：〜ても　いいですか 语法分析： いいえ、いけません：不，不可以 使用场合：否定回答对许可的询问：〜ても　いいですか（通常是根据法律、规章、社会惯例的客观不可以） 语法分析： お大事に：请多保重 使用场合： 对生病或受伤的人表示关心，在看望病人告辞时，或听说别人身体欠佳时使用 不能对身体健康的人使用 变形： どうぞ　お大事に　して　ください 语法分析： まだです：还没有 使用场合： 语法分析：是まだ〜（谓语否定形式）です的省略说法。 " }, { "title": "日语学习笔记：《标准日本语》汉字总结", "url": "/posts/studynotes_Standard-Japanese_kanji/", "categories": "有趣的事情", "tags": "学习笔记, 语言", "date": "2017-02-16 00:00:00 +0800", "snippet": "日语汉字的读音有两类，将古代传来的汉语读音日语化了的音读、将固有日语发音与汉字含义相结合后的训读，前者像汉语发音（又分吴音、汉音、唐音等），后者不像汉语发音。很多日语词汇也是汉字组合起来的，开始我不觉得重要，后来发现记住常见的汉字可以理解造词的由来，对记忆单词有很大的帮助。于是有了这篇总结笔记。本文将《标准日本语》单词中出现的汉字按第一次出现的顺序整理。参考词典为维基词典中的日语汉字与日本文化厅官方的《常用汉字表》。每个汉字会标注不同的音读或训读音，并给出在《标准日本语》出现的单词及位置。汉字的不同读音并不是用于含义的，什么时候读什么音有一些简单规律： 单词中汉字独立出现时，一般读训读； 双音节词中的汉字，一般读音读； 单音节词中的汉字，一般读训读。但很多不服从这一规律，有的汉字音读和训读也不只一个，还需要靠读音后列举的单词来积累。此外，有的汉字在单词中会发生音便，会在单词后面加括号标注；有的单词中汉字完全不属于它本身的任何读音，读音只在此单词连读情况中出现，归类于特殊读音中（这类单词出现在《常用汉字表》的“备考”一列，也在该表的附表中单列）。日语汉字现用的新字体和大陆的简体字都简化自港澳台的仍用的繁体字（日本称旧字体），但日本的简化程度比大陆要浅，更多地保留了原来第字体。每个汉字的写法提示我只标示： 与简体字不同的日本简化字，由什么字简化而来； 与简体字相同的日语简化字； 与繁体字相同的日语汉字，但有微小的笔画区别的； 日本自己造的和制汉字（国字），没有中文的对应。繁体字的写法完全相同的日语汉字不提示写法。 这里所谓的相同/不同以 Unicode 字符区分。但即使 Unicode 相同的日语汉字和中文汉字也有写法上的小区别（例如笔顺加不加勾、出不出头），这里不关心这种区别。中 ちゅう（吴音、汉音） 中国（1-1） なか（训读） 田中（1-1）、中村（1-1）、中（1-4）、夜中（1-6） じゅう（吴音、汉音） 〜中（1-10）、一〜中（1-10） 国 写法：简化自旧字体（國），同简体字（国） こく（吴音、汉音） 〜国（1-1）、外国（1-11） くに（训读） 国（1-3） 人 じん（汉音） 中国人（1-1） ひと（训读） 例：あの人（1-1）、人（1-2）、一人暮らし（1-4） 特殊读音 一人（ひとり）暮らし（1-4） にん 人形（1-10）、人気（1-12） 日 にち（吴音） 日本（に〜）（1-1）、毎日、日曜日（1-5）、〜日（1-5）、日本料理（1-12）、日本酒（1-12） 特殊读音 今日（きょう）（1-3）、昨日（きのう）（1-5）、明日（あした/あす）（1-5）、明後日（あさって）（1-5）、一昨日（おととい）（1-5） ひ（训读） 〜曜日（〜び）（1-3,1-5）、誕生日（び）（1-6）、子供の日（1-6）、日（1-10） じつ（汉音） 日（1-10） 本 ほん（吴音、汉音） 日本（1-1）、本（1-2）、本屋（1-3）、本棚（1-4）、本当に（1-9）、日本料理（1-12）、日本酒（1-12） 韓 かん（音读） 韓国（1-1） 学 写法：简化自旧字体（學）=繁体字，同简体字（学） がく（吴音） 学生（1-1）、学校（がっ～）（1-5）、修学旅行（1-10） 生 せい（汉音） 学生（1-1）、先生（1-1）、留学生（1-1）、研修生（1-1）、生徒（1-4）、生活（1-10） しょう（吴音） 誕生日（〜じょう〜）（1-6） なま（训读） 生ビール（1-13） 先 せん（吴音、汉音） 先生（1-1）、先週（1-5）、先月（1-5） 留 りゅう（音读） 留学生（1-1） 教 きょう（吴音） 教授（1-1）、教室（1-4） おし（える）（训读） 教える（1-8） 授 じゅ（音读） 教授（1-1） 社 しゃ（汉音） 社員（1-1）、会社（1-2） 員 いん（汉音） 社員（1-1）、会社員（1-1）、店員（1-1） 会 写法：简化自旧字体（會），同简体字（会） かい（汉音） 会社員（1-1）、会社（1-2）、会場（1-3）、会議室（1-4）、展覧会（1-5）、歓迎会（1-5）、会議（1-11） あ（う）（训读）：会う（1-8）店 てん（吴音、汉音） 店員（1-1）、喫茶店（1-3）、売店（1-4） みせ お店（1-10） 研 けん（音读惯用音） 研修生（1-1）、研修（1-5） 修 しゅう（音读） 研修生（1-1）、研修（1-5）、修学旅行（1-10）、修理（1-13） 企 き（音读） 企業（1-1） 業 ぎゅう（汉音） 企業（1-1）、卒業する（1-14） 大 だい（吴音） 大学（1-1）、大好きだ（1-12）、大丈夫（1-15） おお（训读） 大阪（1-4）、大きい（1-9） たい（汉音） 大変（1-9）、太極拳（1-14）、大変だ（1-14） 父 ちち（训读） 父（1-1） 課 か（音读） 課長（1-1） 長 ちょう（吴音、汉音） 課長（1-1）、社長（1-1）、部長（1-10） なが（训读） 長島（1-2） 出 でる（训读）：出る（1-14） 出迎え（1-1）、出来る（I-14）、出かける（1-14） しゅつ（汉音） 出張（しゅっ〜）（1-5） だ（す）（训读）：出す（1-8） 引き出し（1-13） 迎 むか（训读） 出迎え（1-1） げい（音读） 歓迎会（1-5） 私 わたし（训读） 私（1-1） 李 り（音读） 李（1-1） 王 おう（吴音、汉音） 王（1-1） 張 ちょう（吴音、汉音） 張（1-1）、出張（1-5） 森 もり（训读） 森（1-1） 林 はやし（训读） 林（1-1） 小 お（训读） 小野（1-1） こ（训读） 小麦粉（1-8） ちい（训读） 只用于：小さい（1-9） 野 の（训读） 小野（1-1） や（吴音、汉音） 野球（1-7）、野菜（1-11） 吉 よし（训读） 吉田（1-1） 田 た（训读） 吉田（～だ）（1-1）、田中（1-1） 村 むら（训读） 中村（1-1） 太 た（日语惯用音） 太郎（1-1） ふと（る）（训读）：太る（1-8）郎 ろう（音读） 例：太郎（1-1） 東 とう（汉音） 東京（1-1） 京 きょう（吴音） 東京（1-1）、京都（1-6） 鉛 えん（吴音、汉音） 鉛筆（1-2） 筆 ひつ（音读） 鉛筆（〜ぴつ）（1-2） 傘 かさ（训读） 傘（1-2） 靴 くつ（训读） 靴（1-2） 新 しん（吴音、汉音） 新聞（1-2）、新幹線（1-6）、新宿（1-6）、新聞紙（1-8） あたら（しい）（训读） 新しい（1-9） 聞 ぶん（汉音） 新聞（1-2）、新聞紙（1-8） き（く）（训读） 聞く（1-7） 雑 ざつ（音读） 雑誌（ざっ〜）（1-2） 誌 し（音读） 雑誌（1-2） 辞 じ（音读） 辞書（1-2） 書 しょ（吴音、汉音） 辞書（1-2）、図書館（1-3）、図書室（1-4）、申込書（1-7）、書類（1-14） か（く）（训读）：書く（1-7） 葉書（〜がき）（1-13） 電 でん（吴音） 電話（1-2）、電車（1-6）、電話番号（1-8）、電気（1-14）、携帯電話（1-15） 話 わ（音读惯用音） 電話（1-2）、電話番号（1-8）、携帯電話（1-15） 机 注：日语中机、機不是同一个字 つくえ（训读） 机（1-2） 椅 い（音读） 椅子（1-2） 子 す（唐音） 椅子（1-2） こ（训读） 子供（1-4）、親子丼（1-7） し（吴音、汉音） お菓子（1-10） 鍵 かぎ（训读） 鍵（1-2） 時 特殊读音 時計(とけい）（1-2） じ（吴音） 〜時（1-5） とき（训读） 時々（ときどき）（1-11） 計 けい（汉音） 時計（1-2） 手 て（训读） 手帳（1-2）、手紙（1-7）、苦手（1-11）、切手（1-13） ず（音读惯用音） 上手（1-11） た（训读） 下手（1-11） 帳 ちょう（吴音、汉音） 手帳（1-2） 写 写法：简化自旧字体（寫），同简体字（写） しゃ（吴音、汉音） 写真（1-2）、写真集（1-8）、写真展（1-11） 真 しん（吴音、汉音） 写真（1-2）、写真集（1-8）、写真展（1-11） 車 くるま（训读） 車（1-2） しゃ （吴音、汉音） 自転車（1-2）、電車（1-6）、自動車（1-10）、駐車禁止（1-15） 自 じ（吴音） 自転車（1-2）、自動車（1-10） 転 写法：简化自旧字体（轉），不同于简体字（转） てん（吴音、汉音） 自転車（1-2）、運転（1-10） 土 特殊读音 土産（みやげ）（1-2） ど（音读惯用音） 土曜日（1-5） 産 特殊读音 土産（みやげ）（1-2） さん（音读） 名産品（1-2） 名 めい（汉音） 名産品（1-2）、有名だ（1-10） な（训读） 名古屋（1-4）、名前（1-8） 品 ひん（音读） 名産品（1-2）、記念品（1-8）、作品（1-10） 方 かた（训读） 方（1-2）、夕方（〜がた）（1-8） 家 か（汉音） 家族（1-2）、作家（1-10） いえ（训读） 家（1-4） 族 ぞく（吴音） 家族（1-2） 母 はは（训读） 母（1-2） かあ（训读） お母さん（1-2） 語 ご（吴音） 〜語（1-2, 1-11） 何 なん（训读） 何（1-2） なに（训读） 何（1-2） 特殊读音 如何（いかが）（1-10） 誰 だれ（训读） 誰（1-2） 島 しま（训读） 長島（1-2）、広島（1-6） 食 しょく（汉音） 食堂（1-2）、食事する（1-14） た（べる）（训读） 食べる（1-7）、食べ物（1-9） 堂 どう（音读） 食堂（1-3） 郵 ゆう（音读） 郵便局（1-3） 便 びん（音读） 郵便局（1-3）、航空便（1-8）、便利だ（1-10）、不便だ（1-10）、船便（1-14） 局 きょく（汉音） 郵便局（1-3）、薬局（1-15） 銀 ぎん（汉音） 銀行（1-3）、銀座（1-6） 行 こう（汉音） 銀行（1-3）、旅行（1-5）、飛行機（1-6）、紙飛行機（1-8）、修学旅行（1-10） い（く）（训读） 行く（1-6） 図 写法：简化自旧字体（圖），不同于简体字（图） と（汉音） 図書館（1-3）、図書室（1-4） ず（吴音） 地図（1-3） 館 かん（吴音、汉音） 図書館（1-3）、美術館（1-6）、旅館（1-11） 喫 写法：是“吃”的简化前的写法，目前简体字与繁体字都用“吃”，而日语保留 きつ（音读） 喫茶店（きっ〜）（1-3） 茶 さ（唐音） 喫茶店（1-3） 病 びょう（音读） 病院（1-3） 院 いん（音读惯用音） 病院（1-3） 屋 や（训读） 本屋（1-3）、部屋（1-3）、花屋（1-4）、名古屋（1-4）、蕎麦屋（1-7）、居酒屋（1-13） 建 たて（训读） 建物（1-3） 物 もの（训读） 建物（1-3）、食べ物（1-9）、物（1-10）、飲み物（1-11）、果物（1-11）、買い物する（1-14） ぶつ（音读） 動物園（1-7） もつ（音读） 荷物（1-13） 売 写法：简化自旧字体（賣），不同于简体字（卖） う（る）（训读）：売る（1-14） 売り場（1-3）、 ばい（汉音） 売店（1-4） 場 ば（训读） 売り場（1-3）、場所（1-4） じょう（吴音） 会場（1-3） 入 い（る）（训读） 入り口（1-3）、立入禁止（1-15） はい（る）（训读）：入る（1-15）口 くち（训读） 入り口（～ぐち）（1-3） 事 じ（吴音） 事務所（1-3）、記事（1-14）、食事する（1-14） こと（训读） 仕事（～ごと）（1-5） 務 む（音读） 事務所（1-3） 所 しょ（吴音） 事務所（1-3）、場所（1-4）、住所（1-8）、市役所（1-15） ところ（训读） 所（1-10） 受 う（ける）（训读）：受ける 受付（1-3） 付 つ（ける）（训读）：付ける 受付（1-3） 服 ふく（音读） 服（1-3） 地 ち（汉音） 地図（1-3）、地下鉄（1-4） 隣 写法：不同于繁体字（鄰），互换了偏旁 となり（训读） 隣（1-3） 周 しゅう（音读） 周辺（1-3） 辺 写法：简化自旧字体（邊），不同于简体字（边） へん（吴音、汉音） 周辺（1-3） 今 特殊读音 今日（きょう）（1-3）、今朝（1-5） いま（训读） 今（1-5）、たった今（1-8） こん（吴音） 今週（1-5）、今晩（1-5） 水 すい（吴音、汉音） 水曜日（1-3）、水泳（1-11） みず（训读） 水（1-9） 曜 よう（吴音、汉音） 〜曜日（1-3） 部 特殊读音 部屋（へや）（1-4） ぶ（音读） 部長（1-10） 庭 にわ（训读） 庭（1-4） 居 いる（训读）：居る 居間（1-4）、居酒屋（1-13） 間 ま（训读） 居間（1-4）、間違い（1-16） 冷 れい（汉音） 冷蔵庫（1-4） 蔵 写法：简化自旧字体（藏），简体字同旧字体 ぞう（音读） 冷蔵庫（1-4） 庫 こ（汉音） 冷蔵庫（1-4） 壁 かべ（训读） 壁（1-4） 棚 たな（训读） 本棚（～だな）（1-4） 猫 ねこ（训读） 猫（1-4） 犬 いぬ（训读） 犬（1-4） 箱 はこ（训读） 箱（1-4）、箱根（1-6） 眼 め（训读） 眼鏡（1-4） 鏡 特殊读音 眼鏡（めがね）（1-4） 供 とも（训读） 子供（～ども）（1-4） 兄 きょう（吴音） 兄弟（1-4） にい（训读） お兄さん（1-8） あに（训读） 兄（1-12） 弟 だい（吴音） 兄弟（1-4） おとうと（训读） 弟（1-6） 両 写法：简化自旧字体（兩），不同于简体字（两） りょう（音读） 両親（1-4） 親 しん（吴音、汉音） 両親（1-4）、親切だ（1-10） おや（训读） 親子丼（1-7） 妹 いもうと（训读） 妹（1-4） 男 おとこ（训读） 男（1-4） 女 おんな（训读） 女（1-4） じょ（汉音） 女性（1-9） 徒 と（音读） 生徒（1-4） 上 うえ（训读） 上（1-4） じょう（吴音） 上手だ（1-11） 外 そと（训读） 外（1-4） がい（汉音） 外国（1-11） 下 した（训读） 下（1-4） か（汉音） 地下鉄（1-4） 特殊读音 下手（へた）（1-11） お（ろす）（训读）：下ろす（1-14）前 まえ（训读） 前（1-4）、名前（1-8）、前に（1-8）、駅前（1-14） ぜん（吴音） 午前（1-5） 後 うしろ（训读）：後ろ（1-4） ご（唐音） 午後（1-5） 特殊读音 明後日（あさって）（1-5） 近 ちか（训读） 近く（1-4）、近い（1-9） きん（汉音） 最近（1-12） 室 しつ（汉音） 教室（1-4）、会議室（1-4）、図書室（1-4） 議 ぎ（音读） 会議室（1-4）、会議（1-11） 公 こう（汉音） 公園（1-4） 園 えん（汉音） 公園（1-4）、動物園（1-7） 花 はな（训读） 花屋（1-4）、花（1-8） 駅 写法：简化自旧字体（驛），不同于简体字（驿） えき（汉音） 駅（1-4）、駅前（1-14） 鉄 てつ（汉音） 地下鉄（1-4） 木 き（训读） 木（1-4）、寄木細工（〜ぎ〜）（1-11） 暮 く（らす）（训读）：暮らす 一人暮らし（〜ぐ〜）（1-4） 一 ひと（训读） 一人暮らし（1-4） いち（吴音） もう一度（1-8）、一番（1-12） 特殊读音 一昨日（おととい）（1-5） 横 よこ（训读） 横浜（1-4） おう（吴音） 横断歩道（1-14） 浜 写法：简化自旧字体（濱），不同于简体字（滨） はま（训读） 横浜（1-4） 古 こ（汉音） 名古屋（～ご〜）（1-4） ふるい（训读）：古い（1-9）阪 さか（训读） 大阪（1-4） 週 しゅう（汉音） 先週（1-5）、来週（1-5）、再来週（1-5）、今週（1-5） 来 らい（吴音、汉音） 来週（1-5）、来年（1-5）、来月（1-6） き（る）（训读）：来る（1-6） 出来る（1-14） 再 さ（训读） 再来週（1-5） 昨 特殊读音 昨日（きのう）（1-5）、一昨日（おととい）（1-5）、昨夜（1-6） 明 特殊读音 明日（あした/あす）（1-5）、明後日（あさって）（1-5） 毎 まい（音读） 毎日（1-5）、毎朝（1-5）、毎晩（1-5）、毎週（1-5） 朝 あさ（训读） 毎朝（1-5）、朝（1-5）、朝ご飯（1-14） 特殊读音 今朝（けさ）（1-5） 晩 ばん（音读） 毎晩（1-5）、今晩（1-5）、晩（1-5）、晩ご飯（1-14） 午 ご（吴音、汉音） 午前（1-5）、午後（1-5） 月 げつ（汉音） 月曜日（1-5）、来月（1-5）、先月（1-5） がつ（音读惯用音） 〜月（1-5） 火 か（吴音、汉音） 火曜日（1-5） ひ（训读） 火（1-15）、火気厳禁（1-15） 金 きん（汉音） 金曜日（1-5） かね（训读） お金（1-8） 年 ねん（吴音） 来年（1-5）、去年（1-5）、年（1-5） 去 きょ（汉音） 去年（1-5） 夜 よる（训读） 夜（1-5） よ（训读） 夜中（1-6） 特殊读音 昨夜（1-6） 校 こう（汉音） 学校（1-5） 試 し（音读） 試験（1-5） 験 写法：简化自旧字体（驗），不同于简体字（验） けん（音读） 試験（1-5） 仕 し（汉音） 仕事（1-5） 遅 写法：简化自旧字体（遲），不同于简体字（迟） ち（音读） 遅刻（1-5） 刻 こく（音读） 遅刻（1-5）、彫刻（1-10） 休 や（すむ）（训读） 休み（1-5）、休む（1-5）、夏休み（1-6）、昼休み（1-8） 旅 りょ（音读） 旅行（1-5）、修学旅行（1-10）、旅館（1-11） 展 てん（吴音、汉音） 展覧会（1-5）、写真展（1-11） 覧 写法：简化自旧字体（覽），不同于简体字（览） らん（音读） 展覧会（1-5） 歓 写法：简化自旧字体（歡），不同于简体字（欢） かん（音读） 歓迎会（1-5） 宅 たく（音读） お宅（1-5） 働 写法：和制汉字 はたら（く）（训读）：働く（1-5）始 はじ（まる）（训读）：始まる（1-5）終 お（わる）（训读）：終わる（1-5）起 お（きる）（训读）：起きる（1-5）寝 ね（る）（训读）：寝る（1-5）勉 べん（音读） 勉強する（1-5） 強 写法：注意简体字与繁体字的区别 きょう（汉音） 勉強する（1-5） 神 こう（训读） 神戸（1-5） 戸 写法：日本汉字（戸）、繁体字（戶）、简体字（户）不同 へ（训读） 神戸（〜べ）（1-5） 分 ふん（汉音） 〜分（1-5） わか（る）（训读）：分かる（1-11） ぶん（吴音） 十分（1-15） 半 はん（吴音、汉音） 〜半（1-5） 誕 たん（音读） 誕生日（1-5） 夏 なつ（训读） 夏休み（1-6） 交 こう（汉音） 交通機関（1-6）、交差点（1-14） 通 つう（吴音） 交通機関（1-6） とおる（训读）：通る（1-14） 通り（1-10） 機 注：日语中机、機不是同一个字 き（音读） 交通機関（1-6）、飛行機（1-6）、紙飛行機（1-8） 関 写法：简化自旧字体（關），不同于简体字（关） かん（音读） 交通機関（1-6） 幹 かん（吴音、汉音） 新幹線（1-6） 線 せん（吴音、汉音） 新幹線（1-6） 飛 ひ（吴音、汉音） 飛行機（1-5）、紙飛行機（1-8） と（ぶ）（训读）：飛ぶ（1-14）美 び（音读） 美術館（1-6） 特殊读音 美味しい（おいしい）（1-9） 術 じゅつ（音读） 美術館（1-6） 友 とも（训读） 友達（1-5） 達 だち（吴音） 友達（1-5） たつ（汉音） 速達（1-8） 帰 写法：简化自旧字体（歸），不同于简体字（归） かえる（训读）：帰る（1-6）確 たし（か）（训读）：確か（1-6）佐 さ（音读） 佐藤（1-6） 藤 とう（音读） 佐藤（1-6） 広 写法：简化自旧字体（廣），不同于简体字（广） ひろ（训读） 広島（1-6）、広い（1-9） 都 と（汉音） 京都（1-6） 北 ほく（吴音、汉音） 北海道（ほっ〜）（1-6） 海 かい（吴音、汉音） 北海道（1-6） うみ（训读） 海（1-6） 道 どう（吴音） 北海道（1-6）、道具（1-10）、横断歩道（1-14） みち（训读） 道（1-14） 根 ね（训读）（注：平假名ね由“根”的草书演变而来） 箱根（1-6） 座 ざ（吴音） 銀座（1-6） すわ（る）（训读）：座る（1-15）渋 写法：简化自旧字体（澁），不同于繁体字（澀），也不同于简体字（涩） しぶ（い）（训读） 渋谷（1-6） 谷 や（训读） 渋谷（1-6） 宿 しゅく（汉音） 新宿（～じゅく）（1-6）、宿題（1-8） 茶 ちゃ（音读惯用音） お茶（1-7）、紅茶（1-12）、ウーロン茶（1-12）、ジャスミン茶（1-12）、緑茶（1-12） 粥 かゆ（训读） お粥（1-7） 昼 ひる（训读） 昼ご飯（1-7）、昼休み（1-8）、昼（1-13） 飯 はん（汉音） 昼ご飯（1-7）、朝ご飯（1-14）、晩ご飯（1-14） 弁 注：是一个独立的汉字，中文不常用，读作 biàn（与便当的“便”没有关系） べん（音读） お弁当（1-7） 当 とう（吴音、汉音） お弁当（1-7）、本当に（1-9） 丼 写法：和制汉字 どん（训读） 親子丼（1-7） 卵 たまご（训读） 卵（1-7） 蕎 特殊读音 蕎麦（そば）（1-7） 麦 写法：简化自旧字体（麥），同简体字（麦） 特殊读音 蕎麦（そば）（1-7） むぎ（训读） 小麦粉（1-8） 球 きゅう（汉音） 野球（1-7） 申 もう（す）（训读）：申す（1-15） 申込書（もうし〜）（1-7） 込 写法：和制汉字 こ（む）（训读）：申込書（～こみ〜）（1-7）紙 かみ（训读） 手紙（1-7）（变音がみ）、紙飛行機（1-8）、紙（1-9） し（吴音、汉音） 新聞紙（1-8） 音 おん（吴音） 音楽（1-7） 楽 写法：简化自旧字体（樂），不同于简体字（乐） がく（吴音、汉音） 音楽（1-7） たの（しい）（训读）：楽しい（1-9）映 えい（音读） 映画（1-7） 画 写法：简化自旧字体（畫），同简体字（画） が（吴音） 映画（1-7）、漫画（1-13） 動 どう（音读） 動物園（1-7）、自動車（1-10） 飲 の（む）（训读）：飲む（1-7） 飲み物（1-11） 買 か（う）（训读）：買う（1-7） 買い物する（1-14） 撮 と（る）（训读）：撮る（1-7） さつ（音读） 撮影禁止（1-15） 読 写法：简化自旧字体（讀），不同于简体字（读） よ（む）（训读）：読む（1-7）見 み（る）（训读）：見る（1-7） み（せる）（训读）：見せる（1-14）掃 そう（音读） 掃除する（1-7） 除 じ（音读） 掃除する（1-7） 記 き（汉音） 記念品（1-8）、記事（1-14） 念 ねん（音读） 記念品（1-8） 表 ひょう（音读） スケジュール表（1-8） 集 しゅう（汉音） 写真集（1-8） 題 だい（吴音） 宿題（1-8） 航 こう（音读） 航空便（1-8） 空 くう（吴音） 航空便（1-8） 速 そく（吴音、汉音） 速達（1-8） 番 ばん（音读惯用音） 電話番号（1-8）、一番（1-12） 号 写法：简化自旧字体（號），同简体字（号） ごう（吴音） 電話番号（1-8） 住 じゅう（吴音） 住所（1-8） 件 けん（汉音） 件（1-8） 粉 こ（训读） 小麦粉（1-8） 箸 はし（训读） 箸（1-8） 夕 ゆう（训读） 夕方（1-8） 送 おく（る）（训读）：送る（1-8）作 つく（る）（训读）：作る（1-8） さく（吴音、汉音） 作品（1-10）、作家（さっ〜）（1-10） 届 とど（く）（训读）：届く（1-8）貸 か（す）（训读）：貸す（1-8）習 なら（う）（训读）：習う（1-8）借 か（りる）（训读）：借りる（1-8）度 ど（吴音） もう一度（1-8） 陳 ちん（音读） 陳（1-8） 様 简化自旧字体（樣），不同于简体字（样） さま（训读） 〜様（1-8）、お客様（1-9） よう（吴音、汉音） 模様（1-11） 料 りょう（音读） 料理（1-9）、日本料理（1-12） 理 り（吴音、汉音） 料理（1-9）、日本料理（1-12）、修理（1-13）、整理する（1-14）、無理（1-15） 四 し（吴音、汉音） 四川（1-9） 川 せん（吴音、汉音） 四川（1-9） 焼 写法：简化自旧字体（燒），不同于简体字（烧） や（く）（训读） すき焼き（1-9）、焼き鳥（1-13） しょう（音读） 焼酎（1-12） 温 写法：简化自旧字体（溫），同简体字（温） おん（音读） 温泉（1-9） あたた（かい）（训读）：暖かい（1-15）泉 せん（音读） 温泉（1-9） 湯 ゆ（训读） お湯（1-9） 浴 特殊读音 浴衣（ゆかた）（1-9） 衣 特殊读音 浴衣（ゆかた）（1-9） 眺 なが（め）（训读）：眺め（1-9）薬 写法：简化自旧字体（藥），不同于简体字（药） くすり（训读） 薬（1-9） やく（音读） 薬局（やっ〜）（1-15） 天 てん（吴音、汉音） 天気（1-9） 気 写法：简化自旧字体（氣），不同于简体字（气） き（音读） 天気（1-9）、気持ち（1-9）、元気だ（1-10）、人気（1-12）、電気（1-14）、気（1-15）、火気厳禁（1-15） 山 やま（训读） 山（1-9） さん（汉音） 富士山（1-10） 性 せい（音读） 女性（1-9） 客 きゃく（吴音） お客様（1-9）、観光客（1-10） 歌 か（吴音、汉音） 歌舞伎（1-9） うた（训读） 歌（1-11） うた（う）（训读）：歌う（1-15）舞 ぶ（音读） 歌舞伎（1-9） 伎 注：是一个独立的汉字，中文不常用，读作 jì（与便当的“便”没有关系） き（音读） 歌舞伎（1-9） 持 も（つ）（训读）： 気持ち（1-9） 辛 から（い）（训读）：辛い（1-9） 塩辛い（1-9） 甘 あま（い）（训读）：甘い（1-9）塩 写法：简化自旧字体（鹽），不同于简体字（盐） しお（い）（训读）： 塩辛い（1-9） 酸 す（い）（训读）： 酸っぱい（1-9） 苦 にが（い）（训读）：苦い（1-9） 苦手（1-11） 味 特殊读音 美味しい（おいしい）（1-9）、不味い（まずい）（1-9） 不 特殊读音 不味い（まずい）（1-9） ふ 不便だ（1-10） 熱 あつ（い）（训读）：熱い（1-9） ねつ（音读） 熱（1-15） 冷 つめ（たい）（训读）：冷たい（1-9）面 おも（训读） 面白い（1-9） 白 しろ（い）（训读）：白い（1-9） 面白い（1-9） 狭 せま（い）（训读）：狭い（1-9）忙 いそ（しい）（训读）：忙しい（1-9）悪 写法：简化自旧字体（惡），不同于简体字（恶） わる（い）（训读）：悪い（1-9）素 特殊读音 素晴らしい（すばらしい）（1-9） 晴 特殊读音 素晴らしい（すばらしい）（1-9） は（れる）（训读） 晴れ（1-10） 遠 とお（い）（训读）：遠い（1-9）高 たか（い）（训读）：高い（1-9）低 ひく（い）（训读）：低い（1-9）安 やす（い）（训读）：安い（1-9）寒 さむ（い）（训读）：寒い（1-9）暑 あつ（い）（训读）：暑い（1-9）青 あお（い）（训读）：青い（1-9）難 むずか（しい）（训读）：難しい（1-9）易 やさ（しい）（训读）：易しい（1-9）多 おお（い）（训读）：多い（1-9）少 すく（ない）（训读）：少ない（1-9） すこ（し）（训读）：少し（1-9）可 か（音读） 可愛い（1-9） 愛 特殊读音 可愛い（かわいい）（1-9） 変 写法：简化自旧字体（變），不同于简体字（变） へん（音读） 大変（1-9）、大変だ（1-14） 全 ぜん（吴音） 全然（1-9） 然 ぜん（汉音） 全然（1-9） 紅 特殊读音 紅葉（もみじ）（1-10） こう（音读） 紅葉（1-10）、紅茶（1-12） 葉 特殊读音 紅葉（もみじ）（1-10） よう（音读） 紅葉（1-10） は（训读） 葉書（1-13） 故 こ（音读） 故郷（1-10） 郷 写法：简化自旧字体（鄉），不同于简体字（乡） きょう（音读） 故郷（1-10） 町 写法：是一个独立的汉字，中文不常用，读作 dīng まち（训读） 町（1-10） 形 ぎょう（吴音） 人形（1-10） 彫 写法：是一个独立的汉字，中文作为雕的异体字而不用 ちょう（音读） 彫刻（1-10） 具 ぐ（吴音） 道具（1-10） 魚 さかな（训读） 魚（1-10） 菓 写法：是一个独立的汉字，中文作为果的异体字而不用 か（吴音，汉音） お菓子（1-10） 観 写法：简化自旧字体（觀），不同于简体字（观） かん（音读） 観光客（1-10） 光 こう（吴音、汉音） 観光客（1-10） 平 へい（音读） 平日（1-10） 活 かつ（汉音） 生活（1-10） 世 せ（吴音）（注：平假名せ由“世”的草书演变而来） 世界（1-10） 界 かい（汉音） 世界（1-10） 雨 あめ（训读） 雨（1-10） 曇 写法：是一个独立的汉字，简体字对应昙花的昙 くも（る）（训读） 曇り（1-10） 雪 ゆき（训读） 雪（1-10） 汚 写法：是一个独立的汉字，中文作为污的异体字而不用 きた（ない）（训读）：汚い（1-10）綺 き（音读） 綺麗だ（1-10） 麗 れい（音读） 綺麗だ（1-10） 有 ゆう（汉音） 有名だ（1-10） 静 写法：简化自旧字体（靜），同简体字（静） しず（か）（训读）：静かだ（1-10）暇 ひま（训读） 暇だ（1-10） 切 せつ（汉音） 親切だ（1-10） き（る）（训读）：切る（1-13） 切手（1-13） 好 す（く）（训读）： 好きだ（1-10）、大好きだ（1-12） 嫌 きら（う）（训读）： 嫌いだ（1-10） 元 げん（汉音） 元気だ（1-10） 簡 かん（音读） 簡単だ（1-10） 単 写法：简化自旧字体（單），不同于简体字（单） たん（吴音、汉音） 簡単だ（1-10） 如 特殊读音 如何（いかが）（1-10） 奈 な（音读）（注：平假名な由“奈”的草书演变而来） 奈良（1-10） 良 ら（音读）（注：平假名ら由“良”的草书演变而来） 奈良（1-10） 富 ふ（音读） 富士山（1-10） 士 じ（音读） 富士山（1-10） 絵 写法：简化自旧字体（繪），不同于简体字（絵） え（音读） 絵（1-11） 英 えい（音读） 英語（1-11） 泳 えい（汉音） 水泳（1-11） およ（ぐ）（训读）：泳ぐ（1-13）運 うん（吴音、汉音） 運転（1-11） 酒 さけ（训读） お酒（1-11） しゅ（吴音） 日本酒（1-12） さか（训读） 居酒屋（〜ざか〜）（1-13） 肉 にく（吴音） 肉（1-11）、肉じゃが（1-13） 菜 さい（音读） 野菜（1-11） 果 くだ（训读）：果物（1-11）窓 写法：是一个独立的汉字，中文作为窗的异体字而不用 まど（训读） 窓（1-11） 結 けつ（音读） 結婚式（けっ〜）（1-11）、結構（1-11） 婚 こん（音读） 結婚式（1-11） 式 しき（吴音） 結婚式（1-11） 别 べつ（音读） 別荘（べっ〜）（1-11） 荘 写法：简化自旧字体（莊），不同于简体字（庄） そう（音读） 別荘（1-11） 寄 よせ（る）（训读） 寄木細工（1-11） 細 さい（吴音） 寄木細工（〜ざい〜）（1-11） 工 く（吴音） 寄木細工（1-11） 模 も（音读） 模様（1-11） 脚 あし（训读） 脚（1-11） 僕 ぼく（音读） 僕（1-11） 迷 まよ（う）（训读） 迷う（1-11） 閉 し（める）（训读）：閉める（1-11）疲 つか（れる）（训读）：疲れる（1-11）散 さん（音读） 散歩する（1-11） 歩 写法：在旧字体（步）上加了一点，简体字和繁体字仍用不加点的“步” ほ（汉音） 散歩（〜ぽ）する（1-11）、横断歩道（1-14） ある（く）（训读） 歩く（1-14） 怖 こわ（い）（训读）：怖い（1-11）赤 あか（い）（训读）：赤い（1-11）痛 いた（い）（训读）：痛い（1-11）構 こう（音读）： 結構（1-11） 季 き（音读） 季節（1-12） 節 せつ（音读） 季節（1-12） 冬 ふゆ（训读） 冬（1-12） 春 はる（训读） 春（1-12） 寿 写法：简化自旧字体（壽），同简体字（寿） す（音读） 寿司（1-12） 司 し（吴音、汉音） 寿司（1-12） 酎 写法：是一个独立的汉字，中文不常用，读作 dīng ちゅう（音读） 焼酎（1-12） 緑 写法：简化自旧字体（綠），不同于简体字（绿） りょく（音读） 緑茶（1-12） 席 せき（音读） 席（1-12） 種 しゅ（音读） 種類（1-12） 類 るい（音读） 種類（1-12）、書類（1-14） 背 せ（训读） 背（1-12） 最 さい（音读） 最近（1-12） 降 ふ（る）（训读）：降る（1-12） お（りる）（训读）：降りる（1-14）若 わか（い）（训读）：若い（1-12）暖 あたた（かい）（训读）：暖かい（1-12）涼 すず（しい）（训读）：涼しい（1-12）速 はや（い）（训读）：速い（1-12）荷 に（训读） 荷物（1-13） 引 ひ（く）（训读） 引き出し（1-13） 漫 まん（音读） 漫画（1-13） 鳥 とり（训读） 焼き鳥（1-13） 唐 から（训读） 唐揚げ（1-13） 揚 あ（げる）（训读）：唐揚げ（1-13）髪 写法：简化自旧字体（髮），不同于简体字（发） かみ（训读） 髪（1-13） 象 ぞう（音读） 象（1-13） 他 ほか（训读） 他（1-13） 掛 か（かる）（训读）：掛かる（1-13）咲 写法：是一个独立的汉字，中文作为笑的异体字而不用 さ（く）（训读）：咲く（1-13）遊 あそ（ぶ）（训读）：遊ぶ（1-13）吸 す（う）（训读）：吸う（1-13）船 ふな（训读） 船便（1-14） 原 げん（汉音） 原稿（1-14） 稿 こう（音读） 原稿（1-14） 橋 はし（训读） 橋（1-14） 角 かど（训读） 角（1-14） 断 写法：简化自旧字体（斷），同简体字（断） だん（音读） 横断歩道（1-14） 右 みぎ（训读） 右（1-14） 左 ひだり（训读） 左（1-14） 差 さ（音读） 交差点（1-14） 点 写法：简化自旧字体（點），同简体字（点） てん（音读） 交差点（1-14） つ（ける）（训读）：点ける（I-14）極 きょく（音读） 太極拳（1-14） 拳 けん（音读） 太極拳（1-14） 急 いそ（ぐ）（训读）：急ぐ（1-14）死 し（ぬ）（训读）：死ぬ（1-14）待 ま（つ）（训读）：待つ（1-14）話 はな（す）（训读）：話す（1-14）渡 わた（る）（训读）：渡る（1-14）選 えら（ぶ）（训读）：選ぶ（1-14）消 け（す）（训读）：消す（1-14）曲 ま（がる）（训读）：曲がる（1-14）洗 あら（う）（训读）：洗う（1-14）開 あ（ける）（训读）：開ける（1-14）過 す（ぐ）（训读）：過ぐ（1-14）卒 そつ（音读） 卒業する（1-14） 整 せい（汉音） 整理する（1-14） 暗 くら（い）（训读）：暗い（1-14）市 し（汉音） 市役所（1-15） 役 やく（吴音） 市役所（1-15） 携 けい（音读） 携帯電話（1-15） 帯 写法：简化自旧字体（帶），不同于简体字（带） たい（音读） 携帯電話（1-15） 禁 きん（音读） 禁煙（1-15）、駐車禁止（1-15）、立入禁止（1-15）、火気厳禁（1-15）、撮影禁止（1-15） 煙 えん（音读） 禁煙（1-15） 風 特殊读音 風邪（かぜ）（1-15） ふ（吴音） お風呂（1-15） 邪 特殊读音 風邪（かぜ）（1-15） 睡 すい（汉音） 睡眠（1-15） 眠 みん（音读） 睡眠（1-15） 呂 写法：注意简体字与繁体字的区别 ろ（音读） お風呂（1-15） 打 う（ち）（训读）： 打ち合わせ（1-15） 合 あ（わせる）（训读） 打ち合わせ（1-15） 無 む（吴音） 無理（1-15） 駐 ちゅう（音读） 駐車禁止（1-15） 止 し（吴音、汉音） 駐車禁止（1-15）、立入禁止（1-15）、撮影禁止（1-15） と（める）（训读）：止める（1-15）立 た（つ）（训读）： 立入禁止（たち〜）（1-15） 厳 写法：简化自旧字体（嚴），不同于简体字（严） げん（音读） 火気厳禁（1-15） 影 えい（音读） 撮影禁止（1-15） 乗 写法：简化自旧字体（乘），简体字同旧字体 のる（训读）：乗る（1-15）使 つかい（训读）：使い（1-15）取 と（る）（训读）：取る（1-15）伝 写法：简化自旧字体（傳），不同于简体字（传） つた（える）（训读）：伝える（1-15）丈 じょう（音读） 大丈夫（1-15） 夫 ぶ（音读） 大丈夫（1-15） 十 じゅう（吴音） 十分（1-15） 勿 もち（音读） 勿論（I-15） 論 ろん（音读） 勿論（I-15） " }, { "title": "日语学习笔记：《标准日本语》按课语法总结", "url": "/posts/studynotes_Standard-Japanese_grammar/", "categories": "有趣的事情", "tags": "学习笔记, 语言", "date": "2017-02-16 00:00:00 +0800", "snippet": "本文接续日语学习笔记：语法简要总结，将《标准日本语》中的语法内容按课整理并对应到语法笔记的体系里。每一条语法给一个完整的例句，以浅色标示，以便记忆。I-1 体言接副助词は作主语； 体言接断定助动词（系动词）だ的敬体丁宁体です作谓语，表示“是”。助动词です用终止形です表示谓语结句（普通陈述句）。 〜は　〜です：～是～。 です的否定形式为ではありません。では可以换成口语化的じゃ。原理是： 断定助动词だ的连用形で连补助动词ある得到一个更书面的断定助动词である； である是动词活用型助动词，它的连用形であり接敬语助动词ます； 助动词ます的せ未然形ませ可接否定助动词ぬん； 最后的助动词ぬん用终止形ぬん表示谓语结句（普通陈述句）； 音便：ぬん省略ぬ音，で后面加个は表示一下强调，最后得到ではありません。总之准确来说，它不是です的否定形式，而是であります的否定形式。 ～は　〜では　ありません：～不是～。 在句尾加终助词か，把陈述句变成疑问句。本课构造了一般疑问句； ～は　〜ですか：～是～吗？ 人称代词 第一人称：私（わたし） 第二人称：あなた 第三人称：あの人 体言接格助词の作连体修饰语，表示所属关系。I-2 指示代词：これ（近称）、それ（中称）、あれ（远称）、どれ（不定称，只能用于疑问句） 连体词：この（近称）、その（中称）、あの（远称）、どの（不定称，只能用于疑问句） 不定称放在谓语部分，可构造特殊疑问句。 ～は　不定称代词 ですか：〜是什么～？ 100 以下的数词。I-3 指示代词：ここ（近称）、そこ（中称）、あそこ（远称）、どこ（不定称，只能用于疑问句） 断定助动词です的第二个意思：在，表示地点 ～は　〜（地点）です：～在～（地点）。 体言接副助词も构成主语，表示“也”的意思 可以有多个 ～も，构成并列的对等文节，表示“～和～都” ～も　〜です：〜也是～。 在选择疑问句中，谓语直接用逗号构成并列的对等文节 ～は　〜ですか、〜ですか：～是～，还是～？ 100 以上的数词。I-4 表示存在的动词：ある、いる 存在动词ある、いる的连用形あり、い接敬语助动词ます，助动词ます用终止形ます表示谓语结句（普通陈述句）； 体言接格助词作连用修饰语，修饰谓语。格助词： が：表示动作的主题（一般紧挨在动词前面） に：表示动作存在的地点 体言接格助词と构成对等文节：表示并列，意为“和” 表示方位的指示代词：上、下、前、後ろ、隣、中、外 在句尾加终助词ね，让句子的口气变柔和，类似于中文的“吧”与“是吗”等 疑问代词 + も + 谓语否定形式，表全面否定 疑问词 も　谓语否定形式：什么～也不～ I-5 敬语助动词ます（书中称动词的ます形） 敬语助动词ます的否定形式为 ません，原理是助动词ます的せ未然形ませ接否定助动词ぬん，助动词ぬん用终止形ぬん表示谓语结句，音便：省略ぬ音。 ます的过去形式为 ました，原理是助动词ます的连用形まし接过去助动词た，助动词た用终止形た表示谓语结句。 ます的过去否定形式为 ませんでした，顺序是先接否定助动词ぬん，再接过去助动词。（这两个助动词是并列的吗？为什么ぬん没有用连用形ず？） 动词连用形 ます/ません/ました/ませんでした：发生/没发生/发生了/没发生过～ 体言接格助词作连用修饰语，修饰谓语动词。格助词： に：表示动作发生的时间 から：表示动作发生的开始时间 まで：表示动作发生的结束时间 ～（时刻）に/から/まで　动词ます形：在/从/到～（时刻）发生～ ～（时刻）から　〜（时刻）まで　动词ます形：从～（时刻）到～（时刻）发生～ （任何词语）接副助词は表示强调。I-6 体言接格助词作连用修饰语，修饰谓语动词。格助词： へ：表示移动动词的目的地 から：表示移动动词的起点 まで：表示移动动词的终点（まで必须与から连用，否则要用へ） と：表示一起做某动作的对象 で：表示移动动词使用的交通工具 ～（地点）へ/から　移动动词ます形：向/从～（地点） 移动 ～（地点）から　〜（地点）まで　移动动词ます形：从～（地点）到～（地点）移动 ～（交通工具）で 移动动词ます形：乘坐～ 移动 ～と　动词ます形：和～一起 做～ 助词接副助词は表示强调。I-7 体言接格助词作连用修饰语，修饰谓语动词。格助词： を：表示动作的对象（即他动词的宾语） 〜を　他动词ます形 で：表示动作进行的地点 〜（地点）で　动词ます形：在～（地点）做～ 体言接并列性副助词か构成对等文节：表示选择并列，意为“或者”I-8 体言接格助词作连用修饰语，修饰谓语动词。格助词： で：表示动作的手段、工具、原材料 〜（工具）で　动词ます形：通过/用～做～ に：表示授受动词的被授受对象 を：表示授受动词的授受物 ～（人）は　〜（人）に　〜（物品）を　授受动词ます形 在句尾加终助词よ，表示句子的提醒、告知、轻微的警告，类似于中文的“哟”I-9 一类形容词接断定助动词（系动词）だ的敬体丁宁体です作谓语，表示主语是什么样的。助动词です用终止形です表示谓语结句（普通陈述句）。 ～は　〜（一类形容词）です：〜是～样的 一类形容词～い（作谓语时）的否定形式为 〜くないです，原理是一类形容词的的连用形〜く接否定助动词ない，助动词ない用的什么形？。注意，是否定形容词本身，而不是把です否定即ではありません。 ～は　〜（一类形容词连用形）+ない です：〜是～样的 一类形容词～い（作谓语时）的过去形式为 〜かったです，原理是一类形容词的的连用形〜かっ接过去助动词た，助动词た用的什么形？。注意，是过去形容词本身，而不是把です过去即でした。 ～は　〜（一类形容词连用形）+かった です：〜是～样的 一类形容词～い（作谓语时）的过去否定形式为 〜くなかったです，顺序是先接否定助动词ない，ない 怎么再接过去助动词。（这两个助动词是并列的吗？为什么ぬん没有用连用形ず？）注意，是否定形容词本身，而不是把です否定即ではありませんでした。 ～は　〜（一类形容词连用形）+なかった です：〜是～样的 一类形容词的连体形（也即它本身）不加助词直接作连体修饰语。 助词 を 接副助词 は 表示强调时，必须去掉を 程度副词不加助词直接作连用修饰语，修饰形容词 否定疑问句。注意，回答用はい和いいえ都表示对否定的认同，与英语习惯不一样 ～ + 谓语否定形式 + か：～不～吗？ I-10 です的过去形式为でした，过去否定形式为 ではありませんでした。原理？ 〜は　〜でした/ではありませんでした：～是/不是～。 二类形容词接断定助动词（系动词）だ的敬体丁宁体です作谓语，表示主语是什么样的。助动词です用终止形です表示谓语结句（普通陈述句）。 二类形容词（作谓语时）的否定、过去、过去否定形式是在助动词です后接的否定、过去助动词，即ではありません、でした、ではありませんでした 〜は　〜（二类形容词词干）でした/ではありませんでした：～是/不是～。 二类形容词～だ的连体形〜な不接助词直接作连体修饰语 连体词：どんな、何の（都是不定称，只能用于疑问句） 连词作独立语。但是：でも；而且，そして。 形容词的连体形作连体修饰语，被修饰的体言可以省略，为了表示此处有体言被省略（防止语法错误），需要加格助词の，即形容词连体形+のI-11 体言接格助词作连用修饰语，修饰谓语形容词。格助词： が：表示情感形容词、能力形容词的情感对象 体言接格助词作连用修饰语，修饰谓语动词。格助词： が：能力动词的对象 体言接并列性副助词や、など构成对等文节：表示列举不完全的并列，など只能用在最后一个（可以省略） ～や　〜や　...（など）：～呀～呀～ 之类的” 接续助词から，是条件性顺接接续（前项为后项的条件），意为“因为” A（用言）から　B（用言）：因为A，所以B” 连词：だがら/ですから，表示因果，意为“所以” 频率副词不加助词直接作连用修饰语，修饰动词 人称代词：第一人称，ぼくI-12 形容词比较级：没有变形，借助字面意义的助词实现体言接格助词作连用修饰语，修饰谓语形容词。格助词： より：表示被比较的对象 ほど：表示被比较的对象（否定形式） のほうが、は：表示比较的对象； Aは　Bより　〜（形容词）です：A比B更〜 Aより　Bの　ほうが　〜（形容词）です：与A相比，B更〜 Aは　Bほど　〜（形容词否定形式）です： A不如B更〜 形容词最高级：没有变形，借助字面意义的助词实现 で：提示比较的范围，范围如时间段、〜の中（なか），也可以用格助词と并列罗列后加の中 〜（范围）で 〜が　一番　〜（形容词）です：在〜（范围）中，〜是最〜的　 提问形容词比较级，使用副助词と提示比较对象（完全列举）（第二个と可以省略） 〜と　〜（と）　疑问代词　〜（形容词）ですか：〜与〜那个更〜？ 〜のほうが　〜（形容词）です：〜更〜 どちらも　〜（形容词）です：都〜 提问形容词最高级。 〜（范围）で 疑问代词が　一番　〜（形容词）ですか：在〜（范围）中，那个是最〜的？　 接续助词が，说半句话时用（单纯的接续），把后半句咽下去（常用于难以启齿的事或拒绝别人请求，表达委婉），类似于中文中有人打字爱发省略号I-13 数量词； 数量词不接助词直接作连用修饰语，修饰动词。例如： 表示时间段的数量词 〜（时间段）延续性动词：在〜时候做〜 频率的表达方式：量词 〜回/度，用格助词に提示频率的时间段 时间段（名词）に + x回/度：〜久x次 注意毎〜的时间段名词不加に 有的时候可以省略一些词，例如一週間に省略为週に 提了一下动词的ます形：动词连用形 + 敬语助动词ます 动词连用形作名词接助词作连用修饰语，例如： 修饰移动动词的目的 〜（地点） 动词连用形に　移动动词ます形：移动到〜（地点）做〜 体言接格助词作连用修饰语，修饰谓语动词。格助词： で：用于以个数计（不称重）的售物 x個で　〜円：x个〜日元 1個副词化了，不加で 数量词后面接副助词くらい/ぐらい，表示大概的数量：〜左右，大约〜。二者没有区别，依个人习惯。I-14 五段活用动词（书中称为一类动词）、上下一段活用动词（书中称为二类动词）、カ行、サ行变格动词、サ行变格复合动词（书中称三类动词）的连用形； 动词的连用形接て比较常见，书中单列称为动词的て形。て可以是接续助词，也可以是例如补助动词的第一个音，等等。五段活用动词有音便。（如何理解动词て形的性质？体言性质？） 接续助词て：接动词连用形： 构成对等文节，表示动作相继发生（不可并列太多，并列太多要用连词それから停顿） A 动词て形、B 动词：A发生后，B发生 接格助词から作连用修饰语修饰动词，表示动作相继发生。（与上一条用法相比，更强调先发生的动作） A 动词てから　B 动词：A发生后　B发生 起到动词与补助动词的连接作用，例如补助动词くださる： 动词B 动词：A发生后　B发生 体言接格助词作连用修饰语，修饰谓语动词。格助词： を：表示移动动词经过的地点；表示动作离开的地点 ～（地点）を 移动动词ます形：从～（地点） 移动经过/离开 I-15 接续助词て：接动词连用形，起到动词与补助动词的连接作用：补助动词いる，表示动作正在进行（现在进行时）； 动词て形 + います：正在做〜 注意动词必须是持续性动词 接续助词ても：接动词连用形，与形容词いい搭配（惯用句型）表示许可：can 动词て形 + も + いいです：可以做〜 接续助词ては：接动词连用形，与形容词いけない搭配，表示禁止： 动词て形 + は +　いけない：不可以做〜 该句型省略了断定助动词です，作为惯用句型； いけない是一类形容词（相当于よくない）；其敬体丁宁体いけません（此时只能用于该句型，即： 动词て形 + は +　いけません：不可以做〜 更客气的说法是：动词ない形 + でください，ない为否定助动词接动词连用形，后面接でください（て要发生浊化音便） 体言接格助词作连用修饰语，修饰谓语动词。格助词： に：表示动词的附着点；表示移动动词的目的地（此时与へ完全相同） 〜（附着点）に　带有附着属性的动词ます形 〜（地点）に　移动动词ます形：向～（地点） 移动 I-16 形容词的连用形接て比较常见，书中单列称为形容词的て形：て一般是接续助词。 接续助词て：接形容词连用形，构成对等文节，表示形容词的并列 体言接格助词で构成对等文节：表示并列，意为“和” 接续助词て：接动词连用形，起到动词与补助动词的连接作用：补助动词いる，表示动作结束后留下的结果状态（现在完成时）； 动词て形 + います：（已经）做过〜 接续助词が，是条件性逆接接续（前项为后项的条件），意为“但是” A（用言）が　B（用言）：虽然A，但是B” " }, { "title": "日语学习笔记：语法简要总结", "url": "/posts/studynotes_Japanese_grammar/", "categories": "有趣的事情", "tags": "学习笔记, 语言", "date": "2017-02-15 00:00:00 +0800", "snippet": "之前《标准日本语》初级上册学到一半就学不下去了，到动词这儿难住了。最近仔细想了一下，感觉可能是书的问题，这套教材每一条语法都是以句型公式编排的，只讲怎么套公式，以实用为主，我并不清楚其中组成部分的语法作用，有“背就完事儿了”的感觉，随着学习越攒越多，就越摸不着头脑，而且背完很容易忘。为改变现状，我找了一些语法方面的资料，主要是皮细庚《新编日语语法教程》和维基百科，系统地了解一下，也为以后记单词或学习指明方向。本文总结日语语法的主要框架，基本是皮细庚书的简要总结。之后会再开一篇笔记，把标日每课的语法内容对应到本文体系中。日语句子的组成逻辑日语最大的特色是黏着语，词语一般不能独立地出现，而是要在后面黏着一个附属词。单词分为独立词与附属词，独立词是有实际意义的词，附属词包括助词与助动词，不具有实际意义。因此下面说到文节中的单词（或者把文节与单词混谈）时，都默认指的是文节中的独立词。独立词挂上若干附属词构成日语的文节：独立词（有且只有一个） + 附属词（若干个，可以没有）一个句子是由一个个文节连接组成，每个文节在句子起到特定的作用（即句子成分），有以下几类： 主语、谓语：是句子的主要框架。 主语用的词类为名词、数量词、代词（这些词类统称为体言）； 谓语用的词类为动词、形容词、形容动词（这些词类统称为用言）。 修饰语：在框架中作修饰（即宾定状补，但日语不这么称呼）。独立词加助词可以作修饰语，也有一些可以不加助词直接修饰的独立词类。 修饰体言的叫连体修饰语（即定语）。连体词可以不加助词直接修饰； 修饰用言的叫连用修饰语（即宾语、状语、补语）。副词可以不加助词直接修饰。 独立语：独立于主谓结构之外，包括起到句子接续作用的连词（接续词）和无接续作用的感叹词。一个完整的句子（单句）形式如下：独立语、（若干个连体修饰语 + ）主语 + （若干个连用修饰语 + ）谓语。注意点： 谓语严格限制在最后面； 其中多个连词（独立语）可以把单句连接为复句； 可以有并列的多个主语、谓语，构成对等文节； 根据这些成分不同的固定搭配，可以得到不同的句型。语法书上按照词类分类编排，其实蛮合理的。因为不是某种成分固定地只使用某些独立词类或者助词类，例如体言既可以作主语也可以作修饰语；副助词可以接在体言和用言后面。所以最好是按照词类，并介绍在句子成分中的可能作用。每个词类会先讲解其本身，再讲解它们可以担任的句子成分（即在句子中的用法）。由于构成句子成分（文节）需要借助助词，所以助词而穿插在句子成分部分详解，不专门讲解，只在后面作简要总结。体言：名词、数量词、代词名词没什么好说的。数量词包括数词和量词，数词即一、二、三等纯数字，量词即单位。数词有不同的读法，也会因后接不同的量词而读法不同，这点没办法只能靠积累。代词包括人称代词、指示代词（指示事物、场所、方向三种）、反身代词。注意指示代词和人称代词的 ta，有近称、中称、远称、不定称，其中在近称、远称间多了中称是日语的特色。体言担任的句子成分作主语体言接合适的助词可以作主语： 接格助词 が：表示主语 接副助词 は：表示主题，是が的替代 作谓语体言接断定助动词（即系动词）だ构成谓语。体言作谓语只有这一种方式。作连体修饰语体言接合适的助词可以作连体修饰语，修饰体言： 接格助词 の：表示所属 作连用修饰语一半来修饰动词？动词是要与一堆 连着助词的修饰语绑定的，形成固定搭配。与英语 give sth to sb 一样体言接合适的助词可以作连用修饰语： 修饰动词： 接格助词： が：表示希望、好恶、巧拙、难易、能力等用言的对象语 を：他动词的宾语，可以用助动词 は。日语根据要不要有宾语将动词分为自动词（即不及物动词）与他动词（即及物动词） に：表示存在的场所、东所发生的时间 へ： と： から： 修饰形容词： より： 用言：动词、形容词用言词有两种：动词、形容词，大都是按照单词的最后一个音来区分词性。例如一般来说，う段结尾的是（五类活用）动词，る结尾的是（上下一段活用等）动词，い结尾的是一类形容词，だ结尾是二类形容词，等等。 不能只以词尾特征判断词类及其活用，有很多例外情况，例外情况需要不断积累，单独记忆。以下所有词尾特征定义的词类活用都应加“通常”二字。动词日语动词有很多形态，原始的形态叫基本形，并有六个活用形：未然形、连用形、终止形、连体形、假定形、命令形，用于不同的意义和用法。本节只介绍动词形态的变化规则，后面在用言担任的句子成分中介绍活用形的应用场景。动词活用的分类动词可以根据词末尾的音分为几类，每一类都有自己的活用形的变化规则，变化的也是词尾。这些类包括五段活用动词、上一段活用动词、下一段活用动词、カ行变格活用动词、サ行变格活用动词。基本形结尾为う段音的动词叫五段活用动词，有的书称为一类动词，活用变化规则如下： 未然形：词尾变为同行あ段/お段音（二者有不同的应用场景，见下）； 连用形：词尾变为同行い段音； 终止形、连体形：不变，即词尾仍为同行う段音； 假定形、命令形：词尾变为同行え段音。 同行x段意味着词尾音节的辅音不变，这是快速掌握五段活用的关键。可以看到，活用涉及同行的所有五个段的音，所以叫五段活用。基本形最后一个音是 る：倒数第二个是い段音的动词是上一段活用动词，倒数第二个是え段音的动词是下一段活用动词。有的书合称为二类动词。活用变化规则如下： 未然形、连用形：去掉 る 终止形、连体形：不变 假定形：る变れ 命令形：る变ろ/よ（二者有不同的应用场景，见下） 可以看到，只对词尾る进行操作，倒数第二个い、え段音不变，活用只涉及同行的一个段的音，所以叫一段活用。カ行变格活用动词（简称カ变动词）只有一个：来る，意思是“来”。按形态规则它属于上一段活用动词，但不符合上一段活用的规则。其活用变化规则如下： 未然形：来（こ） 连用形：来（き） 终止形、连体形：来（く）る 假定形：来（く）れ 命令形：来（こ）いサ行变格活用动词（简称サ变动词）是：する，意思是“做”，相当于英语 do。 未然形：し/せ/さ 连用形：し 终止形、连体形：する 假定形：すれ 命令形：しろ/せよ 可以看到，这两个单词的活用分别涉及カ行、サ行的各段，所以叫カ行、サ行变格活用，但并没有涉及全部五段或只涉及一段。补助动词补助动词是一类特殊的动词，它的作用是补在其他用言（的某种形态）或助动词（见下，类似用言）后面得到新的动词，此动词应该看作一个整体的复合动词。以下是一些常见的补助动词：～ある： 接在断定助动词だ的连用形で后：である 接在形容词连用形：例如ありがとうございます～てくださる： 接在动词连用形后表示敬意。～する： 接在动词连用形后面：xxする，称为サ变复合动词，意味相当于英语 doing sth。有的书把剩下的这些カ变动词、サ变动词及其衍生的サ变复合动词合称为三类动词。～ている： 接在动词连用形后面：表示动词的动作正在进行。（从词源上来看，て是接续助词，起到用言与补助用言的连接作用。）形容词日语形容词也有多个形态，除基本形外，有五个活用形，与动词相比少了命令形，用于不同的意义和用法。形容词活用的分类形容词也可以根据词末尾的音分为 2 类，每一类都有自己的活用形的变化规则，变化的也是词尾。这些类包括一类形容词、二类形容词。基本形结尾为い的形容词叫一类形容词，活用变化规则如下： 未然形：い变かろ/から（二者有不同的应用场景，见下） 连用形：い变く/かっ（二者有不同的应用场景，见下） 终止形、连体形：不变 假定形：い变けれ基本形结尾为だ的形容词叫二类形容词（有的书称为形容动词，原因有点复杂这里不解释），活用变化规则如下： 未然形：だ后加ろ 连用形：だ后加促音っ/だ变で/だ变に（三者有不同的应用场景，见下） 终止形：不变 连体形：だ变な 假定形：だ后加ら补助形容词补助形容词是一类特殊的形容词，它的作用是补在其他用言（的某种形态）后面得到新的形容词，此动词应该看作一个整体的复合形容词。助动词助动词在意义上很像动词或形容词，可以看作是一种用言；但它没有较实际的含义，接在用言（动词或形容词）后面，属于附属词。助动词是其实充当了补语的角色，对动词意义进行了补充精细的说明。与英语类比的话，动词如果是 do，那么接助动词就相当于对 do 的修饰：don’t、want to do、should do、may do、be done、did 等等。所以助动词是包罗万象的，把英语中的否定、祈使、情态动词、时态、被动语态等等围绕动词的变形全部归结为了接在动词后的这个助动词体系里。所以，助动词按照意义可以分为使役、被动、可能、否定、希望，等等，每一类的每一个词都是可以构入语法体系的必要的知识。助动词不仅在意义上，形式上也像动词或形容词，即词尾。因此助动词有活用形，对应的活用变化规则上也与动词或形容词一致。由此助动词根据分为动词活用型助动词、形容词活用型助动词。不能类比到动词或形容词的叫特殊活用型助动词。 由于总是接在用言后面，而且有活用形，语法书不将划为助词，而是定义为单独的词类。此外，不再区分出“助形容词”的概念，所以要注意不要被“助动词”的字面意思迷惑，认为只能接在动词后面。 助动词在意义上很像补助动词、补助形容词，但语法功能不一样！补助动词、补助形容词仍是动词、形容词，是独立词；而助动词是附属词。从意义上看，加补助的用言应当看成复合用言，是一个整体，而加助动词的用言应当看成用言的一种形态（不是指活用形）。由于助动词形式上像用言，所以很多助动词后面还可以再接助动词，由此递归下去。在意义上，相当于对原始用言的进一步补充说明。助动词是附属词，接多个也符合日语文节的规定。这样得到的文节形式如下：动词或形容词（有且只有一个） + 助动词（若干个，可以没有）被接的词（可能是动词或形容词，也可能是助动词）使用活用形，使用什么活用形要遵循后接的具体的助动词的规定，一般包括未然形、连用形、连体形、终止形，不包括假定形和命令形。以下会按类逐一介绍重要的助动词，它们的意思只做简单介绍（主要放在句子成分部分讲），重点是接续法（前面该接用言或助动词的什么形）、活用型。1. 使役助动词使役助动词表示使役，让主语做动作，相当于英语 let, make。せる、させる： 接续法：必须接动词，未然形，五段活用动词和サ变动词接せる，其他接させる 活用型：下一段动词活用型（由于せ是え段音，最后一个是る）2. 被动助动词被动助动词表示被动，构成被动句，相当于英语被动语态れる、られる： 接续法：必须接动词，未然形，五段活用动词和サ变动词接れる，其他接られる 活用型：下一段动词活用型（由于れ是え段音，最后一个是る）3. 可能助动词可能助动词表示可能，自发，相当于英语 can, mayれる、られる： 接续法：必须接动词，未然形，五段活用动词和サ变动词接れる，其他接られる 活用型：下一段动词活用型（由于れ是え段音，最后一个是る）4. 敬语助动词敬语助动词表示尊敬，分为以下两类，前者…，后者…れる、られる： 接续法：必须接动词，未然形，五段活用动词和サ变动词接れる，其他接られる 活用型：下一段动词活用型（由于れ是え段音，最后一个是る）ます： 接续法：接动词或动词活用型助动词，连用形（有的书称连用形接敬语助动词ます为ます形） 活用型：特殊活用型 未然形：ませ/ましょ 连用形：まし 终止形、连体形：不变 假定形：ますれ 命令形：ませ/まし 5. 否定助动词否定助动词表示否定，构成否定形式。分为两个：前者，后者。ない： 接续法： 接动词或动词活用型助动词，未然形 接サ变动词要用未然形し 活用型：一类形容词活用型ぬ（ん）： 接续法： 接动词或动词活用型助动词，未然形 接サ变动词要用未然形せ 助动词ます只能接ぬん，要省略ぬ音，即ません 活用型：特殊活用型 连用形：ず 终止形、连体形：ぬ 假定形：ね 6. 希望助动词希望助动词表示希望或愿望，即英语 hope、wish，有两个：。たい： 接续法：接动词或动词活用型助动词，连用形 活用型：一类形容词活用型たがる： 接续法：接动词或动词活用型助动词，连用形 活用型：五段动词活用型7. 过去助动词过去助动词表示过去，构成过去式。た： 接续法：接动词、形容词或助动词，连用形 活用型：特殊活用型 未然形：たろ 终止形、连体形：ぬ 假定形：たら 8. 断定助动词断定助动词表示“是”的意思，相当于英语 be。だ： 接续法： 接体言； 接一类形容词，终止形 其他复杂的接续法 活用型：二类形容词活用型 だ也可以接二类形容词的词干（去掉词尾だ），由于二类形容词的词干就是对应的名词，所以属于接体言。这里有意思的是词尾就是这个断定助动词だ，说明二类形容词是形容性的名词加上助动词だ，直译为“是名词那样子”。所以二类形容词的活用型本质上是助动词だ的活用型。 这个助动词很特殊，它不接用言性质的动词，而主要接体言性质的名词、形容词等，类似于英语“系动词”的概念。9. 推量助动词推量助动词表示推测或推断。う、よう： 接续法：接动词、形容词或助动词，未然形，非五段活用动词、下一段动词活用型助动词接よう，其他接う 活用型：词形不变まい： 接续法： 接五段活用动词与助动词ます，终止形； 接非五段活用动词与助动词せる、させる、れる、られる，未然形 活用型：词形不变らしい： 接续法：比较复杂 活用型：一类形容词活用型10. 比况助动词比况助动词表示比喻。ようだ、みたいだ： 接续法： 接体言+助词の 或 连体词　xの 接动词、形容词与部分助动词，连体形 活用型：二类形容词活用型11. 样态助动词样态助动词表示感觉，相当于英语 feel like。そうだ： 接续法：比较复杂 活用型：二类形容词活用型用言担任的句子成分动词作谓语动词可以不接助动词直接作谓语，也可以接助动词作谓语。最后一个词可能是动词，也可能是助动词，它们允许使用如下的活用形，表达不同的语气： 未然形：表示可能、不确定的语气，意为“吧”； 终止形：表示普通的语气； 假定形：表示假定语气，后面必须和接续助词ば连用，意为“如果”（有的书称动词假定形接接续助词ば为动词ば形）； 命令形：表示命令语气。形容词作谓语一类形容词的终止形接断定助动词作谓语。二类形容词的基本形可以直接作谓语，因为词尾だ可以看作断定助动词，接在词干表示的名词后。断定助动词的活用形也像上面动词作谓语时最后一个词一样，根据不同的活用形，表达不同的语气。区别是断定助动词没有命令形，不能表示命令语气（从意义上讲是合理的，不能命令一个东西是什么样的，而是“变成”什么样，变成有专门的动词）。时态：时与体与英语一样，日语的时态也体现在谓语的变化形上，但不是动词本身，而是体现在助动词的变形上。上面已经看到过去助动词た，还有更多的其他用法，在本节一并总结。日语将时态分为时与体两部分，例如英语时态“过去进行时”，“过去”对应时，“进行”对应体。时有两种，现在时、过去时，区别在于接不接过去助动词た。麻烦的是它与其他助动词组合的优先级顺序： 断定助动词 一类形容词：在断定助动词前面，形式： 二类形容词：由于断定助动词是词语的一部分，只能在断定助动词后面。 体言：与二类形容词性质相同，以下不再赘述 否定助动词 动词：在否定助动词后面； 一类形容词：在否定助动词后面； 二类形容词、体言：在否定助动词后面 体有开始、正在进行、完成、一直在进行等，描述的是动作，所以只有动词作谓语时有，形容词和体言没有这一概念。它是通过动词接补助动词实现的。以下列举一些表示体的补助动词： 开始：～始める、～だす、～かける 正在进行：～ている 一直在进行：～ている、～てある 完成：～てしまう、～終わる以上补助动词全部是接在动词连用形后面。这些都是补助动词，不是助动词，所以优先级是最高的，需要第一个先与动词结合。（也由此可知，体是先于时的）形容词作连体修饰语形容词的连体形直接作连体修饰语。形容词作连用修饰语形容词的连用形直接作连用修饰语。动词作连体修饰语助词助词是附属词，按照能接续的独立词分为几类： 格助词：接在体言后面，决定体言的格（即体言在句中的地位）； 一共有九个：が、の、を、に、へ、と、から、より、で 接续助词：接在用言（的某种活用形）后面，通常起到承前启后的作用（很像连词，但是黏着在独立词后的助词，不能构成小句）； 例子：て、たり、ば、ては、から、なり、ても、たって 副助词：什么都接，作用是将被接的词副词化； 例子：は、も、こそ、ほど、など、か、や 终助词：什么都接，但必须在句尾，用于表示句子的各种语气（相对来说比较独立，像感叹词一样）。有一些起相同作用的助词可以单独拎出来说： 表示并列的助词，用于构造体言或用言的对等文节： 只表体言的并列：と、に，属格助词 只表用言的并列：て、し、たり，属接续助词 表各类词的并列：や、やら、か、とか、だの、の，属副助词 系助词，属副助词，为前面接的各种成分增强陈述能力，例如：は、も、こそ、ぞ其他词类以下是剩下的用法比较简单的词类，语法知识不太多。连体词可认为是简化用法的形容词，只能单独作连体修饰语用，且后面不加附属词。副词只能单独作连用修饰语用（或者修饰副词），且后面不加附属词。以下是作且只能作独立语的词类：连词（有的书称为接续词）起到连接文节（注意不是单个词，要与表示并列的助词区分开）或单句的作用。连词很多是一些助词独立出来形成的。感叹词只起到感叹作用。词语的变化形式音便音便是指当一些音连在一起读时，为了发音方便而变化某些音。它一般取决于音的排列关系，与语法关系不大。下面列举一些： 五段活用动词的音便：五段活用动词的连用形接た、て、たり时， イ音便：若词在カ、ガ行，词尾改为い；（对ガ行）后续词浊化：だ、で、だり 拨音便：若词在ナ、バ、マ行，词尾改为拨音ん；后续词浊化 促音便：若词在タ、ラ、ワア行，词尾改为促音っ う音便：若词在ワア行，词尾改为う 一类形容词的う音便：一类形容词连用形接补助动词～ございます或～存じます时，词尾く变成う转用各词类按照一定的规律（构词法）可以转换成其他词类，称为转用。这里只讨论特别有规律的用言相关的转用。单个词直接转化： 动词变名词 动词连用形作名词 形容词变名词 一类形容词的词干作名词 一类形容词连用形作名词 二类形容词的词干本身就是名词 形容词变副词 一类形容词连用形（词尾い变く）作副词 两个或多个词复合起来转化：复合的基本元素有： 名词 动词连用形 形容词词干组合情况太多，在此不列举了。 在组合的时候，有时会省略词尾的假名（例如动词连用形的い段音），只用汉字，但是该假名还要发音，相当于假名合并到汉字里面了。敬语敬语应当看成词语的变化形式，从一个词衍生出的另一个词。各种词类都有一些词可衍生出对应的敬体（原词称为简体）。可以对词语进行变形，也可以加前缀或后缀。敬语根据表达敬意的区别分为三类： 尊敬语：涉及对方时，使用尊敬语表示对对方的尊敬 谦让语：涉及自己的，使用谦让语表示自己的谦逊 郑重语（丁宁语）：郑重地、有礼貌地、客气地陈述任何事情以下列举几个比较重要的。以上内容中不涉及任何敬语词（除了敬语助动词），我统一放在这里讲解了。 断定助动词だ的敬体是です，属丁宁语，是特殊活用。此道理同样适合于二类形容词。 未然形：でしょ 连用形：でし 终止形：不变 连体形：不变 敬语助动词ます属丁宁语 补助动词～ある的敬体是～ございます。属丁宁语。（它其实是〜ある的敬语〜ござる的连用形ござい接敬语助动词ます） 补助动词～てくれる（表示对方给予自己的动作）的敬体是～てくださる，属尊敬语。（通常使用命令形～てください，用于表命令的语句。）" } ]
