<!DOCTYPE html><html lang="zh-CN" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="prefer-datetime-locale" content="zh-cn"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="代码学习笔记：从论文 HAT 学会写持续学习代码" /><meta property="og:locale" content="zh_CN" /><meta name="description" content="本文是我看一篇论文代码整理的总结。下面这篇论文（简称 HAT）是持续学习领域的一个经典工作，我曾经以此论文为样板，完整仔细地看过整个项目的代码，从而理解并开始自己实现持续学习的实验的，现将心得整理于此。" /><meta property="og:description" content="本文是我看一篇论文代码整理的总结。下面这篇论文（简称 HAT）是持续学习领域的一个经典工作，我曾经以此论文为样板，完整仔细地看过整个项目的代码，从而理解并开始自己实现持续学习的实验的，现将心得整理于此。" /><link rel="canonical" href="https://pengxiang-wang.github.io/posts/codenotes_HAT/" /><meta property="og:url" content="https://pengxiang-wang.github.io/posts/codenotes_HAT/" /><meta property="og:site_name" content="Shawn Wang" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-10-09T00:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="代码学习笔记：从论文 HAT 学会写持续学习代码" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-05-30T13:54:40+08:00","datePublished":"2022-10-09T00:00:00+08:00","description":"本文是我看一篇论文代码整理的总结。下面这篇论文（简称 HAT）是持续学习领域的一个经典工作，我曾经以此论文为样板，完整仔细地看过整个项目的代码，从而理解并开始自己实现持续学习的实验的，现将心得整理于此。","headline":"代码学习笔记：从论文 HAT 学会写持续学习代码","mainEntityOfPage":{"@type":"WebPage","@id":"https://pengxiang-wang.github.io/posts/codenotes_HAT/"},"url":"https://pengxiang-wang.github.io/posts/codenotes_HAT/"}</script><title>代码学习笔记：从论文 HAT 学会写持续学习代码 | Shawn Wang</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Shawn Wang"><meta name="application-name" content="Shawn Wang"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src=" /assets/img/avatar.jpg " alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Shawn Wang</a></div><div class="site-subtitle font-italic">WPX 的个人主页</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>首页</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>分类</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>标签</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>时间表</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>关于本站</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/pengxiang-wang" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://space.bilibili.com/88684674" aria-label="bilibili" target="_blank" rel="noopener"> <i class="fas fa-tv"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['shawn.pxwang','qq.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> 首页 </a> </span> <span>代码学习笔记：从论文 HAT 学会写持续学习代码</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> 文章</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> </span> <span id="search-cancel" >取消</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>代码学习笔记：从论文 HAT 学会写持续学习代码</h1><div class="post-meta text-muted"><div> 作者 <em> <a href="https://github.com/pengxiang-wang">Shawn Wang</a> </em></div><div class="d-flex"><div> <span> 发表于 <em class="timeago" data-ts="1665244800" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-10-09 </em> </span> <span> 更新于 <em class="timeago" data-ts="1685426080" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2023-05-30 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="7513 字"> <em>41 分钟</em>阅读</span></div></div></div><div class="post-content"><p>本文是我看一篇论文代码整理的总结。下面这篇论文（简称 HAT）是持续学习领域的一个经典工作，我曾经以此论文为样板，完整仔细地看过整个项目的代码，从而理解并开始自己实现持续学习的实验的，现将心得整理于此。</p><p>这篇总结将详细介绍代码的各个细节，目的是一站式搞懂一个持续学习项目乃至深度学习项目的写法，也提供了一种阅读他人代码的思路，供刚入门该领域的同学参考。我将按照自外到内的顺序，先介绍整个工程的逻辑，到主函数，再到具体函数或类的细节，剥洋葱式地讲解。</p><p>这个项目的代码有些地方写的乱或不规范，注意取其精华，掌握思想，并了解代码不合理的地方在哪。</p><p>看懂这篇笔记的先修条件是掌握 Python 和 PyTorch，以及 Linux 系统的基本使用，并了解深度学习和持续学习的基础知识，请参考我的相关笔记：</p><ul><li><a href="https://pengxiang-wang.github.io/posts/studynotes_Python/">Python 学习笔记</a>；<li><a href="https://pengxiang-wang.github.io/tags/PyTorch/">PyTorch 学习笔记系列</a>；<li><a href="https://pengxiang-wang.github.io/posts/studynotes_Linux/">Linux 学习笔记</a>；<li><a href="https://pengxiang-wang.github.io/posts/continual_learning/">持续学习基础知识</a>；<li><a href="">基于 mask 的持续学习</a>。</ul><h1 id="论文信息">论文信息</h1><h3 id="overcoming-catastrophic-forgetting-with-hard-attention-to-the-task"><span class="mr-2"><a href="https://proceedings.mlr.press/v80/serra18a.html"><span class="mr-2">Overcoming Catastrophic Forgetting with Hard Attention to the Task</a></span><a href="#overcoming-catastrophic-forgetting-with-hard-attention-to-the-task" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>会议：ICML 2018<li>作者：西班牙巴塞罗那的大学<li>内容：持续学习模型 HAT，是将 mask 机制加到持续学习的第一篇论文，提出了一个很简单的、每个神经元引入一个任务 mask 的方法，并给出了训练方法，和一个解决模型容量问题的稀疏正则项，让新旧任务 mask 重合。它属于参数隔离方法，之后很多带 mask 机制的持续学习论文以此篇为基础。</ul><p>论文代码：<a href="https://github.com/joansj/hat">https://github.com/joansj/hat</a></p><hr /><h1 id="工程逻辑">工程逻辑</h1><p>我们从根目录开始，<code class="language-plaintext highlighter-rouge">src</code> 文件夹存放的是真正的代码，我们稍后讨论。根目录下的其他文件都是与代码没有直接关系的：</p><ul><li><code class="language-plaintext highlighter-rouge">LICENSE</code>: 一个文本文件（可以看到没有后缀名），打开可以看到，里面的文字是在声明版权，告诉他人可以怎么用此项目、禁止怎么用（否则追究责任）。一般项目都会在根目录放一个这种名为 LICENSE 的文件，里面声明性的文字不需要自己写，去网上查查各种常见的选项（如 CC 4.0、MIT License 等），选一个合适的复制过来就好。在 Github 创建项目时有个选项可以选 license，之后会自动在根目录里创建 LICENSE 文件文本，很方便。在项目代码中大家一般是这样做，在其他例如文章、博客中也有其他方式声明版权，起到相同的作用，例如，可以看看我这个博客每篇文章结尾有一段话：“本文由作者按照 CC BY 4.0 进行授权，转载请注明”，点击就能跳转到 CC 4.0 指示的版权声明文本。<li><code class="language-plaintext highlighter-rouge">readme.md</code>: 顾名思义就是“请先读我”，它是作者写的对项目的描述性文本，给用户看的。可以在这里写任何想跟用户说的，例如使用说明等信息。在 Github 项目首页也是默认展示这段文本（在 Github 创建项目时可以选择是否 add a README file）。众所周知在电脑上做笔记用 Markdown 格式很方便，现在的代码项目也是用它写（语法请自行学习）readme，而不是 word 之类的文件，因为技术上可以方便地接到网页上显示（例如我的博客每一篇文章都是 Markdown 格式写的）。本项目由于是一篇论文的代码，所以作者主要在这里写了论文的信息，并简要介绍了程序的安装和运行方法。<li><code class="language-plaintext highlighter-rouge">requirements.txt</code>: 一个文本文件，列举了代码所需要的环境，放在项目根目录中，告诉别人运行此项目需要装什么第三方库。注意这个不一定是手打的或复制的，而是可以通过 Pip 或 Conda 自动生成的，此时具有一定的语法（当然有时候自动生成的太过详细反而不合适，于是只需要手打一些重要的即可。本项目就做的不太好，把其他无关的环境里的包也包含进来了）。以下是相关命令：<ul><li><code class="language-plaintext highlighter-rouge">pip freeze &gt;requirements.txt</code> 或 <code class="language-plaintext highlighter-rouge">conda list -e &gt;requirements.txt</code>: 生成环境列表到 <code class="language-plaintext highlighter-rouge">requirements.txt</code>；<li><code class="language-plaintext highlighter-rouge">pip install -r requirements.txt</code> 或 <code class="language-plaintext highlighter-rouge">conda install --file requirements.txt</code> 或 <code class="language-plaintext highlighter-rouge">conda create --name XXX --file requirements.txt</code>: 安装 requirements.txt 的环境到当前环境或新建环境。</ul><li><code class="language-plaintext highlighter-rouge">.gitignore</code>: 一个文本文件，表示在上传到 Github 时应该忽略哪些文件（语法请自行学习），注意它用 <code class="language-plaintext highlighter-rouge">.</code> 开头，在 Linux 或 Mac 系统中代表隐藏文件，下同。这些文件通常是临时的、runtime 的或结果性质的非代码、非必要的文件，不需要上传占用空间让别人看到。在 Github 创建项目时可以选择是否 add .gitignore。本项目忽略上传的文件有（其中一些文件在运行之后才会出现）：<ul><li><code class="language-plaintext highlighter-rouge">logs</code> 文件夹: 存放保存的实验结果文件（见“处理实验结果”一节）。它是非代码性质的数据文件，无需上传；<li><code class="language-plaintext highlighter-rouge">dat</code> 文件夹: 存放深度学习的数据集。它是非代码性质的数据文件，并且占用大量空间，不能上传；<li><code class="language-plaintext highlighter-rouge">res</code> 文件夹: 存放另外一些实验结果（见“处理实验结果”一节）；<li><code class="language-plaintext highlighter-rouge">old</code>、<code class="language-plaintext highlighter-rouge">temp</code> 文件夹: 作用未知，但看名字应该是临时文件；<li>其他无关紧要的文件：<code class="language-plaintext highlighter-rouge">src/.idea</code> 文件夹:（使用 IDE PyCharm 的配置文件）；<code class="language-plaintext highlighter-rouge">src/__pycache__</code> 文件夹（Python 缓存文件）；<code class="language-plaintext highlighter-rouge">pyc</code> 类型文件（py 文件编译后的二进制文件）；<code class="language-plaintext highlighter-rouge">.DS_Store</code> 类型文件（苹果电脑的文件系统配置文件）；<code class="language-plaintext highlighter-rouge">.png</code> 格式的文件；两个脚本文件（<code class="language-plaintext highlighter-rouge">src/work.sh</code>、<code class="language-plaintext highlighter-rouge">src/immalpha.sh</code>），可能是作者写了试的但最终没用。</ul></ul><p>下面看代码的 <code class="language-plaintext highlighter-rouge">src</code> 文件夹。有四个文件起到<strong>程序入口</strong>的作用：</p><ul><li><code class="language-plaintext highlighter-rouge">run.py</code>: 是最基本的主函数，负责运行一次深度学习实验；<li><code class="language-plaintext highlighter-rouge">run_multi.py</code>: 是用 <code class="language-plaintext highlighter-rouge">run.py</code> 改写的，负责运行多次深度学习实验；<li><code class="language-plaintext highlighter-rouge">work.py</code>：作者写的另一个可以运行多次深度学习实验的程序；<li><code class="language-plaintext highlighter-rouge">run_compression.sh</code>: 负责运行模型压缩（compression）实验的程序，见论文 4.4 节。它是 Linux 系统的 shell 脚本命令（即命令行中的单个命令组合成的打包命令）。可以看到，这里它包含了多行固定的运行 <code class="language-plaintext highlighter-rouge">run.py</code> 的命令行命令，当运行 <code class="language-plaintext highlighter-rouge">run_compression.sh</code> 时，相当于运行了里面写的这些命令。</ul><p>其他的是具体的模块代码：</p><ul><li><code class="language-plaintext highlighter-rouge">approaches</code> 文件夹：定义各种持续学习算法，每个算法是一个 py 文件；<li><code class="language-plaintext highlighter-rouge">dataloaders</code> 文件夹：定义了数据预处理方法，每个数据集是一个 py 文件。由于持续学习数据集一般是现有数据集构造的，这里也定义了如何构造数据集；<li><code class="language-plaintext highlighter-rouge">networks</code> 文件夹：定义了神经网络结构，每个结构是一个 py 文件；<li><code class="language-plaintext highlighter-rouge">plot_results.py</code>: 可视化实验结果的工具（见“处理实验结果”一节）.本项目是先把结果存下来，再在需要时单独对其可视化。可视化与核心业务分离，这个 py 文件就是单独的可视化程序；<li><code class="language-plaintext highlighter-rouge">utils.py</code> 文件：存放各种工具函数，为了不让主要部分的代码过长，如打印函数，计算某个量等。</ul><h1 id="主程序">主程序</h1><p><code class="language-plaintext highlighter-rouge">run.py</code> 是整个项目的核心，它完成一次深度学习实验的整个流程。</p><h2 id="解析命令行参数"><span class="mr-2">解析命令行参数</span><a href="#解析命令行参数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>一次深度学习实验需要指定很多东西：数据集、网络结构、学习算法、各种超参数，还有一些细节的配置如随机数的种子、输出格式等。这些信息一般是不出现在代码里的，而是作为运行程序时用户指定的参数，即命令行参数。关于如何使用 Python 命令行参数，我在<a href="https://pengxiang-wang.github.io/posts/studynotes_Python_argparse/">这篇博文</a>有详细讨论。</p><p><code class="language-plaintext highlighter-rouge">run.py</code> 定义命令行参数的部分在 9-20 行，解析命令行参数在 29-97 行。可以看到，它定义了如下 7 个命令行参数：</p><ul><li><code class="language-plaintext highlighter-rouge">--seed</code>: 见“随机数种子”一节；<li><code class="language-plaintext highlighter-rouge">--experiment</code>: 解析时通过 if 语句手动对应选择选项。根据命令行选项，把 <code class="language-plaintext highlighter-rouge">dataloaders</code> 文件夹中的模块统一解析到名为 <code class="language-plaintext highlighter-rouge">dataloader</code> 的变量中，在下面统一调用；<li><code class="language-plaintext highlighter-rouge">--approach</code>: 解析时通过 if 语句手动对应选择选项。根据命令行选项，把 <code class="language-plaintext highlighter-rouge">approaches</code> 文件夹中的模块统一解析到名为 <code class="language-plaintext highlighter-rouge">approach</code> 的变量中，在下文统一调用；<li><code class="language-plaintext highlighter-rouge">--nepochs</code>: 训练轮数，是比较重要的超参数，需要用户手动指定；<li><code class="language-plaintext highlighter-rouge">--lr</code>: 学习率，是比较重要的超参数，需要用户手动指定；<li><code class="language-plaintext highlighter-rouge">--parameter</code>: 为其他超参数的预留位置（因为每个 approach 的超参数都可以有所不同），具体是解析成几个、什么超参数，要看具体 approach 的定义；<li><code class="language-plaintext highlighter-rouge">--output</code>: 指定输出结果文件名路径，见“处理实验结果”一节。</ul><h2 id="深度学习流程"><span class="mr-2">深度学习流程</span><a href="#深度学习流程" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>接下来的代码对应深度学习流程：</p><ul><li><strong>读取数据集</strong>（99-102行）：可以看到，所有 <code class="language-plaintext highlighter-rouge">dataloaders</code> 中的模块都只有一个 <code class="language-plaintext highlighter-rouge">get</code> 函数，在这里统一调用，用于得到数据集（包括训练集、验证集、测试集，作者的处理办法是先打包成一个 data 变量，再在训练或测试时抽离出来，见125、154等行），以及每个任务有几个类、输入维度等信息（用于定义网络）；<li><strong>网络结构初始化</strong>（104-107行）：可以看到，所有 <code class="language-plaintext highlighter-rouge">networks</code> 中的模块都只有一个 <code class="language-plaintext highlighter-rouge">Net</code> 类，在这里统一实例化为要训练的网络结构。实例化需要确定每个任务有几个类、输入维度等信息，来自上面数据集 <code class="language-plaintext highlighter-rouge">get</code> 函数的返回值；<li><strong>定义学习算法</strong>（109-112行）：可以看到，所有 <code class="language-plaintext highlighter-rouge">approaches</code> 中的模块都只有一个 <code class="language-plaintext highlighter-rouge">Appr</code> 类，在这里统一实例化为学习算法。粗略阅读其代码可发现，这种 <code class="language-plaintext highlighter-rouge">Appr</code> 类：<ul><li>不仅定义了持续学习的机制（因此实例化时需要传入持续学习有关的超参数，作者的做法是把 <code class="language-plaintext highlighter-rouge">args</code> 整个传进去，例如 <code class="language-plaintext highlighter-rouge">/approaches/hat.py</code> 中27-31行解析了 <code class="language-plaintext highlighter-rouge">args.parameter</code> 为 lamb 和 smax 两个超参数，用户在传 <code class="language-plaintext highlighter-rouge">--parameters</code> 时就知道 <code class="language-plaintext highlighter-rouge">--parameter</code> 代表这两个超参数）；<li>还把优化器和损失函数一并包进来定义，因此实例化时需要指定优化器的超参数、训练轮数等，这些都在命令行参数 <code class="language-plaintext highlighter-rouge">args</code> 里；<li>请注意，<code class="language-plaintext highlighter-rouge">Appr</code> 类还把网络也包进来作为实例属性了，从这里开始程序不再出现网络 <code class="language-plaintext highlighter-rouge">net</code> 变量；</ul><li><strong>训练</strong>（148-149行）：统一调用 <code class="language-plaintext highlighter-rouge">Appr</code> 类的 <code class="language-plaintext highlighter-rouge">train</code> 方法，它接受上面抽离出来的训练集和验证集，以及第几个任务这个信息。注意不需要传网络，它在 <code class="language-plaintext highlighter-rouge">Appr</code> 里面，这个训练函数本质上也是在修改更新它；<li><strong>测试</strong>（152-159行）：统一调用 <code class="language-plaintext highlighter-rouge">Appr</code> 类的 <code class="language-plaintext highlighter-rouge">eval</code> 方法，它接受上面抽离出来的测试集，以及第几个任务这个信息。注意这里外层有个 u 循环，是要测试所有任务的。仍然不需要传网络。</ul><h1 id="dataloaders">Dataloaders</h1><p>项目在 <code class="language-plaintext highlighter-rouge">dataloaders</code> 文件夹定义了数据集、预处理方法和构造持续学习任务的代码，每个数据集是一个 py 文件。每个文件都只定义了一个 <code class="language-plaintext highlighter-rouge">get</code> 函数，我们以持续学习经典的、较为简单的 <code class="language-plaintext highlighter-rouge">pmnist.py</code>（Permuted MNIST）为例来讲解。<code class="language-plaintext highlighter-rouge">get</code> 函数它返回如下内容：</p><ul><li>数据集变量为 <code class="language-plaintext highlighter-rouge">data</code>：是一个嵌套字典，即字典的值还是字典<ul><li>第一层（11行）为任务，键为任务 ID；<ul><li>有一个额外的键 ‘ncla’ 存放所有任务的类数之和（80行）；</ul><li>第二层（34行）为任务的元信息，包括：<ul><li>任务名字 ‘name’：作者命名为 ‘pmnist-任务ID’（35行）；<li>类的数量 ‘ncla’：在 Permuted MNIST 中，每个任务类的数量固定为 10（36行）；<li>训练、验证、测试数据 ‘train’, ‘valid’, ‘test’；</ul><li>第三层（39行）在数据集 ‘train’, ‘valid’, ‘test’ 里面：<ul><li>输入 ‘x’：一个大 Tensor，事实上本项目输入模型的数据集并不是用的 PyTorch 的 Dataloader，作者是手动划分 batch 的，例如，见 <code class="language-plaintext highlighter-rouge">approaches/sgd.py</code> 的 81-82 行；<li>标签 ‘y’：一个大 Tensor；</ul></ul><li>每个任务有几个类 <code class="language-plaintext highlighter-rouge">taskcla</code>（78行）：是一个列表，对 Permuted MNIST，它固定是 <code class="language-plaintext highlighter-rouge">[10,...,10]</code>；<li>输入维度 <code class="language-plaintext highlighter-rouge">size</code>（13行）：直接定义为常量 <code class="language-plaintext highlighter-rouge">[1,28,28]</code>。</ul><p>从 <code class="language-plaintext highlighter-rouge">get</code> 函数参数可以看到，作者没有为用户提供什么选择，一个 Permuted MNIST 数据集基本是固定的，用户只能设置：</p><ul><li>控制随机数种子的 <code class="language-plaintext highlighter-rouge">seed</code> 和 <code class="language-plaintext highlighter-rouge">fixed_order</code>：见“随机数种子”一节；<li><code class="language-plaintext highlighter-rouge">pc_valid</code>：验证集数据比例。</ul><p>下面来看 <code class="language-plaintext highlighter-rouge">data</code> 变量第三层的数据集是如何一步步构造的：</p><ul><li>首先通过 <code class="language-plaintext highlighter-rouge">torchvision.datasets</code> 将原始的 MNIST 数据集下载到 <code class="language-plaintext highlighter-rouge">dat</code> 变量中（27-30行），再一步步解析到 <code class="language-plaintext highlighter-rouge">data</code> 中；<li>用原始数据集 <code class="language-plaintext highlighter-rouge">dat</code> 构造 batch=1 的 Dataloader（38行，应该是为了方便写循环），逐张图片作 permute 操作（41-43行），添加到 <code class="language-plaintext highlighter-rouge">data</code> 中。注意此时 <code class="language-plaintext highlighter-rouge">data</code> 的数据部分 ‘x’,’y’ 现在是列表；<li>将此时的列表数据保存下来（20-21、51-52行），以后可以直接读取（55-67行）。究其原因，是前面逐张图片的处理操作太慢了，哪怕保存读取也更节省时间；<li>将列表转换为可以输入到 nn.Module 的 Tensor（48-52行）；<li>注意上面只是分了训练集和测试集，还要从训练集 ‘train’ 中划分验证集 ‘valid’（70-73行）。注意作者在 pmnist.py 中验证集是直接复制了训练集，也就是说模型选择是按照训练集上最好的来选的。我不知道是因为懒还是别的原因，但这样是容易过拟合的，越小的数据集更是如此。</ul><p>其他的数据集大同小异，我简要介绍之，主要关注区别：</p><ul><li><code class="language-plaintext highlighter-rouge">mnist2.py</code>：是 2 个任务的 Split MNIST 数据集。由于不涉及逐张 Permute 操作，作者也没有在中间设计保存读取；<li><code class="language-plaintext highlighter-rouge">cifar.py</code>：是 10 个任务的 Split CIFAR 数据集，前 5 个任务用 CIFAR10 数据集，每个任务有 2 个类；后 5 个任务用 CIFAR100 数据集，每个任务有 20 个类。对此数据集作者终于随机划分了训练集给验证集（79-90行），比例 <code class="language-plaintext highlighter-rouge">pc_valid</code> 在 <code class="language-plaintext highlighter-rouge">get</code> 函数参数由用户指定；<li><code class="language-plaintext highlighter-rouge">mixture.py</code>：是很多种数据集的混合，有 8 个任务，每个任务是一种数据集，分别是 CIFAR10、CIFAR100、MNIST、SVHN、FashionMNIST、TrafficSigns、Facescrub、notMNIST（不是按顺序，而是固定地随机打乱）。这里有些数据集是 <code class="language-plaintext highlighter-rouge">torchvision.datasets</code> 没有的，作者在下面定义了相应的数据集类（相当于自定义 Dataset 类）。</ul><p>下面浅看一下作者是怎么自定义数据集的（在 <code class="language-plaintext highlighter-rouge">mixture.py</code>）：</p><ul><li>FashionMNIST：实际上 <code class="language-plaintext highlighter-rouge">torchvision.datasets</code> 是有这个数据集的，可能是作者在使用其 API 时遇到了 bug，然后自己重写了一个（249-257行）；<li>TrafficSigns、Facescrub、notMNIST：都继承自 <code class="language-plaintext highlighter-rouge">Dataset</code> 类，写法遵从<a href="">此笔记</a>讲的自定义方法。<code class="language-plaintext highlighter-rouge">__init__()</code> 函数中从本地文件读取整个数据集到 <code class="language-plaintext highlighter-rouge">data</code> 和 <code class="language-plaintext highlighter-rouge">labels</code> 变量，然后在 <code class="language-plaintext highlighter-rouge">__getitem__()</code> 直接索引。与 MNIST 类似，<code class="language-plaintext highlighter-rouge">download</code> 和 <code class="language-plaintext highlighter-rouge">train</code> 参数控制下载和选择训练还是测试集。下载操作需要复杂的网络通讯和纠错机制，也是打包在一个 <code class="language-plaintext highlighter-rouge">download()</code> 函数。通过判断 <code class="language-plaintext highlighter-rouge">train=True</code> 的条件语句，选择读取训练还是测试数据集。</ul><h1 id="networks">Networks</h1><p>项目在 <code class="language-plaintext highlighter-rouge">networks</code> 文件夹的代码定义了网络结构，每个模型是一个 py 文件。每个文件都只定义了一个 <code class="language-plaintext highlighter-rouge">nn.Module</code> 名为 <code class="language-plaintext highlighter-rouge">Net</code> 的类。会写这些 <code class="language-plaintext highlighter-rouge">nn.Module</code> 类是深度学习的基础，请参考<a href="">此笔记</a>。每个深度学习项目都大同小异，大都用到 MLP、AlexNet、ResNet 等网络，写法也差不多。</p><p>我们更需要关注的是网络结构是如何适配持续学习的场景或方法的。在我看来有两点：</p><ul><li>问题一：如何处理持续学习场景中新任务新来的类（输出头）；<li>问题二：对 HAT 这种 model-based 的持续学习方法，涉及修改网络结构，怎么改。 我们以较简单的 MLP 为例来看作者是如何处理的，见 <code class="language-plaintext highlighter-rouge">mlp.py</code> 和 <code class="language-plaintext highlighter-rouge">mlp_hat.py</code>。</ul><p>对于问题一，作者是事先把所有任务的输出头都定义好（19-21行），前馈时也会输出所有输出头结果的拼接（30-32行）；而不是每来一个新任务动态地增加输出头，因为这是在做一个固定的实验，这样写代码比较方便。什么时候、什么任务用那个输出头，这些都定义在持续学习方法 <code class="language-plaintext highlighter-rouge">approach</code> 的训练和测试函数中。另外，即使像 Permuted MNIST 这样所有任务类别相同的数据集，也是每个任务给一个自己的输出头，而不是共用相同的输出头。</p><p>对于问题二，不可避免地要对每种网络结构衍生出一个修改版本，例如本项目中 <code class="language-plaintext highlighter-rouge">mlp.py</code> 衍生出 <code class="language-plaintext highlighter-rouge">mlp_hat.py</code>，<code class="language-plaintext highlighter-rouge">alexnet.py</code> 衍生出 <code class="language-plaintext highlighter-rouge">alexnet_hat.py</code>, <code class="language-plaintext highlighter-rouge">alexnet_pathnet.py</code>, <code class="language-plaintext highlighter-rouge">alexnet_progressive.py</code>, <code class="language-plaintext highlighter-rouge">alexnet_lfl.py</code> 等，者都是涉及修改网络结构的 model-based 方法，在使用这些 model-based 方法时，要求使用相应的网络结构。</p><p>来看一下 <code class="language-plaintext highlighter-rouge">mlp_hat.py</code>，它定义了 HAT 方法的网络结构，即加了 mask 的 MLP。<code class="language-plaintext highlighter-rouge">mlp.py</code> 提供了一个 3 个隐藏层的 MLP，每层神经元个数相同；而 <code class="language-plaintext highlighter-rouge">mlp_hat.py</code> 提供了 1、2、3 个隐藏层的 MLP（由 <code class="language-plaintext highlighter-rouge">__init__</code> 函数的参数 <code class="language-plaintext highlighter-rouge">nlayers</code> 指定，观察下文代码它实际上只能取 1、2、3）。以 3 层 MLP 为例：</p><ul><li><code class="language-plaintext highlighter-rouge">__init__()</code> 函数中，比普通 MLP 多了三个 efc1、efc2、efc3（20、23、26行），即在每层神经元上的 task embedding，可见它们实现为 <code class="language-plaintext highlighter-rouge">nn.Embedding</code>，这个类用于表示一组长度相同的模型参数（称为 embedding），第一个参数 <code class="language-plaintext highlighter-rouge">num_embeddings</code> 为 embedding 的个数，第二个参数 <code class="language-plaintext highlighter-rouge">embedding_dim</code> 为 embedding 长度。这个类一般用于词向量表示（一组词库，每个词用一个 embedding 表示），但在这里作者用于表示<em>各个任务</em>的 task embedding 向量，注意每个 efc1、efc2、efc3 各自都是预定义好了所有任务各层的 embedding，而不是单个任务。<li>有了 task embedding，通过论文中的公式(1)：乘以尺度参数 \(s\) 再过 gate function 即 Sigmoid，得到 mask。这个过程打包成了一个 mask 函数（65-71行）；<li><code class="language-plaintext highlighter-rouge">forward</code> 函数中，比普通 MLP 多了 mask 的步骤：在每个神经元激活后乘以 mask（53、56、59行）。注意，这个 forward 函数不仅接受输入 \(x\)，还包括了任务 \(t\)和计算 mask 用的 \(s\)。也就是说，模型是在这里提供接口给训练和测试时确定第几个任务这个信息的，这个 <code class="language-plaintext highlighter-rouge">Net</code> 类是预定义好了所有任务，然后通过 <code class="language-plaintext highlighter-rouge">forward</code> 函数区分任务。</ul><p>此外还有一些细节。有 <code class="language-plaintext highlighter-rouge">alexnet_hat.py</code> 和 <code class="language-plaintext highlighter-rouge">alexnet_hat_test.py</code> 两个，它们区别在 43-50 行：是否对 task embedding 作归一初始化，与模型压缩实验有关。另外，有些带 hat 的 <code class="language-plaintext highlighter-rouge">Net</code> 类写了 <code class="language-plaintext highlighter-rouge">get_view_for</code> 函数，它使用 torch 的拉直操作（view）将一个 mask Tensor 拉直，属于 HAT 算法的一个工具函数，写在了模型类里，在 HAT 的 train 函数调用，见下文。</p><h1 id="approaches">Approaches</h1><p>项目在 <code class="language-plaintext highlighter-rouge">approaches</code> 文件夹的代码定义了各种持续学习算法的代码，每个算法是一个 py 文件。每个文件都只定义了一个名为 <code class="language-plaintext highlighter-rouge">Appr</code> 的类，其中都有训练和测试函数 <code class="language-plaintext highlighter-rouge">train</code>、<code class="language-plaintext highlighter-rouge">eval</code>以及定义的优化器 <code class="language-plaintext highlighter-rouge">self.optimizer</code>、损失函数 <code class="language-plaintext highlighter-rouge">self.criterion</code>。我们先来看不加持续学习防遗忘机制的微调算法 <code class="language-plaintext highlighter-rouge">sgd.py</code>，理清楚基本的训练和测试流程的细节。</p><ul><li>损失函数定义在 <code class="language-plaintext highlighter-rouge">self.criterion</code> 中。对于 sgd.py 这种简单的，直接在 <code class="language-plaintext highlighter-rouge">__init__</code> 函数规定了是 <code class="language-plaintext highlighter-rouge">nn.CrossEntropyLoss()</code>，对于 <code class="language-plaintext highlighter-rouge">hat.py</code> 等，在类中自定义了 <code class="language-plaintext highlighter-rouge">criterion</code> 函数；<li>优化器定义在 <code class="language-plaintext highlighter-rouge">self.optimizer</code> 中，是用 <code class="language-plaintext highlighter-rouge">__init__</code> 传入的优化器超参数 <code class="language-plaintext highlighter-rouge">lr</code> 等构造的（作者还多写了一层 <code class="language-plaintext highlighter-rouge">_get_optimizer</code>，可能是嫌定义优化器的代码太长）；<li><code class="language-plaintext highlighter-rouge">train</code> 函数是训练一个任务，的核心工作在 40 行调用 <code class="language-plaintext highlighter-rouge">train_epoch</code> 函数，定义在 72 行，它的任务是训练任务 t 的一个 epoch。它的流程与普通的深度学习训练过程没什么区别，唯一要注意的是它是在训练任务 t（这个信息当做函数参数传入），体现在 88 行结果截取输出头 t；其他部分代码都在做一件事——动态调整学习率，这是深度学习的训练技巧，我放在后面的章节专门讨论。<li><code class="language-plaintext highlighter-rouge">eval</code> 函数是测试当前模型（训练了任务 t 后）在一个任务上（反映在测试集上）的准确率，注意 <code class="language-plaintext highlighter-rouge">run.py</code> 有个外循环测试所有任务。这个写的和普通深度学习的测试没什么差别，最后返回测试 loss 和准确率。</ul><p>接下来看 HAT 算法 <code class="language-plaintext highlighter-rouge">hat.py</code>，它是在 <code class="language-plaintext highlighter-rouge">sgd.py</code> 基础上改的，并且要求传入的 <code class="language-plaintext highlighter-rouge">model</code> 必须是 <code class="language-plaintext highlighter-rouge">_hat</code> 版本的：</p><ul><li><code class="language-plaintext highlighter-rouge">criterion</code> 函数（196行）定义了 HAT 论文的 sparse 正则项（公式(5)）和分类损失。这个正则项需要用到旧任务 <code class="language-plaintext highlighter-rouge">&lt;t</code> mask 的合并，存放在 <code class="language-plaintext highlighter-rouge">self.mask_pre</code>。为了这个正则项，损失函数 <code class="language-plaintext highlighter-rouge">criterion</code> 不仅接受模型输出 <code class="language-plaintext highlighter-rouge">outputs</code> 和真实标签 <code class="language-plaintext highlighter-rouge">targets</code>，还要 <code class="language-plaintext highlighter-rouge">masks</code>。注意 if 语句区分了第一个任务的情况；<li>核心的 <code class="language-plaintext highlighter-rouge">train_epoch</code> 函数调用了加 mask 的 HAT 版 <code class="language-plaintext highlighter-rouge">forward</code> 函数，并计算了上述 <code class="language-plaintext highlighter-rouge">criterion</code> 定义的损失，再反向传播计算梯度。在更新之前：<ul><li>首先屏蔽不更新旧任务 mask 掉的参数，实现方式是梯度置 0。在 135 行，梯度乘以的 <code class="language-plaintext highlighter-rouge">mask_back</code> 就是旧任务 <code class="language-plaintext highlighter-rouge">mask_pre</code> 的反转（1-x），它在上一步就由 <code class="language-plaintext highlighter-rouge">mask_pre</code> 计算了出来（97-102行）；<li>接着应用论文 2.5 节的梯度补偿机制，将梯度乘以了一个补偿因子；<li>更新后再把训练好的 task embedding 统一收缩（clamp）到一个较小范围，这个在论文 2.5 节的最后一段提到。</ul><li><code class="language-plaintext highlighter-rouge">train</code> 函数（训练一个任务）最后包含了 task embedding 到 mask 的转换（87-90行）并计算旧任务 <code class="language-plaintext highlighter-rouge">mask_pre</code>（91-95行）的过程。可以看到，作者用同样的 float 类型的数据结构同时存放了 task embedding 和二元 mask：训练前是前者，训练后就用此处的代码转换成后者（因为 task embedding 再也不用，没必要存储了）；<li><code class="language-plaintext highlighter-rouge">eval</code> 函数：没有太大区别，多了对正则项的统计。</ul><p>除了上述算法，为了 baseline 的比较，还实现了其他与 HAT 类似的参数隔离方法如 PathNet、Progressive NN 等，也有其他方法如 EWC、IMM、LwF 等。它们的区别就在训练、测试、损失函数、优化器以及用到的相关变量，它们全都可以在一个 <code class="language-plaintext highlighter-rouge">Appr</code> 类写明白。</p><p>注意，这些方法只有 HAT 有 <code class="language-plaintext highlighter-rouge">_test</code> 版本的，它的意思是正式跑的程序，可以看到里面的代码更完善，它在 <code class="language-plaintext highlighter-rouge">__init__()</code> 函数定义了 <code class="language-plaintext highlighter-rouge">logs</code>、<code class="language-plaintext highlighter-rouge">logpath</code>（<code class="language-plaintext highlighter-rouge">hat_test.py</code> 29-59行，从命令行参数 <code class="language-plaintext highlighter-rouge">--parameter</code> 解析），如果定义了它们，从 <code class="language-plaintext highlighter-rouge">run.py</code> 的最后一段可以看到，会把测试的详细结果存作处理（见“处理实验结果”一节）。此外，<code class="language-plaintext highlighter-rouge">_test</code> 版本还有两个 <code class="language-plaintext highlighter-rouge">criterion</code> 函数，它们是一样的，这是作者整理代码时的疏忽，但可以看出来他是想在 <code class="language-plaintext highlighter-rouge">_test</code> 版本处理更多信息的。</p><h1 id="其他细节">其他细节</h1><h2 id="打印调试信息"><span class="mr-2">打印调试信息</span><a href="#打印调试信息" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>作者穿插了各种调试信息在代码中。一般是用 print 语句实现，对于复杂的，为了不想让主函数过长，代码打包在了 <code class="language-plaintext highlighter-rouge">utils.py</code> 中 print 开头的函数来调用。</p><p>我在这里按顺序整理一下作者穿插的调试信息，也可以帮助梳理总结一下上面的内容：</p><ul><li>23-27行：打印了用户指定的命令行参数，供用户确认；<li>102 行：打印数据集信息和持续学习的任务信息；<li>107 行：打印模型信息；<li>110-111行：打印损失函数和优化器信息；<li>119、176 行：打印训练进程、时间信息；<li>157、166-174 行：打印每一次测试结果、测试汇总结果；<li>162 行：打印结果保存的信息。</ul><p>除了上面讲述的整个流程，代码中还有很多细节需要我们注意。它们往往是很重要的事情。</p><h2 id="处理实验结果"><span class="mr-2">处理实验结果</span><a href="#处理实验结果" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>纵观整个代码，作者把以下实验结果保存到了本地文件中：</p><ul><li>命令行 <code class="language-plaintext highlighter-rouge">--output</code> 参数：它定义的路径存放的是测试准确率上三角矩阵 <code class="language-plaintext highlighter-rouge">acc</code>（161-163 行），t行u列表示训练完第t个任务时模型在任务u的准确率，这个是持续学习最主要的指标（见<a href="">持续学习基础知识笔记</a>）。命令行参数如果为空，作者定义了默认的文件路径（21-22 行），可以看到是用<code class="language-plaintext highlighter-rouge">--experiment</code>、<code class="language-plaintext highlighter-rouge">--approach</code>等元信息命名的，用于区分不同实验；<li><code class="language-plaintext highlighter-rouge">Appr</code> 类的 <code class="language-plaintext highlighter-rouge">logpath</code> 参数：只有 <code class="language-plaintext highlighter-rouge">hat_test.py</code> 出现，如果在 <code class="language-plaintext highlighter-rouge">--parameter</code> 包含这一部分，<code class="language-plaintext highlighter-rouge">run.py</code> 在 178 行之后把一些结果信息用 pickle 保存下来（pickle 是一个 Python 内置库，可以完整保存、还原任意 Python 变量），在需要画图的时候被 <code class="language-plaintext highlighter-rouge">plot_results.py</code> 还原调用。</ul><h2 id="随机数种子"><span class="mr-2">随机数种子</span><a href="#随机数种子" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>在 <code class="language-plaintext highlighter-rouge">run.py</code> 对种子做了全局设定（31-33行），在代码内部也有一些局部随机数变量的种子设定，如 <code class="language-plaintext highlighter-rouge">pmnist.py</code> 18 行的任务顺序。</p><h2 id="使用-gpu"><span class="mr-2">使用 GPU</span><a href="#使用-gpu" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>这种规模的深度学习实验一定是用 GPU 跑的。在代码里：</p><ul><li>34-35 行检查能不能用 GPU，不能用则强制退出；<li>106 行把模型放到了 GPU 上；<li>142-145、154-155 行把数据集放到了 GPU 上。</ul><h2 id="实验细节"><span class="mr-2">实验细节</span><a href="#实验细节" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>代码中有一些细节，是深度学习实验经常要做的：</p><ul><li>要做数据标准化：先手算了均值方差，再在构造数据集时应用 <code class="language-plaintext highlighter-rouge">transforms.Normalize</code> 变换，例如 <code class="language-plaintext highlighter-rouge">pmnist.py</code> 24-30 行；</ul><p>以下是代码中使用的一些调参技巧：</p><ul><li>动态调整学习率：每一轮都在验证集上测试（调用 <code class="language-plaintext highlighter-rouge">eval</code> 函数）一下 loss，如果连续 lr_patience 个 epoch 验证集 loss 一直不下降，则把学习率调小一点：除以 lr_factor。<li>Dropout 层防止过拟合：例如 <code class="language-plaintext highlighter-rouge">mlp.py</code> 第 15 行；<li></ul><p><br /></p><p>这就是一个科研用深度学习项目的全貌。看完了代码，也能感受到其中的不足之处，例如：</p><ul><li>调用格式不太统一；<li>用户可以指定的东西太少；<li>一些常用的参数藏得太深（例如 <code class="language-plaintext highlighter-rouge">--parameter</code> 的解析规则，尤其是 <code class="language-plaintext highlighter-rouge">appr.logs</code>），用户必须非常仔细阅读代码才知道怎么用；</ul><p>当然，科研用的代码以做出实验结果为目的，自己方便能看懂就行，不是产品，不需要呈现给用户，自己需要什么就写什么，挂在网上的目的只是在别人质疑的给他一个参考。这种性质也决定了作者没有必要写的更健全、完美，我们看下来也就理解其代码逻辑和思想即可，无需追究细节。</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%E7%A7%91%E7%A0%94/'>科研</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/pytorch/" class="post-tag no-text-decoration" >PyTorch</a> <a href="/tags/%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/" class="post-tag no-text-decoration" >代码笔记</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-tag no-text-decoration" >机器学习</a> <a href="/tags/%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0/" class="post-tag no-text-decoration" >持续学习</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> 本文由作者按照 <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> 进行授权，转载请注明</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="http://service.weibo.com/share/share.php?title=代码学习笔记：从论文 HAT 学会写持续学习代码 - Shawn Wang&amp;url=https://pengxiang-wang.github.io/posts/codenotes_HAT/" data-toggle="tooltip" data-placement="top" title="Weibo" target="_blank" rel="noopener" aria-label="Weibo"> <i class="fa-fw fab fa-weibo"></i> </a> <a href="https://twitter.com/intent/tweet?text=代码学习笔记：从论文 HAT 学会写持续学习代码 - Shawn Wang&amp;url=https://pengxiang-wang.github.io/posts/codenotes_HAT/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=代码学习笔记：从论文 HAT 学会写持续学习代码 - Shawn Wang&amp;u=https://pengxiang-wang.github.io/posts/codenotes_HAT/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="分享链接" data-title-succeed="链接已复制！"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">最近更新</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/accordion_transcribed_March-of-Steel-Torrent/">编配：《钢铁洪流进行曲》手风琴四重奏</a><li><a href="/posts/accordion_transcribed_Baikal-Lake/">编配：《贝加尔湖畔》手风琴独奏</a><li><a href="/posts/accordion_transcribed_Por-una-Cabeza/">编配：《一步之遥》手风琴四重奏</a><li><a href="/posts/accordion_transcribed_Katyusha/">编配：《喀秋莎》手风琴独奏</a><li><a href="/posts/accordion_transcribed_Miyazaki-Hayao-Movie-Themes/">编配：宫崎骏电影主题曲，手风琴二重奏</a></ul></div><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF/">技术</a> <a class="post-tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> <a class="post-tag" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a> <a class="post-tag" href="/tags/%E6%89%8B%E9%A3%8E%E7%90%B4/">手风琴</a> <a class="post-tag" href="/tags/%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0/">持续学习</a> <a class="post-tag" href="/tags/%E4%B9%90%E8%B0%B1/">乐谱</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/pytorch/">PyTorch</a> <a class="post-tag" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">《动手学深度学习》</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80/">语言</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">文章内容</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>相关文章</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/papernotes_Queried-Unlabeled-Data-Improves-and-Robustifies-Class-Incremental-Learning/"><div class="card-body"> <em class="timeago small" data-ts="1663862400" > 2022-09-23 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>论文笔记：Queried Unlabeled Data Improves and Robustifies Class-Incremental Learning</h3><div class="text-muted small"><p> 论文信息 Queried Unlabeled Data Improves and Robustifies Class-Incremental Learning 期刊：TMLR 2022 作者：德州大学奥斯汀分校等 本文在类别增量（CIL）场景的简单模型 LwF 基础上做了改进，并使用了三个机制，提升了模型的效果：无标签查询数据（QUD）、辅助分类器平衡训练、对抗样本训练...</p></div></div></a></div><div class="card"> <a href="/posts/papernotes_continual_learning_fast_and_slow/"><div class="card-body"> <em class="timeago small" data-ts="1667491200" > 2022-11-04 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>快慢网络式持续学习</h3><div class="text-muted small"><p> 本文介绍快慢网络式持续学习，即构建一个两部分的网络，慢网络负责粗略特征的学习，快网络负责任务特定的细节特征的学习。它们适用于 TIL、CIL 场景不限。它们都借鉴自神经科学中的互补学习系统（complementary learning systems, CLS）理论。 快慢网络一般要利用人为的规定来区分开，通常是规定训练方式，让二者的训练速度有差别：即让一个学得快，另一个学得慢。 论文信...</p></div></div></a></div><div class="card"> <a href="/posts/papernotes_continual_learning_using_training_info/"><div class="card-body"> <em class="timeago small" data-ts="1668441600" > 2022-11-15 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>使用训练信息的持续学习</h3><div class="text-muted small"><p> 有一类持续学习方法的想法是从旧任务的训练过程中获取信息，存放在记忆中，作为新任务防遗忘的参考。本文统一介绍这种思路。这类方法是为了防止遗忘，属于防遗忘机制的另一种分类法。 此类方法的两要素： 有哪些训练信息可以利用？ 获得的训练信息如何使用？ 下面依次讨论，并给出几篇论文使用的例子。 训练信息 我所谓的训练信息是指随训练过程得到的中间产物，而不是原始的训练数据等信息。这...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/papernotes_Queried-Unlabeled-Data-Improves-and-Robustifies-Class-Incremental-Learning/" class="btn btn-outline-primary" prompt="上一篇"><p>论文笔记：Queried Unlabeled Data Improves and Robustifies Class-Incremental Learning</p></a> <a href="/posts/papernotes_continual_learning_fast_and_slow/" class="btn btn-outline-primary" prompt="下一篇"><p>快慢网络式持续学习</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "pengxiang-wang/pengxiang-wang.github.io", "data-repo-id": "R_kgDOHJZRFQ", "data-category": "Announcements", "data-category-id": "DIC_kwDOHJZRFc4COm2c", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "zh-CN", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/pengxiang-wang">Shawn Wang</a>.</p></div><div class="footer-right"><p class="mb-0"> KEEP CALM & CARRY ON</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF/">技术</a> <a class="post-tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> <a class="post-tag" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a> <a class="post-tag" href="/tags/%E6%89%8B%E9%A3%8E%E7%90%B4/">手风琴</a> <a class="post-tag" href="/tags/%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0/">持续学习</a> <a class="post-tag" href="/tags/%E4%B9%90%E8%B0%B1/">乐谱</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/pytorch/">PyTorch</a> <a class="post-tag" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">《动手学深度学习》</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80/">语言</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">搜索结果为空</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/zh-cn.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-H7GH1F7FH5"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-H7GH1F7FH5'); }); </script>
