<!DOCTYPE html><html lang="zh-CN" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="prefer-datetime-locale" content="zh-cn"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="论文笔记：Variational Continual Learning 系列" /><meta property="og:locale" content="zh_CN" /><meta name="description" content="论文信息" /><meta property="og:description" content="论文信息" /><link rel="canonical" href="https://pengxiang-wang.github.io/posts/papernotes_Variational-Continual-Learning/" /><meta property="og:url" content="https://pengxiang-wang.github.io/posts/papernotes_Variational-Continual-Learning/" /><meta property="og:site_name" content="Shawn Wang" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-08-12T00:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="论文笔记：Variational Continual Learning 系列" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-03-01T22:12:22+08:00","datePublished":"2022-08-12T00:00:00+08:00","description":"论文信息","headline":"论文笔记：Variational Continual Learning 系列","mainEntityOfPage":{"@type":"WebPage","@id":"https://pengxiang-wang.github.io/posts/papernotes_Variational-Continual-Learning/"},"url":"https://pengxiang-wang.github.io/posts/papernotes_Variational-Continual-Learning/"}</script><title>论文笔记：Variational Continual Learning 系列 | Shawn Wang</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Shawn Wang"><meta name="application-name" content="Shawn Wang"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src=" /assets/img/avatar.jpg " alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Shawn Wang</a></div><div class="site-subtitle font-italic">WPX 的个人主页</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>首页</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>分类</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>标签</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>时间表</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>关于本站</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/pengxiang-wang" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://space.bilibili.com/88684674" aria-label="bilibili" target="_blank" rel="noopener"> <i class="fas fa-tv"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['shawn.pxwang','qq.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> 首页 </a> </span> <span>论文笔记：Variational Continual Learning 系列</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> 文章</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> </span> <span id="search-cancel" >取消</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>论文笔记：Variational Continual Learning 系列</h1><div class="post-meta text-muted"><div> 作者 <em> <a href="https://github.com/pengxiang-wang">Shawn Wang</a> </em></div><div class="d-flex"><div> <span> 发表于 <em class="timeago" data-ts="1660233600" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-08-12 </em> </span> <span> 更新于 <em class="timeago" data-ts="1677679942" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2023-03-01 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="1832 字"> <em>10 分钟</em>阅读</span></div></div></div><div class="post-content"><h2 id="论文信息"><span class="mr-2">论文信息</span><a href="#论文信息" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="variational-continual-learning"><span class="mr-2"><a href="https://openreview.net/forum?id=BkQqq0gRb"><span class="mr-2">Variational Continual Learning</a></span><a href="#variational-continual-learning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>会议：ICLR 2018<li>作者：剑桥大学</ul><h3 id="improving-and-understanding-variational-continual-learning"><span class="mr-2"><a href="https://arxiv.org/pdf/1905.02099.pdf"><span class="mr-2">Improving and Understanding Variational Continual Learning</a></span><a href="#improving-and-understanding-variational-continual-learning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><ul><li>发表：ArXiv 2019<li>作者：剑桥大学<li><hr /></ul><h1 id="贝叶斯观点下的监督学习">贝叶斯观点下的监督学习</h1><p>贝叶斯学派将模型参数 \(\theta\) 当作随机变量。普通的贝叶斯监督学习只需要求一次后验分布（即一次推断），而对在线学习/持续学习场景，数据是分批来的，需要根据如下迭代公式多次求后验分布：</p>\[\begin{align}p(\theta\mid D_{1:t}) &amp;= p(\theta \mid D_{1:t-1},D_t) \\&amp;\propto p(\theta \mid D_{1:t-1})p(D_t\mid D_{1:t-1},\theta)\\&amp;= p(\theta \mid D_{1:t-1})p(D_t\mid \theta) \end{align}\] \[p(\theta\mid D_1) = p(\theta)p(D_1\mid \theta)\]<p>\(p(\theta)\) 为先验分布。其中最后一个等号是假设了 \(D_t\) 与 \(D_{t-1}\) 独立。</p><p>求出后验分布后，测试阶段用推断算法作预测：</p>\[p\left(y^*\mid \boldsymbol{x}^*, D_{1: t}\right)=\int q_t(\theta) p\left(y^* \mid \theta, \boldsymbol{x}^*\right) \mathrm{d} \theta\]<h1 id="近似算法">近似算法</h1><p>用迭代公式 \(p(\theta\mid D_{1:t})=p(\theta \mid D_{1:t-1})p(D_t\mid \theta)\) 直接计算后验分布是很难的，需要近似算法来计算。这里近似算法的通用框架是：引入一个 \(q_t(\theta)\) 作为后验分布的近似 \(p(\theta\mid D_{1:t})\)，初始化与其相同，但迭代公式改为近似公式：\(p(\theta\mid D_{1:t})=proj(p(\theta \mid D_{1:t-1})p(D_t\mid \theta))\)，\(proj(p)\) 代表近似计算 \(p\)。</p><p>不同的 \(proj\) 代表了不同的近似算法。作者列举了四个：</p><ul><li>Laplace 近似：用一个正态分布来近似。只需求出均值、方差两参数即可；<li>变分法近似：从一个（概率）函数族 \(Q\)中找一个最接近的作为近似。科普一下，变分问题是指泛函（函数的函数）的极值问题。<li>Moment Matching：是一种最优传输算法。最优传输目的是将一个普通的分布映射到另一个分布，使其传输代价最小。Moment Matching 想让分布映射到指数族分布的组合；<li>重要性采样：一种采样方式</ul><p>在在线学习中，每一步迭代都是用同一分布的一部分数据更新。以上四种对应的迭代更新算法，都有相应的工作：</p><ul><li><a href="https://proceedings.neurips.cc/paper/2003/file/7fd804295ef7f6a2822bf4c61f9dc4a8-Paper.pdf">Laplace Propagation</a><li>Online VI / Streaming Variational Bayes<li>Assumed Density Filtering<li>Sequential Monte Carlo</ul><p>本文选用的是变分法近似，变分法常用 KL 散度作为分布间相似程度的度量：</p>\[q_t(\theta) = \arg\min_{q(\theta)\in Q} KL(q(\theta)\|\frac1{Z_t} q_{t-1}(\theta)p(D_t\mid \theta)), t = 2,\cdots, T\]<p>\(1/Z\) 是归一化常数。</p><p>这等价于训练时在最大化似然的损失函数中加入 KL 项：</p>\[\mathcal{L}_t\left(q_t(\theta)\right)=\sum_{n=1}^{N_t} \mathbb{E}_{\theta \sim q_t(\theta)}\left[\log p\left(y_t^{(n)} \mid \theta, \mathbf{x}_t^{(n)}\right)\right]-KL\left(q_t(\theta) \| q_{t-1}(\theta)\right)\]<p>本文中取 \(Q\) 为简单的正态分布的乘积族（称为 Gaussian mean-field Approximation）：\(q_t(\theta)=\prod_{d=1}^D \mathcal{N}\left(\theta_{t, d} ; \mu_{t, d}, \sigma_{t, d}^2\right)\)（对应地 \(q_0(\theta)\) 应初始化为正态分布）。这样，泛函优化转化为对正态分布参数 \(\mu_{t, d}, \sigma_{t, d}^2\) 的优化。注意，模型参数 \(\theta\) 不是优化的目标，贝叶斯方法从来不是更新 \(\theta\) 的确定值，它只是分布的自变量。</p><p>训练时，使用了 Monte Carlo（类似随机梯度下降）处理似然项 \(\sum_{n=1}^{N_t}\) 太大的情况，也用了再参数化（reparameterization）技巧减少参数量。有空我开一篇笔记总结一下训练这种损失函数对技巧。</p><h1 id="防止遗忘的手段coreset">防止遗忘的手段：Coreset</h1><p>持续学习与在线学习的区别是不同任务之间的数据不服从同分布假设，必须要采取<strong>防止遗忘的手段</strong>。在非贝叶斯框架下，防止遗忘的手段有重演、正则化、网络结构三种方法；在贝叶斯框架下，也需要发展出类似的手段。</p><p>本文提出了一个简单的防止遗忘的方法 —— <strong>coreset</strong>，直译为核心数据集，是对数据做操作的，类似于重演数据的方法。</p><p>每个任务有数据 \(D_t\)，也有一个 coreset \(C_t\)。\(C_t\) 需要迭代地构造出来，作者给了几种简单的方法：</p><ul><li>随机取 \(D_t\) 中 K 个点加到 \(C_{t-1}\)；<li>K-center 算法，确保 K 个点平摊在 \(D_t\) 中，面面俱到；<li>其他启发式算法……</ul><p>这里迭代求后验 \(p(\theta\mid D_{1:t}/C_t)\) 的近似，而不是 \(p(\theta\mid D_{1:t})\)。求出了 \(p(\theta\mid D_{1:t}/C_t)\) 后，可以继续算出 \(p(\theta\mid D_{1:t})\)，这才是我们要用的。</p><p>\(p(\theta\mid D_{1:t}/C_t)\) 的迭代公式推导：</p>\[\begin{align} p(\theta|D_{1:t}/C_t) &amp;= p(\theta|D_{1:t-1}\cup D_t/C_t\cup C_{t-1}/C_{t-1})\\&amp;=p(\theta|D_{1:t-1}/C_{t-1} ,D_t\cup C_{t-1}/C_t)\\&amp;\propto p(\theta|D_{1:t-1} /C_{t-1})p( D_t \cup C_{t-1}/C_t|\theta)\\ \end{align}\]<p>以 \(\tilde{q}(\theta)\) 表示 \(p(\theta\mid D_{1:t}/C_t)\) 的近似，使用变分法近似：</p>\[\tilde{q}_t(\theta) = \arg\min_{q(\theta)\in Q} KL(q(\theta)\|\frac1{Z_t} \tilde{q}_{t-1}(\theta)p(D_t \cup C_{t-1}/C_t\|\theta)), t = 2,\cdots, T\]<p>在测试时，才求出 \(p(\theta\mid D_{1:t})\)：</p>\[p(\theta\mid D_{1:t})= p(\theta\mid D_{1:t}/C_t\cup C_t)=p(\theta\mid D_{1:t}/C_t,C_t)\propto p(\theta\mid D_{1:t}/C_t)p(C_t\mid \theta)\]<h1 id="剪枝效应">剪枝效应</h1><p>论文的实验考虑了两个数据集：Split MNIST 和 Permuted MNIST，分别对应持续学习的类别增量和任务增量场景。实验将 VCL、VCL+Coreset 与其他持续学习方法对比平均准确率等指标，也对比了 Coreset 不同的大小的影响。</p><p>在该团队的另一篇论文 Improving and Understanding Variational Continual Learning 中，提到了一个很有趣的事情：<strong>剪枝效应</strong>（pruning effect）——每个任务训练时会只用极少部分的神经元，剩下的神经元看起来被 prune 掉了。被 prune 掉的神经元表现出两方面：</p><ul><li>前面连接的权重的（边缘）分布在更新时几乎不动；<li>后面连接的权重的（边缘）分布几乎为 0 的单点分布（密度为 delta 函数），使得它对最后结果的影响几乎为 0。</ul><p>具体来说，在 Split MNIST 实验（一次来两个新类）中：</p><ul><li>选用了包含一个 200 神经元隐藏层的网络；<li>发现每来一个新任务，只用一个神经元；<li>有无 coreset 不影响剪枝效应。</ul><p>在 Permuted MNIST 实验（一次 10 个类都有）中：</p><ul><li>选用了包含两个 200 神经元隐藏层的网络；<li>发现每来一个新任务，下层隐藏层神经元一次只用一部分神经元，而上层只用到 11 个神经元，且每个任务都用这 11 个神经元。见下图。<li>有无 coreset 不影响剪枝效应。</ul><p><img data-src="/assets/img/VCL_PermutedMNIST_pruning_effect.png" alt="" data-proofer-ignore></p><p>对该现象的解释，作者认为这个效应是 VCL 特有的，是 “变分” 导致的，即那个 KL + 似然的损失函数导致的。作者从这个函数给出了直观的解释，没有严格推导，但我觉得很玄学，就不再说了。</p><p>这个剪枝效应对持续学习是好是坏？作者倾向于认为是好，原因有二：</p><ul><li>每次只用一小部分神经元，自动地为后面的任务预留了空间，解决了持续学习模型 “容量” 不够的问题；<li>天然地完成了 forward / backward transfer。作者解释这个主要在 Split MNIST 体现：假设任务 1 只用了第 1 个隐藏层神经元，任务 2 只用了第 2 个。第 1 个神经元更新输出到 2 的权重会帮助任务 2 的分类；第 2 个神经元更新输出到 1 的权重会帮助任务 1 的分类。</ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%E7%A7%91%E7%A0%94/'>科研</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/" class="post-tag no-text-decoration" >论文笔记</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-tag no-text-decoration" >机器学习</a> <a href="/tags/%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0/" class="post-tag no-text-decoration" >持续学习</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> 本文由作者按照 <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> 进行授权，转载请注明</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="http://service.weibo.com/share/share.php?title=论文笔记：Variational Continual Learning 系列 - Shawn Wang&amp;url=https://pengxiang-wang.github.io/posts/papernotes_Variational-Continual-Learning/" data-toggle="tooltip" data-placement="top" title="Weibo" target="_blank" rel="noopener" aria-label="Weibo"> <i class="fa-fw fab fa-weibo"></i> </a> <a href="https://twitter.com/intent/tweet?text=论文笔记：Variational Continual Learning 系列 - Shawn Wang&amp;url=https://pengxiang-wang.github.io/posts/papernotes_Variational-Continual-Learning/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=论文笔记：Variational Continual Learning 系列 - Shawn Wang&amp;u=https://pengxiang-wang.github.io/posts/papernotes_Variational-Continual-Learning/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="分享链接" data-title-succeed="链接已复制！"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">最近更新</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/accordion_transcribed_March-of-Steel-Torrent/">编配：《钢铁洪流进行曲》手风琴四重奏</a><li><a href="/posts/accordion_transcribed_Baikal-Lake/">编配：《贝加尔湖畔》手风琴独奏</a><li><a href="/posts/accordion_transcribed_Por-una-Cabeza/">编配：《一步之遥》手风琴四重奏</a><li><a href="/posts/accordion_transcribed_Katyusha/">编配：《喀秋莎》手风琴独奏</a><li><a href="/posts/accordion_transcribed_Miyazaki-Hayao-Movie-Themes/">编配：宫崎骏电影主题曲，手风琴二重奏</a></ul></div><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF/">技术</a> <a class="post-tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> <a class="post-tag" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a> <a class="post-tag" href="/tags/%E6%89%8B%E9%A3%8E%E7%90%B4/">手风琴</a> <a class="post-tag" href="/tags/%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0/">持续学习</a> <a class="post-tag" href="/tags/%E4%B9%90%E8%B0%B1/">乐谱</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/pytorch/">PyTorch</a> <a class="post-tag" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">《动手学深度学习》</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80/">语言</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">文章内容</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>相关文章</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/papernotes_Queried-Unlabeled-Data-Improves-and-Robustifies-Class-Incremental-Learning/"><div class="card-body"> <em class="timeago small" data-ts="1663862400" > 2022-09-23 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>论文笔记：Queried Unlabeled Data Improves and Robustifies Class-Incremental Learning</h3><div class="text-muted small"><p> 论文信息 Queried Unlabeled Data Improves and Robustifies Class-Incremental Learning 期刊：TMLR 2022 作者：德州大学奥斯汀分校等 本文在类别增量（CIL）场景的简单模型 LwF 基础上做了改进，并使用了三个机制，提升了模型的效果：无标签查询数据（QUD）、辅助分类器平衡训练、对抗样本训练...</p></div></div></a></div><div class="card"> <a href="/posts/papernotes_continual_learning_fast_and_slow/"><div class="card-body"> <em class="timeago small" data-ts="1667491200" > 2022-11-04 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>快慢网络式持续学习</h3><div class="text-muted small"><p> 本文介绍快慢网络式持续学习，即构建一个两部分的网络，慢网络负责粗略特征的学习，快网络负责任务特定的细节特征的学习。它们适用于 TIL、CIL 场景不限。它们都借鉴自神经科学中的互补学习系统（complementary learning systems, CLS）理论。 快慢网络一般要利用人为的规定来区分开，通常是规定训练方式，让二者的训练速度有差别：即让一个学得快，另一个学得慢。 论文信...</p></div></div></a></div><div class="card"> <a href="/posts/papernotes_continual_learning_using_training_info/"><div class="card-body"> <em class="timeago small" data-ts="1668441600" > 2022-11-15 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>使用训练信息的持续学习</h3><div class="text-muted small"><p> 有一类持续学习方法的想法是从旧任务的训练过程中获取信息，存放在记忆中，作为新任务防遗忘的参考。本文统一介绍这种思路。这类方法是为了防止遗忘，属于防遗忘机制的另一种分类法。 此类方法的两要素： 有哪些训练信息可以利用？ 获得的训练信息如何使用？ 下面依次讨论，并给出几篇论文使用的例子。 训练信息 我所谓的训练信息是指随训练过程得到的中间产物，而不是原始的训练数据等信息。这...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/continual_learning/" class="btn btn-outline-primary" prompt="上一篇"><p>持续学习基础知识</p></a> <a href="/posts/web-crawler-pipeline/" class="btn btn-outline-primary" prompt="下一篇"><p>爬虫项目通用架构</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "pengxiang-wang/pengxiang-wang.github.io", "data-repo-id": "R_kgDOHJZRFQ", "data-category": "Announcements", "data-category-id": "DIC_kwDOHJZRFc4COm2c", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "zh-CN", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/pengxiang-wang">Shawn Wang</a>.</p></div><div class="footer-right"><p class="mb-0"> KEEP CALM & CARRY ON</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF/">技术</a> <a class="post-tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> <a class="post-tag" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a> <a class="post-tag" href="/tags/%E6%89%8B%E9%A3%8E%E7%90%B4/">手风琴</a> <a class="post-tag" href="/tags/%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0/">持续学习</a> <a class="post-tag" href="/tags/%E4%B9%90%E8%B0%B1/">乐谱</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/pytorch/">PyTorch</a> <a class="post-tag" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">《动手学深度学习》</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80/">语言</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">搜索结果为空</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/zh-cn.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-H7GH1F7FH5"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-H7GH1F7FH5'); }); </script>
