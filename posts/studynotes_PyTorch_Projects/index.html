<!DOCTYPE html><html lang="zh-CN" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="prefer-datetime-locale" content="zh-cn"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="PyTorch 学习笔记：工程性知识" /><meta property="og:locale" content="zh_CN" /><meta name="description" content="本文汇总使用 PyTorch 搭建项目时的一些边缘性的工程性知识，让代码真正地成为一个完整的深度学习项目。这部分内容包括如何可视化数据、读写训练进度等。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 5.5 节：读写文件；" /><meta property="og:description" content="本文汇总使用 PyTorch 搭建项目时的一些边缘性的工程性知识，让代码真正地成为一个完整的深度学习项目。这部分内容包括如何可视化数据、读写训练进度等。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 5.5 节：读写文件；" /><link rel="canonical" href="https://pengxiang-wang.github.io/posts/studynotes_PyTorch_Projects/" /><meta property="og:url" content="https://pengxiang-wang.github.io/posts/studynotes_PyTorch_Projects/" /><meta property="og:site_name" content="Shawn Wang" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-02-11T00:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="PyTorch 学习笔记：工程性知识" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-07-24T15:45:01+08:00","datePublished":"2022-02-11T00:00:00+08:00","description":"本文汇总使用 PyTorch 搭建项目时的一些边缘性的工程性知识，让代码真正地成为一个完整的深度学习项目。这部分内容包括如何可视化数据、读写训练进度等。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 5.5 节：读写文件；","headline":"PyTorch 学习笔记：工程性知识","mainEntityOfPage":{"@type":"WebPage","@id":"https://pengxiang-wang.github.io/posts/studynotes_PyTorch_Projects/"},"url":"https://pengxiang-wang.github.io/posts/studynotes_PyTorch_Projects/"}</script><title>PyTorch 学习笔记：工程性知识 | Shawn Wang</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Shawn Wang"><meta name="application-name" content="Shawn Wang"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src=" /assets/img/avatar.jpg " alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Shawn Wang</a></div><div class="site-subtitle font-italic">WPX 的个人主页</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>首页</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>分类</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>标签</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>时间表</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>关于本站</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/pengxiang-wang" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://space.bilibili.com/88684674" aria-label="bilibili" target="_blank" rel="noopener"> <i class="fas fa-tv"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['shawn.pxwang','qq.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> 首页 </a> </span> <span>PyTorch 学习笔记：工程性知识</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> 文章</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> </span> <span id="search-cancel" >取消</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>PyTorch 学习笔记：工程性知识</h1><div class="post-meta text-muted"><div> 作者 <em> <a href="https://github.com/pengxiang-wang">Shawn Wang</a> </em></div><div class="d-flex"><div> <span> 发表于 <em class="timeago" data-ts="1644508800" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-02-11 </em> </span> <span> 更新于 <em class="timeago" data-ts="1690184701" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2023-07-24 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3577 字"> <em>19 分钟</em>阅读</span></div></div></div><div class="post-content"><p>本文汇总使用 PyTorch 搭建项目时的一些边缘性的工程性知识，让代码真正地成为一个完整的深度学习项目。这部分内容包括如何可视化数据、读写训练进度等。本文参考 <a href="https://d2l.ai">Dive into Deep Learning (PyTorch 版)</a> 中的以下内容：</p><ul><li>5.5 节：读写文件；</ul><p>我也写过一个持续学习项目 HAT 的代码笔记，有助于理解一个完整的深度学习项目是如何写出来的，请参考：&lt;&gt;。</p><hr /><h1 id="读写训练进度">读写训练进度</h1><p>深度学习程序的一个特点是运行时间长，一个任务经常需要跑几天、几个月。可以把深度学习程序比作 RPG 游戏，打通关时间长的 RPG 时我们需要定期存档，不仅为了下一次打开游戏时接续进度，还能预防电脑未响应、死机等突发情况导致游戏白打，甚至有时需要换台电脑玩这个进度、应当存档拷贝到新电脑；而且有时候会存多份档，为了预防游戏中某一次策略错误（如，买错了道具；打 boss 打不过去或者游戏有 bug 导致的陷入死循环，俗称坏档）导致的严重后果，起到后悔药的作用。</p><p>大型的深度学习程序需要<strong>定期存档且存多份档</strong>，和上面是一个道理，不必多解释了。它与游戏的不同在于用户无法在运行过程中手动控制，只有停止程序这一个选择；定期存档的操作需要预先写进代码里。</p><p>深度学习程序也是 Python 程序，当然可以用 Python 自带的文件读写功能，将变量保存于本地文件。但 PyTorch 为深度学习设计了专门更高级的 API，更加方便，最好使用这套 API。PyTorch 可以读写 Tensor 对象，<code class="language-plaintext highlighter-rouge">nn.Parameter</code> 对象，还可以是 <code class="language-plaintext highlighter-rouge">{字符串:Tensor或Parameter}</code> 的字典：</p><ul><li><code class="language-plaintext highlighter-rouge">torch.save(obj, path)</code>：将对象 obj 存到路径为 path 的文件中；<li><code class="language-plaintext highlighter-rouge">obj = torch.load(filename)</code>：将 filename 文件存储的变量赋值到 obj。 此类文件属文本文件，PyTorch 推荐使用扩展名 <code class="language-plaintext highlighter-rouge">.pt</code>,<code class="language-plaintext highlighter-rouge">.pth</code>（书中使用了 <code class="language-plaintext highlighter-rouge">.params</code>）。存档文件最好存储在项目单独的一个子目录下。</ul><p>深度学习最需要存档的东西是<strong>模型参数</strong>，它是训练的目标。网络结构无需保存，因为它就写在代码里，只需保存其参数即可。保存模型参数的推荐方法是存它的 <code class="language-plaintext highlighter-rouge">.state_dict()</code>（前面说过它是存所有模型参数的字典），因为 <code class="language-plaintext highlighter-rouge">nn.Module</code> 有一个方便的 API：<code class="language-plaintext highlighter-rouge">net.load_state_dict(state_dict)</code>，能将 state_dict 一步读取所有参数到模型 net 中。</p><p>除了模型参数，还有一些必须存档的信息：当前 epoch 轮数，优化器里还有一些状态量（<code class="language-plaintext highlighter-rouge">optimizer.state_dict()</code>），如果用了调度器它也有状态量 <code class="language-plaintext highlighter-rouge">scheduler.state_dict()</code>，等等。可以将其统统打包成一个字典，类似下面的做法：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'epoch'</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
    <span class="s">'net'</span><span class="p">:</span> <span class="n">net</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span>
    <span class="s">'optimizer'</span><span class="p">:</span> <span class="n">optimizer</span><span class="p">.</span><span class="n">state_dict</span><span class="p">()</span>
<span class="p">}</span>
</pre></table></code></div></div><p>除此之外，为了方便，也可以打包进去其他需要记住的东西，如超参数、配置变量、当前 loss 等统计信息，等等。写到字典里是为了方便程序内使用，如果只是给人看一下，一些小的信息也可以传给 <code class="language-plaintext highlighter-rouge">path</code>，写到文件名内。</p><p>下面讨论存档的频率。首先要说一点，为了实现多份存档，文件名最好不一样，防止覆盖。存档太频会浪费硬盘空间，例如一个 batch 或 epoch 一存；太不频则有更大的重新训练风险。而且并不是所有的档都需要存，和游戏一个道理，一般是在比较关键的进度存一下档。常见的做法是在训练循环体中设置条件判断语句写的检查点（checkpoint），判断是不是关键的存档。</p><p>以下是一套完整的流程（引自<a href="https://www.zhihu.com/question/340567722/answer/2505072802">知乎</a>，作者“”人类之奴）：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre><td class="rouge-code"><pre><span class="n">start_epoch</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

<span class="c1"># 如果接续训练（RESUME=1），则加载 checkpoint
</span><span class="k">if</span> <span class="n">RESUME</span><span class="p">:</span>
    <span class="n">path_checkpoint</span> <span class="o">=</span> <span class="n">checkpoint_path</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="n">path_checkpoint</span><span class="p">)</span>
    <span class="n">start_epoch</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s">'epoch'</span><span class="p">]</span>
    <span class="n">net</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s">'net'</span><span class="p">])</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s">'optimizer'</span><span class="p">])</span>
    <span class="n">scheduler</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">[</span><span class="s">'lr_schedule'</span><span class="p">])</span>


<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">start_epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">train</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">)</span>
    <span class="n">test_loss</span> <span class="o">=</span> <span class="n">test</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>

    <span class="c1"># 检查点：测试集 loss 小于一定阈值。epoch 小于一半总训练轮数时认为训练不够，不设检查点
</span>    <span class="n">min_loss_val</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">epoch</span> <span class="o">&gt;</span> <span class="nb">int</span><span class="p">(</span><span class="n">num_epochs</span><span class="o">/</span><span class="mi">2</span><span class="p">)</span> <span class="ow">and</span> <span class="n">test_loss</span> <span class="o">&lt;=</span> <span class="n">min_loss_val</span><span class="p">:</span> 
        <span class="n">min_loss_val</span> <span class="o">=</span> <span class="n">test_loss</span>
        <span class="n">checkpoint</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s">'loss'</span><span class="p">:</span><span class="n">test_loss</span><span class="p">,</span>
            <span class="s">'epoch'</span><span class="p">:</span><span class="n">epoch</span><span class="p">,</span>
            <span class="s">'net'</span><span class="p">:</span><span class="n">net</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s">'optimizer'</span><span class="p">:</span><span class="n">optimizer</span><span class="p">.</span><span class="n">state_dict</span><span class="p">(),</span>
            <span class="s">'lr_schedule'</span><span class="p">:</span><span class="n">scheduler</span><span class="p">.</span><span class="n">state_dict</span><span class="p">()}</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">isdir</span><span class="p">(</span><span class="sa">r</span><span class="s">'tf-logs/'</span><span class="o">+</span><span class="n">save_model</span><span class="p">):</span>
            <span class="n">os</span><span class="p">.</span><span class="n">mkdir</span><span class="p">(</span><span class="sa">r</span><span class="s">'tf-logs/'</span><span class="o">+</span><span class="n">save_model</span><span class="p">)</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">,</span><span class="sa">r</span><span class="s">'tf-logs/'</span><span class="o">+</span><span class="n">save_model</span><span class="o">+</span><span class="s">'/ckpt_best_%s.pth'</span><span class="o">%</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">epoch</span><span class="o">+</span><span class="mi">1</span><span class="p">)))</span>
</pre></table></code></div></div><blockquote class="prompt-tip"><div><p>读写功能除了上述存档接续训练进度，还有其他常见的应用场景：例如保存训练好的参数给别人使用。常见的大型网络可以使用别人预训练的权重，再在自己的任务上微调，这些预训练权重通常保存在 <code class="language-plaintext highlighter-rouge">.pth</code> 文件中，从网上下载。</p></div></blockquote><h1 id="可视化">可视化</h1><p>深度学习里很多内容需要<strong>可视化</strong>，辅助研究并呈现结果，例如：</p><ul><li>数据集中的数据；<li>网络结构；<li>学习曲线、指标的变化曲线等。</ul><p>为实现此目的，除了可以手动调用例如 Matplotlib 等天然的可视化工具，深度学习框架也开发了自己的可视化工具。本章介绍 <strong>TensorBoard</strong> 的使用，它是 TensorFlow 的可视化工具，目前也支持了 PyTorch。</p><h2 id="tensorboard"><span class="mr-2">TensorBoard</span><a href="#tensorboard" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><h3 id="tensorboard-逻辑"><span class="mr-2">TensorBoard 逻辑</span><a href="#tensorboard-逻辑" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>TensorBoard 的逻辑可以看成一个画家，以及一个画布，给画家各种作画指示，它就会按要求在画布上作出各种图。</p><p>工具的两个部分：</p><ul><li><strong>画家</strong>和各种<strong>作画指示</strong>：在 <code class="language-plaintext highlighter-rouge">torch</code> 库中，存放在 <code class="language-plaintext highlighter-rouge">torch.utils.TensorBoard</code>。画家是 <code class="language-plaintext highlighter-rouge">TensorBoard.SummaryWriter</code> 类，作画指令就是类方法 <code class="language-plaintext highlighter-rouge">add_xx(...)</code>（xx 表示各种支持的内容，例如 scalar、graph 等），每调用一次就会在画布上画方法参数中对应的内容；<li><strong>画布</strong>：是一个本地软件，在本地端口运行（浏览器打开，类似于 Jupyter Notebook），需要额外安装。必须启动画布，才能看到画家作的画。</ul><p>问题来了，画家和画布是两个程序，画布怎么知道画家的作画内容呢？这是通过<strong>日志</strong>实现的。画家作画其实是输出了一些画布能读懂的日志，画布通过输入日志来呈现画家的作画。这些日志存放在文本文件（称为<strong>日志文件</strong>），并通常放于专门的日志目录下（在代码中，画家和画布都是从指定目录下输出、输入日志），使用时应当为画家和画布指定<strong>相同的日志目录</strong>。</p><p>命令总结：</p><ul><li>安装画布：<code class="language-plaintext highlighter-rouge">conda install tensorboard</code>；<li>启动画布：<code class="language-plaintext highlighter-rouge">tensorboard --logdir=log</code>（runs 为日志目录，必须指定），并按提示打开浏览器端口；<li>召唤画家：<div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>
<span class="n">summaryWriter</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="n">log_dir</span><span class="o">=</span><span class="s">'log'</span><span class="p">)</span> <span class="c1"># 实例化画家，log_dir 为日志目录
</span></pre></table></code></div></div></ul><p>日志文件的组织方式：每运行一次（一个 “run”，即每实例化一个画家 SummaryWriter）都会产生一个新的日志文件。日志文件中记录了时间、设备等元信息与该画家的作画内容信息。<strong>画布会呈现所有日志文件所画内容的并集</strong>（可以在界面左下角选择部分的 “run” 显示），因此画家之间唯一的区别方式就是日志目录。</p><h3 id="tensorboard-能画什么"><span class="mr-2">TensorBoard 能画什么</span><a href="#tensorboard-能画什么" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>官方文档：<a href="https://pytorch.org/docs/stable/TensorBoard.html">https://pytorch.org/docs/stable/TensorBoard.html</a></p><p>TensorBoard 通过 SummaryWriter 类的 <code class="language-plaintext highlighter-rouge">add_xx</code> 方法来画不同的内容，呈现在画布的各个<strong>版块</strong>上（上方选项卡），每个版块都有包含若干<strong>子版块</strong>（右方）；画布是交互式的而非静态，可以在画布上进一步调整可视化的效果，甚至导出（左方）。</p><p><img data-src="/assets/img/TensorBoard.gif" alt="" data-proofer-ignore></p><p><code class="language-plaintext highlighter-rouge">add_xx</code> 方法有共同的参数：</p><ul><li><code class="language-plaintext highlighter-rouge">tag</code> 参数：这部分内容的名字（字符串），必须指定。<li><code class="language-plaintext highlighter-rouge">walltime</code> 参数：默认为系统时间 <code class="language-plaintext highlighter-rouge">time.time()</code>。可以在 TensorBoard 界面红可视化这一信息。在画布 TIME SERIES 版块可以查看所有作画历史记录，会将调用的 <code class="language-plaintext highlighter-rouge">add_xx</code> 作出的内容按照该时间顺序排列。</ul><p>以下是 TensorBoard 能画的东西（详细用法见文档，我只总结核心的东西）：</p><ul><li>画曲线 <code class="language-plaintext highlighter-rouge">add_scalar</code><ul><li>呈现在画布 SCALARS 版块；<li>在曲线 <code class="language-plaintext highlighter-rouge">tag</code> 上添加一个坐标为 (global_step, scalar_value) 的点（注意 global_step 必须为整数）；<li>不同的曲线画在不同的图上，一个子版块对应一个图；<code class="language-plaintext highlighter-rouge">add_scalars</code> 可以把多个曲线画在同一个图上；<li>同理可以画直方图：<code class="language-plaintext highlighter-rouge">add_histogram</code>、二维图 <code class="language-plaintext highlighter-rouge">add_mesh</code> 等。</ul><li>画图像：<code class="language-plaintext highlighter-rouge">add_image</code><ul><li>呈现在画布 IMAGES 版块；<li>在 IMAGES 板块的子版块 <code class="language-plaintext highlighter-rouge">tag</code> 上打印格式为 Tensor 类型的图像 img_tensor；<li><code class="language-plaintext highlighter-rouge">add_images</code> 可以在一个子版块打印多个图像；<li>同理可以画其他数据：音频 <code class="language-plaintext highlighter-rouge">add_audio</code>，文本 <code class="language-plaintext highlighter-rouge">add_text</code>，视频 <code class="language-plaintext highlighter-rouge">add_video</code>，Matplotlib 的 figure <code class="language-plaintext highlighter-rouge">add_figure</code>；</ul><li>画表格：<code class="language-plaintext highlighter-rouge">add_hparams</code><ul><li>呈现在画布 HPARAMS 版块；<li>一次调用就添加一条表格记录（表格的一行）；<li>传入字典，字典的键对应属性，值对应属性值；<li>应传入两个字典，一个是自变量，一个是因变量；区分它们的意义是画布有对因变量做数据分析的交互功能；</ul><li>画计算图：<code class="language-plaintext highlighter-rouge">add_graph</code><ul><li>画 tensor 中存储的计算图；<li>传入 <code class="language-plaintext highlighter-rouge">torch.nn.Module</code> 模型即可；</ul><li>画 PR 曲线：<code class="language-plaintext highlighter-rouge">add_pr_curve</code><ul><li>传入预测标签和真实标签，会自动计算准确率和召回率；<li>一次调用就在 PR 坐标上添加一个点。</ul><li>在低维空间（不超过 3 维）上展示高维数据：<code class="language-plaintext highlighter-rouge">add_embedding</code><ul><li>采用的降维方法是 <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding">t-SNE</a>，是数据可视化常用的降维方法；<li>传入数据矩阵 Tensor；<li>可以传入类别标签，则会以不同颜色显示类别；也可以传入其他类型的标签如字符串乃至图像，则图中的数据点会显示字符串或图像。</ul></ul><h3 id="tensorboard-在深度学习中的用处"><span class="mr-2">TensorBoard 在深度学习中的用处</span><a href="#tensorboard-在深度学习中的用处" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>从上面看，TensorBoard 和通用的可视化工具的功能与逻辑差别不大。但它一开始就是为深度学习可视化设计的，主要兼容深度学习框架 Tensor 类型的数据，设计的可画内容都是深度学习需要可视化的。深度学习需要可视化：</p><ul><li>数据：<code class="language-plaintext highlighter-rouge">add_images</code>, <code class="language-plaintext highlighter-rouge">add_video</code> 等；<li>学习曲线、loss 曲线等指标（随时间变化）：<code class="language-plaintext highlighter-rouge">add_scalar</code>；<li>网络结构图：用 <code class="language-plaintext highlighter-rouge">add_graph</code>；<li>不同超参数的效果比较：<code class="language-plaintext highlighter-rouge">add_hparams</code>（顾名思义，<code class="language-plaintext highlighter-rouge">add_hparams</code> 画表格就专门为超参数这事的）；<li>在低维空间可视化模型中间层 Embedding：<code class="language-plaintext highlighter-rouge">add_embedding</code>；<li>PR 曲线：<code class="language-plaintext highlighter-rouge">add_pr_curve</code>。</ul><p>深度学习的程序往往耗时很长，需要有存档机制，在代码中设置一些检查点（checkpoint），保存、加载训练到一半的模型参数等在<a href="https://pengxiang-wang.github.io/posts/readingnotes_Dive-into-DL_Part4/">这篇笔记</a>中有详细的讨论。TensorBoard 也需要有存档机制，根据日志文件的组织方式——每次运行都会保存一个日志文件，画布会加载日志目录下的所有日志文件，所以我们无需手动保存、加载 TensorBoard 画图的进度。</p><p>##</p><p>本文介绍 Python <code class="language-plaintext highlighter-rouge">logging</code>库的使用方法，该库是 Python 中更复杂的调试工具，可以将程序的调试信息输出为格式更丰富的日志或日志文件，适合大型项目（例如深度学习）的调试与监控。</p><p><code class="language-plaintext highlighter-rouge">logging</code></p><h2 id="tqdm"><span class="mr-2">tqdm</span><a href="#tqdm" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><h1 id="超参数优化">超参数优化</h1><p>（待更新）</p><p>对于很大的实验，在真正开始训练前，最好对代码做一个完整的检查。</p><h2 id="学习曲线"><span class="mr-2">学习曲线</span><a href="#学习曲线" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>过拟合、欠拟合的判断。</p><h1 id="随机数种子">随机数种子</h1><p>深度学习有很多地方涉及随机数：</p><ul><li>数据增强；<li><li>持续学习里构造数据集的 Permuted 操作；</ul><p>为了避免每次实验因为随机数的原因都不一样，也为了别人能够复现，一个完整的深度学习项目通常要<strong>设定随机数种子（seed）</strong>。计算机里的随机数生成算法都是伪随机数，算法接受一个随机数种子作为输入，通过固定的计算过程模拟某个分布得到这种随机数的。因此，一但随机数种子被人为设定，生成的随机数也就固定了，设定随机数种子起到<strong>固定随机数的作用</strong>。</p><p>Python 中凡是随机算法的程序都有设定种子的接口，包括两种：</p><ul><li>局部变量设定：随机数函数一般有指定种子的参数如 <code class="language-plaintext highlighter-rouge">random_state</code>；<li>全局设定：对随机数模块调用 <code class="language-plaintext highlighter-rouge">seed</code> 函数；</ul><p>深度学习程序中，最省心的做法是在程序开头对所有随机数模块（包括 Python 内置 random 模块、numpy.random、torch 等），也一般打包成函数：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">setup_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
    <span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">cudnn</span><span class="p">.</span><span class="n">deterministic</span> <span class="o">=</span> <span class="bp">True</span>

<span class="n">setup_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># 调用
</span></pre></table></code></div></div><blockquote class="prompt-warning"><div><p>使用随机数种子应当注意：</p><ul><li>设定随机数种子会拖慢程序，对运算量大的深度学习程序有影响；<li>随机数的随机性一般对深度学习结果的影响不会太大；<li>随机数种子是依赖于机器的，设定同样的种子，在不同的机器上结果会不同。 应当根据需要，决定需不需要设定种子。</ul></div></blockquote></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%E7%A7%91%E7%A0%94/'>科研</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" class="post-tag no-text-decoration" >读书笔记</a> <a href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-tag no-text-decoration" >《动手学深度学习》</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-tag no-text-decoration" >机器学习</a> <a href="/tags/%E6%8A%80%E6%9C%AF/" class="post-tag no-text-decoration" >技术</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> 本文由作者按照 <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> 进行授权，转载请注明</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="http://service.weibo.com/share/share.php?title=PyTorch 学习笔记：工程性知识 - Shawn Wang&amp;url=https://pengxiang-wang.github.io/posts/studynotes_PyTorch_Projects/" data-toggle="tooltip" data-placement="top" title="Weibo" target="_blank" rel="noopener" aria-label="Weibo"> <i class="fa-fw fab fa-weibo"></i> </a> <a href="https://twitter.com/intent/tweet?text=PyTorch 学习笔记：工程性知识 - Shawn Wang&amp;url=https://pengxiang-wang.github.io/posts/studynotes_PyTorch_Projects/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=PyTorch 学习笔记：工程性知识 - Shawn Wang&amp;u=https://pengxiang-wang.github.io/posts/studynotes_PyTorch_Projects/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="分享链接" data-title-succeed="链接已复制！"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">最近更新</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/accordion_transcribed_March-of-Steel-Torrent/">编配：《钢铁洪流进行曲》手风琴四重奏</a><li><a href="/posts/accordion_transcribed_Baikal-Lake/">编配：《贝加尔湖畔》手风琴独奏</a><li><a href="/posts/accordion_transcribed_Por-una-Cabeza/">编配：《一步之遥》手风琴四重奏</a><li><a href="/posts/accordion_transcribed_Katyusha/">编配：《喀秋莎》手风琴独奏</a><li><a href="/posts/accordion_transcribed_Miyazaki-Hayao-Movie-Themes/">编配：宫崎骏电影主题曲，手风琴二重奏</a></ul></div><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF/">技术</a> <a class="post-tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> <a class="post-tag" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a> <a class="post-tag" href="/tags/%E6%89%8B%E9%A3%8E%E7%90%B4/">手风琴</a> <a class="post-tag" href="/tags/%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0/">持续学习</a> <a class="post-tag" href="/tags/%E4%B9%90%E8%B0%B1/">乐谱</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/pytorch/">PyTorch</a> <a class="post-tag" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">《动手学深度学习》</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80/">语言</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">文章内容</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>相关文章</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/studynotes_PyTorch_autograd_and_pipeline/"><div class="card-body"> <em class="timeago small" data-ts="1642780800" > 2022-01-22 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PyTorch 学习笔记（一）：自动微分，简单模型的实现</h3><div class="text-muted small"><p> 本系列博文是我学习深度学习框架的学习笔记。深度学习框架大同小异，只须学习一种的原理，其他的都可以快速上手。我使用的是 PyTorch。笔记将着重强调代码原理、思想的理解，而不是具体的代码。 PyTorch 官方文档：https://pytorch.org/docs/stable/index.html PyTorch 中文文档：https://pytorch-cn.readthed...</p></div></div></a></div><div class="card"> <a href="/posts/studynotes_PyTorch_nnModule/"><div class="card-body"> <em class="timeago small" data-ts="1644249600" > 2022-02-08 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PyTorch 学习笔记（三）：自定义网络结构（nn.Module）</h3><div class="text-muted small"><p> 本文介绍如何自定义模型，即 nn.Module 模块的逻辑与使用方法。由此可以搭建自己的深度网络结构。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 5.1、5.6 节：层和块，自定义层。 nn.Module 官方文档：https://pytorch.org/docs/stable/generated/torch.nn.Module...</p></div></div></a></div><div class="card"> <a href="/posts/studynotes_PyTorch_computing/"><div class="card-body"> <em class="timeago small" data-ts="1644508800" > 2022-02-11 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PyTorch 学习笔记（五）：计算性能</h3><div class="text-muted small"><p> 本文介绍 PyTorch 与计算性能有关的代码知识，包括如何使用 GPU、并行计算、多服务器计算等等。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 5.6 节：GPU； 第 12 章：计算性能； 深度学习与 GPU 众所周知，深度学习计算可以使用 GPU，往往能极大提高效率。GPU 用于深度学习时与其他任务不同，它更偏向...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/studynotes_Minnan_pinyin/" class="btn btn-outline-primary" prompt="上一篇"><p>闽南语学习笔记：语音系统</p></a> <a href="/posts/studynotes_PyTorch_computing/" class="btn btn-outline-primary" prompt="下一篇"><p>PyTorch 学习笔记（五）：计算性能</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "pengxiang-wang/pengxiang-wang.github.io", "data-repo-id": "R_kgDOHJZRFQ", "data-category": "Announcements", "data-category-id": "DIC_kwDOHJZRFc4COm2c", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "zh-CN", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/pengxiang-wang">Shawn Wang</a>.</p></div><div class="footer-right"><p class="mb-0"> KEEP CALM & CARRY ON</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF/">技术</a> <a class="post-tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> <a class="post-tag" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a> <a class="post-tag" href="/tags/%E6%89%8B%E9%A3%8E%E7%90%B4/">手风琴</a> <a class="post-tag" href="/tags/%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0/">持续学习</a> <a class="post-tag" href="/tags/%E4%B9%90%E8%B0%B1/">乐谱</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/pytorch/">PyTorch</a> <a class="post-tag" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">《动手学深度学习》</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80/">语言</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">搜索结果为空</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/zh-cn.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-H7GH1F7FH5"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-H7GH1F7FH5'); }); </script>
