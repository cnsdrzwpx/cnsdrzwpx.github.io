<!DOCTYPE html><html lang="zh-CN" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="prefer-datetime-locale" content="zh-cn"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="PyTorch 学习笔记（一）：自动微分，简单模型的实现" /><meta property="og:locale" content="zh_CN" /><meta name="description" content="本系列博文是我学习深度学习框架的学习笔记。深度学习框架大同小异，只须学习一种的原理，其他的都可以快速上手。我使用的是 PyTorch。笔记将着重强调代码原理、思想的理解，而不是具体的代码。" /><meta property="og:description" content="本系列博文是我学习深度学习框架的学习笔记。深度学习框架大同小异，只须学习一种的原理，其他的都可以快速上手。我使用的是 PyTorch。笔记将着重强调代码原理、思想的理解，而不是具体的代码。" /><link rel="canonical" href="https://pengxiang-wang.github.io/posts/studynotes_PyTorch_autograd_and_pipeline/" /><meta property="og:url" content="https://pengxiang-wang.github.io/posts/studynotes_PyTorch_autograd_and_pipeline/" /><meta property="og:site_name" content="Shawn Wang" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-01-22T00:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="PyTorch 学习笔记（一）：自动微分，简单模型的实现" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-05-27T23:26:31+08:00","datePublished":"2022-01-22T00:00:00+08:00","description":"本系列博文是我学习深度学习框架的学习笔记。深度学习框架大同小异，只须学习一种的原理，其他的都可以快速上手。我使用的是 PyTorch。笔记将着重强调代码原理、思想的理解，而不是具体的代码。","headline":"PyTorch 学习笔记（一）：自动微分，简单模型的实现","mainEntityOfPage":{"@type":"WebPage","@id":"https://pengxiang-wang.github.io/posts/studynotes_PyTorch_autograd_and_pipeline/"},"url":"https://pengxiang-wang.github.io/posts/studynotes_PyTorch_autograd_and_pipeline/"}</script><title>PyTorch 学习笔记（一）：自动微分，简单模型的实现 | Shawn Wang</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Shawn Wang"><meta name="application-name" content="Shawn Wang"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src=" /assets/img/avatar.jpg " alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Shawn Wang</a></div><div class="site-subtitle font-italic">WPX 的个人主页</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>首页</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>分类</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>标签</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>时间表</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>关于本站</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/pengxiang-wang" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://space.bilibili.com/88684674" aria-label="bilibili" target="_blank" rel="noopener"> <i class="fas fa-tv"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['shawn.pxwang','qq.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> 首页 </a> </span> <span>PyTorch 学习笔记（一）：自动微分，简单模型的实现</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> 文章</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> </span> <span id="search-cancel" >取消</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>PyTorch 学习笔记（一）：自动微分，简单模型的实现</h1><div class="post-meta text-muted"><div> 作者 <em> <a href="https://github.com/pengxiang-wang">Shawn Wang</a> </em></div><div class="d-flex"><div> <span> 发表于 <em class="timeago" data-ts="1642780800" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-01-22 </em> </span> <span> 更新于 <em class="timeago" data-ts="1685201191" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2023-05-27 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="5714 字"> <em>31 分钟</em>阅读</span></div></div></div><div class="post-content"><p>本系列博文是我学习深度学习框架的学习笔记。深度学习框架大同小异，只须学习一种的原理，其他的都可以快速上手。我使用的是 PyTorch。笔记将着重强调代码原理、思想的理解，而不是具体的代码。</p><ul><li>PyTorch 官方文档：<a href="https://pytorch.org/docs/stable/index.html">https://pytorch.org/docs/stable/index.html</a><li>PyTorch 中文文档：<a href="https://pytorch-cn.readthedocs.io/zh/latest/">https://pytorch-cn.readthedocs.io/zh/latest/</a></ul><p>本系列博文的参考书为亚马逊团队编写的 <a href="https://d2l.ai">Dive into Deep Learning (PyTorch 版)</a>，编排顺序基本遵从此书。导师最近很推荐这本书。这是一本把深度学习从头开始讲的技术书，虽然大部分内容是会的，但难得找到一本书在代码上讲得系统，看一遍也是很有好处的。我计划是利用寒假时间看一看，整理出一套笔记。B 站上有<a href="https://space.bilibili.com/1567748478/">李沐</a> 主讲的<a href="https://c.d2l.ai/zh-v2/">配套课程</a>可供参考。</p><p>本文介绍深度学习框架的基本数据结构——Tensor 及其核心功能——自动微分，并搭建几个最简单的监督学习模型，主要参考书的：</p><ul><li>2.5 节：自动微分；<li>3.2-3.3 节：线性回归的从零开始实现、简洁实现；<li>3.5-3.7 节：Softmax 多分类的从零开始实现、简洁实现；<li>4.1-4.3 节：多层感知机（MLP）的从零开始实现、简洁实现。</ul><hr /><p>在开始前还是提示一下如何安装 PyTorch。去<a href="https://pytorch.org">官网</a>翻到 Install PyTorch，根据自己机器的系统等信息选择后，用下面生成的指令安装。如果不想用或没有 GPU，选择 CPU 版本即可；如果想用，请参考 <a href="">PyTorch 学习笔记：使用 GPU</a>，了解 CUDA 的意思后选择合适的 CUDA 版本安装。 <img data-src="/assets/img/PyTorch_installation.png" alt="选择" data-proofer-ignore> 安装过程如果报错，尝试使用国内镜像，参见 <a href="https://pengxiang-wang.github.io/posts/studynotes_conda/">Conda 学习笔记</a>。</p><h1 id="基本数据结构tensor">基本数据结构：Tensor</h1><p>PyTorch 是深度学习框架，预备知识一定是基本的数据结构、数据操作。<strong>张量</strong>（Tensor）是 PyTorch 的基本数据结构，它的性质和用法就是数学上的张量，在<a href="">这篇博文</a>已详细讲述。书中 2.1,2.3,2.4 等节大部分篇幅在讲述 Tensor 的基本用法，这些与 Numpy 也是一致的，就跳过了。</p><p>这篇博文也总结过，PyTorch 和 Numpy 的基本数据结构本质都是数学上的张量，而且 PyTorch 是基于 Numpy 的，为什么还要自己封装一个 Tensor 类型？书中第 2 章开头总结的不错，PyTorch 在 Tensor 中融入了深度学习相关的功能：</p><ul><li>在 GPU 上加速计算（Numpy 只能在 CPU）；<li>储存梯度、计算图等信息，实现自动微分功能。</ul><h2 id="自动微分"><span class="mr-2">自动微分</span><a href="#自动微分" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p><a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"><strong>自动微分</strong></a>（求导）是深度学习框架的主要功能，顾名思义就是给出一个函数后，即可直接算出在某点的导数（梯度）值（注意，并不是待求导函数的表达式）。计算图、链式法则是自动微分基于的原理，但也不需要搞明白其底层实现方式，只要会用即可。</p><blockquote><p>自动微分只能完成数值计算，而不是fu只能求在某点的导数值，而不能求出导函数的表示</p></blockquote><p>需要理解的是，自动微分功能是实现在 Tensor 里的，自动微分的计算过程和结果都是存的 Tensor 的属性中的：</p><ul><li><code class="language-plaintext highlighter-rouge">grad_fn</code>：存放待求导函数（的计算图）；<li><code class="language-plaintext highlighter-rouge">grad</code>：存放求得的导数向量（Tensor）。 这个 Tensor 即为被求导点。</ul><p>假设要求 \(\frac{\operatorname{d} y}{\operatorname{d} \mathbf{x}}_{\mathbf{x}=\mathbf{x}_{0}}\)，以求 \(y = 2\mathbf{x}^T \mathbf{x}\) 在 \(x_0 = (0,1,2,3)^T\) 点的梯度为例，完整的自动微分过程如下：</p><ol class="prompt-tip"><li>定义 \(x_0\)：将 \(x_0\) 点的值以 tensor 的形式赋给变量 <code class="language-plaintext highlighter-rouge">x</code><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre> <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">4.0</span><span class="p">)</span>
</pre></table></code></div></div><li>开启求导模式：把 tensor <code class="language-plaintext highlighter-rouge">x</code> 的 <code class="language-plaintext highlighter-rouge">requires_grad</code> 属性设为 True<div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre> <span class="n">x</span><span class="p">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
</pre></table></code></div></div><blockquote><p>求导模式可以在 Tensor 构造时即刻开启，只需在构造的函数传入参数 <code class="language-plaintext highlighter-rouge">requires_grad=True</code>。例如上面 1,2 两步可合为 <code class="language-plaintext highlighter-rouge">x = torch.arange(4.0, requires_grad=True)</code>。</p></blockquote></ol><ol><li>定义被求导函数 \(y\)：将含 <code class="language-plaintext highlighter-rouge">x</code> 的 torch 表达式赋给变量 <code class="language-plaintext highlighter-rouge">y</code> （此时 tensor <code class="language-plaintext highlighter-rouge">y</code> 存放了计算图）<div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre> <span class="n">y</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
</pre></table></code></div></div><li>求导：调用 <code class="language-plaintext highlighter-rouge">y</code> 的 <code class="language-plaintext highlighter-rouge">backward</code> 方法，导数值存放在 <code class="language-plaintext highlighter-rouge">x</code> 的 <code class="language-plaintext highlighter-rouge">grad</code> 属性中（与 <code class="language-plaintext highlighter-rouge">x</code> 维数相同）<div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre> <span class="n">y</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
</pre></table></code></div></div></ol><p>注意点：</p><ul><li>存放求导结果的 <code class="language-plaintext highlighter-rouge">grad</code> 属性是累加的：第一次求导前默认为 0，求导后将结果叠加到 0 上，第二次求导后会叠加到第一次的结果上。所以如需反复求导一定要<strong>清零</strong>。清零的方法：<div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>  <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></table></code></div></div><li>被求导函数可以额外打包成一个 Python 函数赋给 <code class="language-plaintext highlighter-rouge">y</code>（只要函数里面用的都是 torch 的表达式）；<li>构建计算图极容易粗心，一定注意好求导模式的开关，不在不该的地方引入计算图。除了修改 <code class="language-plaintext highlighter-rouge">requires_grad_</code> 属性，还可以：<ul><li>全局地关闭求导模式，用以下代码包裹：<div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>  <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
</pre></table></code></div></div><li>分离：即去掉 <code class="language-plaintext highlighter-rouge">grad_fn</code> 存放的计算图，只保留 tensor 值。以下代码将 <code class="language-plaintext highlighter-rouge">y</code> 分离成 <code class="language-plaintext highlighter-rouge">u</code>：<div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>  <span class="n">u</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">detach</span><span class="p">()</span>
</pre></table></code></div></div></ul><li>上面求导要求 \(y\) 必须是标量，而 \(x\) 可以是向量。事实上 \(y\) 也可以是向量，需要在 backward 函数中加参数，见下例。</ul><blockquote class="prompt-tip"><div><p>使用自动微分工具可以画一个函数导数的图像，参见书第 4 章画各种激活函数及其导数。例，Sigmoid 函数：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mf">8.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span><span class="p">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">detach</span><span class="p">(),</span> <span class="n">x</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</pre></table></code></div></div><p>注意此时 y 是向量，要令其每一维对 x 每一维分别求导，须在 backward 函数中加 <code class="language-plaintext highlighter-rouge">torch.ones_like(x)</code> 参数。</p></div></blockquote><h1 id="深度学习模型的-pipeline">深度学习模型的 Pipeline</h1><p>深度学习的完整流程如下：</p><ol><li>数据预处理；<li>定义模型、损失函数、优化器、初始化等；<li>训练模型；<li>测试模型。 以下各节具体讲解细节。</ol><p>在 PyTorch 实现中，以上每一步都包含许多值得单独讲的专题。作为学习笔记系列的第一篇，先从简单模型出发，将这些流程实现一遍，好对 PyTorch 有个整体的认识。将介绍三个简单模型，分别是：</p><ul><li>线性回归；<li>Softmax 多分类；<li>多层感知机（MLP）。 每个模型都分从头开始实现和简洁实现两种实现方法。简洁实现是调用 PyTorch 提供的高级 API，从头开始实现是自己写训练过程等细节，仅利用 PyTorch 的自动微分功能。这样有利于理解深度学习框架相比于其他包为深度学习带来的极大方便。</ul><h2 id="一线性回归"><span class="mr-2">一、线性回归</span><a href="#一线性回归" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>本节欲训练线性回归模型：\(\mathbf{y} = \mathbf{X}\mathbf{w} + b + \epsilon\)。</p><p>PyTorch 作深度学习使用的数据集都是它定义的 Dataset 类型。这里用到的数据暂时不涉及该类型，而是手动生成的普通的 Tensor。本例生成方法：给定 \(\mathbf{w}, b\) 的真实值，按正态分布（<code class="language-plaintext highlighter-rouge">torch.normal</code>）生成 \(\mathbf{X}, \mathbf{y}\)，用一个 <code class="language-plaintext highlighter-rouge">synthetic_data(w, b, num_examples)</code> 函数实现（可以自己写一下试试，练练 Tensor 的使用）。</p><h3 id="从头开始实现"><span class="mr-2">从头开始实现</span><a href="#从头开始实现" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>深度学习通常是按批（batch）训练的，因此数据 \(\mathbf{X}, \mathbf{y}\) 还需按一定的批数据量（batch_size）划分成各批数据。代码没有简单地切片成 batch 并存到列表里，而是通过<strong>生成器</strong>（generator）生成（参考我的 <a href="">Python 笔记</a>），这样的好处是每次训练需要时调用一次生成器，它现场给你生成新的一个 batch 的数据（<strong>是这些数据拼接成的矩阵</strong>），而无需一开始就划好，否则一开始切片 batch 这种预处理工作就要花费很多时间，会使训练过程迟迟不能开动。生成器函数 <code class="language-plaintext highlighter-rouge">data_iter(batch_size, features, labels)</code> 也可以自己试试，要注意两个细节：shuffle 的实现只需 shuffle 索引；如何 num_examples 不能整除 batch_size，尾部如何处理。</p><p>先看训练框架：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_iter</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">l</span><span class="p">.</span><span class="nb">sum</span><span class="p">().</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">sgd</span><span class="p">([</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">],</span> <span class="n">lr</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">train_l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">labels</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, loss </span><span class="si">{</span><span class="nb">float</span><span class="p">(</span><span class="n">train_l</span><span class="p">.</span><span class="n">mean</span><span class="p">()):</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</pre></table></code></div></div><p>中间的三句分别是前向传播、后向传播、梯度下降。每次从生成器生成一个 batch 的数据训练用。最外层循环为轮数，每一轮结束都要统计一下当前训得的模型在整个训练集上的 loss。</p><blockquote class="prompt-warning"><div><p>统计的时候不需求导引入计算图，可以用 <code class="language-plaintext highlighter-rouge">with torch.no_grad():</code> 包裹，纯粹为了减少计算量，不包裹也不会出错（例如下面的简洁实现就没有包裹）。但是有的地方引入计算图会引起混乱，如下面 <code class="language-plaintext highlighter-rouge">sgd()</code> 函数里的梯度下降更新式，一定要包裹。</p></div></blockquote><p>训练过程就是不断更新参数 <code class="language-plaintext highlighter-rouge">w,b</code>，中间三句是如何更新的呢？答案就是自动微分。在此代码前应对 <code class="language-plaintext highlighter-rouge">w,b</code> 作初始化：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></table></code></div></div><p>开启求导模式后，定义被求导函数，在深度学习中就是损失函数 \(l = \sum_{i=1}^{batch_size} loss(net(X, y, b))\)。<code class="language-plaintext highlighter-rouge">net()</code> 是模型函数（输出预测值），<code class="language-plaintext highlighter-rouge">loss()</code> 是损失函数，它们都是事先定义的 Python 函数（参见“自动微分”的注意点 2）：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">linreg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>

<span class="k">def</span> <span class="nf">squared_loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">linreg</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">squared_loss</span>
</pre></table></code></div></div><blockquote class="prompt-warning"><div><p>要注意的是 \(X\) 不是一个数据点，而是一批数据拼成的矩阵，在写上述函数时要注意应当完成对一整个 batch 的数据的操作。</p></div></blockquote><p>被求导函数就是通过 <code class="language-plaintext highlighter-rouge">l = loss(net(X, w, b), y)</code> 和 <code class="language-plaintext highlighter-rouge">l.sum()</code> 定义的。注意前者得到的 l 是一个长度为 batch_size 的向量，因为 <code class="language-plaintext highlighter-rouge">X,y</code> 是一个 batch 的数据，它们是一起计算的（矩阵化比 for 循环要快），求和后才是这一 batch 的损失函数。</p><p>接下来 <code class="language-plaintext highlighter-rouge">.backward()</code> 执行求导。由于只有 <code class="language-plaintext highlighter-rouge">w,b</code> 开启了求导模式，也只会求 \(\frac{\partial l}{\partial w}, \frac{\partial l}{\partial b}\)，导数结果存放在 <code class="language-plaintext highlighter-rouge">w,b</code> 的 grad 属性中。</p><p>下一步是梯度下降，打包成一个函数 <code class="language-plaintext highlighter-rouge">sgd(params, lr, batch_size)</code>。首先一个小细节是捆起来传参数列表 <code class="language-plaintext highlighter-rouge">params</code>，除了代码易于维护，另外就是将其变为可变类型，直接修改 <code class="language-plaintext highlighter-rouge">w,b</code> 而不需返回。再说一遍，无需单独传导数，已经存放在 grad 属性中。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">sgd</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
            <span class="n">param</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">param</span><span class="p">.</span><span class="n">grad</span> <span class="o">/</span> <span class="n">batch_size</span>
            <span class="n">param</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></table></code></div></div><p>主体部分是每个参数都执行梯度下降，<code class="language-plaintext highlighter-rouge">lr</code> 是事先定义好的学习率，还要除以 batch_size 是因为之前 l 的计算没有平均，放到这里，也起到规范化步长的作用。注意梯度下降的表达式会构造新的计算图，导致混乱，一定要以 <code class="language-plaintext highlighter-rouge">with torch.no_grad():</code> 包裹。</p><p>梯度下降结束后，<code class="language-plaintext highlighter-rouge">w,b</code> 的值随即更新。由于下一个 batch 还要求导，不要忘了给 grad 清零。这里把它巧妙地写在 <code class="language-plaintext highlighter-rouge">sgd()</code> 函数里面，能充分利用 for 循环遍历参数，而不需分别写 <code class="language-plaintext highlighter-rouge">w,b</code>。</p><p>训练结束后，我们得到 <code class="language-plaintext highlighter-rouge">w,b</code>，就得到了训练好的模型，调用 <code class="language-plaintext highlighter-rouge">net(x, w, b)</code> 可对 x 进行预测。本文比较了训练的 <code class="language-plaintext highlighter-rouge">w,b</code> 和生成数据时真实的 <code class="language-plaintext highlighter-rouge">w,b</code> 的误差。</p><h3 id="简洁实现"><span class="mr-2">简洁实现</span><a href="#简洁实现" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>上述实现中很多步骤可以换成 PyTorch 简洁的 API 实现。</p><p>首先是现成的生成器，PyTorch 里有现成的 Dataloader 类可使用（<a href="https://pytorch.org/docs/stable/data.html">文档</a>）。这个类的实例就是生成器，构造函数为</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">torch.utils</span> <span class="kn">import</span> <span class="n">data</span>
<span class="n">data_iter</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">Dataloader</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></table></code></div></div><p>此句为从 dataset 构造大小为 batch_size 的数据生成器。dataset 是 PyTorch 的 Dataset 类型（<code class="language-plaintext highlighter-rouge">torch.utils.data.Dataset</code>），需要按规则构造，当然也有现成的数据集（从 <code class="language-plaintext highlighter-rouge">torchvision.datasets</code> 里 import 即可）。构造规则是一个比较麻烦的事，将在别的笔记中再讨论。书中这里的代码暂时省略了对它的讨论。</p><p>其他选项：</p><ul><li><code class="language-plaintext highlighter-rouge">shuffle</code>：指定需不需要打乱数据的顺序；<li><code class="language-plaintext highlighter-rouge">sampler</code>,<code class="language-plaintext highlighter-rouge">batch_sampler</code>：自定义采集 batch 的方式，传入的是 <code class="language-plaintext highlighter-rouge">torch.utils.data.Sampler</code> 类型。不指定则采用顺序采集（<code class="language-plaintext highlighter-rouge">shuffle=False</code>）或随机采集（<code class="language-plaintext highlighter-rouge">shuffle=True</code>）。</ul><p>第二是现成的模型、损失函数。上述线性模型函数 <code class="language-plaintext highlighter-rouge">linreg</code>、平方损失 <code class="language-plaintext highlighter-rouge">squared_loss</code> 无需自己定义，在 PyTorch 中有现成的：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">MSELoss</span><span class="p">()</span>
</pre></table></code></div></div><p>PyTorch 中的线性模型就是 <code class="language-plaintext highlighter-rouge">nn.Linear(m, n)</code>，<code class="language-plaintext highlighter-rouge">m, n</code> 分别为输入、输出神经元数。<code class="language-plaintext highlighter-rouge">nn.Sequential()</code> 将不同的 Layer 串联起来构造成一个大的模型，它其实是一个容器，通过下标索引 <code class="language-plaintext highlighter-rouge">net[0]</code> 可以选中各层。它们的类型是 PyTorch 自己的 <code class="language-plaintext highlighter-rouge">torch.nn.modules</code> 里的“模块”类型，各个“模块”具有树状的父子关系，例如本例是父模块 <code class="language-plaintext highlighter-rouge">net</code>（nn.Sequential）下嵌套子模块 <code class="language-plaintext highlighter-rouge">net[0]</code>（nn.Linear）。在<a href="">此笔记</a>中将介绍复杂的深度网络，将见到更多复杂的模块组合。</p><p>这些现成的函数作用相当于“自动微分”注意点 2 的所说的函数，但是还是有不一样的地方。它们的一个重要特点是<strong>模型参数都存放到这里面了</strong>，它是真正意义上的模型。通过以下代码，体会一下如何查看模型参数：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="c1"># 参数初始化
</span><span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># 打印参数
</span><span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">:</span><span class="n">f</span><span class="si">}</span><span class="s"> </span><span class="si">{</span><span class="n">net</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span><span class="p">:</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</pre></table></code></div></div><p>优化器也是事先构造好的：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</pre></table></code></div></div><p>即实例化一个 <code class="language-plaintext highlighter-rouge">Optimizer</code> 优化器类，优化器可从 <code class="language-plaintext highlighter-rouge">torch.optim</code> 里挑选，有 SGD, Adam 等。注意这里 <code class="language-plaintext highlighter-rouge">net.parameters()</code>，模型参数从一开始就与优化器绑定到一起了。（注意这个事情，有助于理解下面 <code class="language-plaintext highlighter-rouge">trainer</code> 不需要传模型参数。）</p><p>训练步骤简化为：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">data_iter</span><span class="p">:</span>
        <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">trainer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">l</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">trainer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">features</span><span class="p">),</span> <span class="n">labels</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'epoch </span><span class="si">{</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="si">}</span><span class="s">, loss </span><span class="si">{</span><span class="n">l</span><span class="p">:</span><span class="n">f</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</pre></table></code></div></div><p>到现在看，简洁性体现在以下几个方面，PyTorch 的设计者把所有有门槛的、需要深度理解的细节全都隐藏了：</p><ul><li>不需要自己写模型、损失等函数，省去了考虑那些复杂的矩阵操作；<li>优化器不需要自己写，而且 <code class="language-plaintext highlighter-rouge">trainer</code> 什么参数也不用传（甚至模型参数），调用一下 <code class="language-plaintext highlighter-rouge">step()</code> 搞定；甚至 grad 清零的工作挪到了 <code class="language-plaintext highlighter-rouge">trainer.zero_grad()</code>，同样不需要传模型参数。</ul><p>还有几个小细节也足以体现：</p><ul><li><code class="language-plaintext highlighter-rouge">net()</code>不需要显式地传入模型参数，直接写 <code class="language-plaintext highlighter-rouge">net(X)</code> 即可；<li><code class="language-plaintext highlighter-rouge">l</code> 不需要 <code class="language-plaintext highlighter-rouge">sum()</code> 了，直接 <code class="language-plaintext highlighter-rouge">backward()</code> 后面也能知道什么意思；</ul><p>一开始学习深度学习框架，只需看懂高级 API 表面的工作流程，会写即可，并不特别需要了解这些 API 背后的细节。</p><h2 id="二softmax-多分类"><span class="mr-2">二、Softmax 多分类</span><a href="#二softmax-多分类" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>第二个模型是 Softmax 多分类模型：\(\mathbf{O} = \mathbf{X} \mathbf{W} + \mathbf{b}, \mathbf{y} = Softmax(\mathbf{O})\)。</p><p>此部分做的是图像分类问题，用的是 Fashion-MNIST 图像数据集，做 10 分类。这是会涉及使用 Dataset 类型的使用，但仅限于调用 PyTorch 自带的 Dataset 数据集实例。在安装 PyTorch 时可以看到，它包含 3 个库，torch 即深度学习框架，是工具；而 torchvision，torchaudio 是专门提供例子的库：数据集，网络，变换，分成视觉和语音两部分。因此 PyTorch 提供的图像数据集在 torchvision.datasets 里。这里面常见的 MNIST、CIFAR、ImageNet 数据集都有，见<a href="https://pytorch.org/vision/stable/datasets.html">官方文档</a>。</p><p>读取数据集即从其中的类中创建实例：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">torchvision</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>

<span class="n">trans</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">()</span>
<span class="n">mnist_train</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">"../data"</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">trans</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">mnist_test</span> <span class="o">=</span> <span class="n">torchvision</span><span class="p">.</span><span class="n">datasets</span><span class="p">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="n">root</span><span class="o">=</span><span class="s">"../data"</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">trans</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></table></code></div></div><p>它将从 <code class="language-plaintext highlighter-rouge">root</code> 指示的目录（路径格式见<a href="">Linux 学习笔记</a>）中寻找 FashionMNIST 数据，如果没有：<code class="language-plaintext highlighter-rouge">download=False</code> 时报错，<code class="language-plaintext highlighter-rouge">download=True</code> 时将从网上下载数据到该目录内，同时语句返回 Dataset 类型的变量 <code class="language-plaintext highlighter-rouge">mnist_train</code>,<code class="language-plaintext highlighter-rouge">mnist_test</code>，它包含一对对 \((X,y)\) 元组，可以像列表一样中括号索引。要注意，需要规定 <code class="language-plaintext highlighter-rouge">transform=transform.ToTensor()</code>，这样里面的 X 才是 Tensor 类型，否则默认为 PIL 类型（Python Image Library，是 Python 图像处理标准库 Pillow 表示图像的类型）。</p><h3 id="从头开始实现-1"><span class="mr-2">从头开始实现</span><a href="#从头开始实现-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>对于数据生成器，这里直接使用简洁实现——Dataloader，没有再从头实现。要注意测试数据也需要构造 Dataloader。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">train_iter</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">Dataloader</span><span class="p">(</span><span class="n">mnist_train</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">test_iter</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">Dataloader</span><span class="p">(</span><span class="n">mnist_test</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></table></code></div></div><p>以下是从头开始实现定义的模型和损失函数，这里不打算细讲，看看就好，基本上是各种矩阵操作、广播机制的巧妙运用。可以看到，自己写这些东西是比较麻烦的，就是因为需要注意一整个 batch 的数据传入的问题，这就涉及更高阶的矩阵操作。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">X_exp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">partition</span> <span class="o">=</span> <span class="n">X_exp</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># keepdim 是为了下面用广播机制
</span>    <span class="k">return</span> <span class="n">X_exp</span> <span class="o">/</span> <span class="n">partition</span>

<span class="k">def</span> <span class="nf">softmax_linreg</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">W</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">cross_entropy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">y_hat</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y_hat</span><span class="p">)),</span> <span class="n">y</span><span class="p">])</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">softmax_linreg</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">cross_entropy</span>
</pre></table></code></div></div><p>注意，X 是 (28,28) 图像，将其拉直这一操作放到了模型里（<code class="language-plaintext highlighter-rouge">X.reshape(-1, 28*28)</code>），而不是数据预处理过程中。其他小细节是没有把参数 <code class="language-plaintext highlighter-rouge">W,b</code> 传入函数参数，而是当作全局变量了（其实这样不太好）。<code class="language-plaintext highlighter-rouge">W,b</code> 也像之前一样手动构造并初始化：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">28</span><span class="o">*</span><span class="mi">28</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></table></code></div></div><p>书中到这里第一次涉及测试过程的写法。测试过程涉及准确率，在从头实现中也是要自己写的。注意它和 loss 一样要考虑一整个 batch 传入的问题，if 语句就是在检查是否为单个数据的：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_hat</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">y_hat</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">y_hat</span> <span class="o">=</span> <span class="n">y_hat</span><span class="p">,</span><span class="n">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="nb">cmp</span> <span class="o">=</span> <span class="n">y_hat</span><span class="p">.</span><span class="nb">type</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">==</span> <span class="n">y</span>
    <span class="k">return</span> <span class="nb">float</span><span class="p">(</span><span class="nb">cmp</span><span class="p">.</span><span class="nb">type</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">dtype</span><span class="p">).</span><span class="nb">sum</span><span class="p">())</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></table></code></div></div><p>书中用了自己构造的数据结构 <code class="language-plaintext highlighter-rouge">Accumulator</code> 作统计工作，有点麻烦，我翻译了一下伪代码：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">test_iter</span><span class="p">:</span>
        <span class="c1"># 计算累加 loss(net(X), y)
</span>        <span class="c1"># 计算累加 accuracy(net(X), y)
</span>    <span class="c1"># 打印统计后的 loss 和 accuracy
</span></pre></table></code></div></div><p>从此模型开始，作者自己写了一个 <code class="language-plaintext highlighter-rouge">Animator</code> 类用于展示每个 epoch 训练情况，可实时画出训练 loss，测试准确率等。这个东西实在没必要自己写，有现成的工具 TensorBoard 很好用，参考我的 <a href="">TensorBoard 学习笔记</a>。</p><h3 id="简洁实现-1"><span class="mr-2">简洁实现</span><a href="#简洁实现-1" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>简洁实现仍然使用了现成的模型、损失函数、优化器。优化器的简化同上，这里就看一看模型和损失函数的定义：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</pre></table></code></div></div><p>等等！Softmax 函数哪儿去了？这是一个重要的问题。实际上，Softmax 函数放到了 <code class="language-plaintext highlighter-rouge">CrossEntropyLoss</code> 里面了，也就是说 <code class="language-plaintext highlighter-rouge">net(X)</code> 输出的是未经 Softmax 规范化的预测。PyTorch 这样设计的原因涉及背后的计算机理，是为效率服务的，详见书 3.7.2 节。</p><p>这里的初始化用了另一套 API：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">:</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="n">net</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
</pre></table></code></div></div><p>对于 <code class="language-plaintext highlighter-rouge">nn.Module</code> 模块，它的 <code class="language-plaintext highlighter-rouge">apply</code> 方法可以将传入的函数递归地作用到它包含的所有模块上。本例即将 <code class="language-plaintext highlighter-rouge">init_weights</code> 作用在 <code class="language-plaintext highlighter-rouge">net</code>（<code class="language-plaintext highlighter-rouge">nn.Sequential</code>），<code class="language-plaintext highlighter-rouge">net[0]</code>（<code class="language-plaintext highlighter-rouge">nn.Flatten</code>），<code class="language-plaintext highlighter-rouge">net[1]</code>（<code class="language-plaintext highlighter-rouge">nn.Linear</code>）。由于 <code class="language-plaintext highlighter-rouge">init_weights</code> 里的 if 语句，只对 <code class="language-plaintext highlighter-rouge">net[1]</code> 应用 <code class="language-plaintext highlighter-rouge">nn.init.normal_</code>。<code class="language-plaintext highlighter-rouge">nn.init</code> 的用法详见<a href="">笔记（三）</a>。</p><p>另外，PyTorch 里没有实用的求准确率的 API，因为实在是没必要，自己写两个小函数就解决了。</p><p>测试过程同上从头开始实现。</p><h2 id="三多层感知机mlp"><span class="mr-2">三、多层感知机（MLP）</span><a href="#三多层感知机mlp" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>本节问题仍为图像多分类问题。MLP 模型与上面 Softmax 多分类相比，无非是网络层数由一层变为多层，层间引入了激活函数。</p><h3 id="从头开始实现-2"><span class="mr-2">从头开始实现</span><a href="#从头开始实现-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>模型定义和初始化大同小异。这里值得关注的新东西是：参数打包成 <code class="language-plaintext highlighter-rouge">nn.Parameter</code> 实例。前面见过简洁实现中模型的参数 <code class="language-plaintext highlighter-rouge">net.parameters()</code> 就是 <code class="language-plaintext highlighter-rouge">nn.Parameter</code> 类型的，它是进一步封装的类。而这里即使没有用到简洁实现的 <code class="language-plaintext highlighter-rouge">nn.modules</code>，也能当作一般的 Tensor 正常使用，还是很灵活的（原因：<code class="language-plaintext highlighter-rouge">nn.Parameter</code> 源代码定义了 <code class="language-plaintext highlighter-rouge">__new__()</code> 方法，它返回 Tensor 类型）。另外，下面的 <code class="language-plaintext highlighter-rouge">@</code> 运算符是 PyTorch 重载的，等价于矩阵乘法 <code class="language-plaintext highlighter-rouge">torch.matmul()</code>。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
</pre><td class="rouge-code"><pre><span class="n">W1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">b1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
<span class="n">W2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="n">b2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">W1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">W2</span><span class="p">,</span> <span class="n">b2</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">mlp</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_outputs</span><span class="p">))</span>
    <span class="n">H</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">X</span><span class="o">@</span><span class="n">W1</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">H</span><span class="o">@</span><span class="n">W2</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">mlp</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
</pre></table></code></div></div><h3 id="简洁实现-2"><span class="mr-2">简洁实现</span><a href="#简洁实现-2" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3></h3><p>这里唯一的变化是定义模型多了两个模块：隐藏层和激活函数。不再详述。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">Flatten</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>
</pre></table></code></div></div><p><br /></p><p>总结一下，有了自动微分这一工具后，深度学习看似简单，但是上面所有的从头开始实现，写起来真的特别麻烦，要顾虑很多细节如矩阵化，有很多坑。深度学习框架的高级 API 不仅写法简单，写模型就跟搭积木一样，不用考虑细节，而且采取了额外的预防措施确保数值稳定性，帮助编程人员避免从头实现可能遇到的陷阱。所以以后如非学习目的，能用框架就不要自己手写！</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%E7%A7%91%E7%A0%94/'>科研</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/pytorch/" class="post-tag no-text-decoration" >PyTorch</a> <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" class="post-tag no-text-decoration" >读书笔记</a> <a href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-tag no-text-decoration" >《动手学深度学习》</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-tag no-text-decoration" >机器学习</a> <a href="/tags/%E6%8A%80%E6%9C%AF/" class="post-tag no-text-decoration" >技术</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> 本文由作者按照 <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> 进行授权，转载请注明</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="http://service.weibo.com/share/share.php?title=PyTorch 学习笔记（一）：自动微分，简单模型的实现 - Shawn Wang&amp;url=https://pengxiang-wang.github.io/posts/studynotes_PyTorch_autograd_and_pipeline/" data-toggle="tooltip" data-placement="top" title="Weibo" target="_blank" rel="noopener" aria-label="Weibo"> <i class="fa-fw fab fa-weibo"></i> </a> <a href="https://twitter.com/intent/tweet?text=PyTorch 学习笔记（一）：自动微分，简单模型的实现 - Shawn Wang&amp;url=https://pengxiang-wang.github.io/posts/studynotes_PyTorch_autograd_and_pipeline/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=PyTorch 学习笔记（一）：自动微分，简单模型的实现 - Shawn Wang&amp;u=https://pengxiang-wang.github.io/posts/studynotes_PyTorch_autograd_and_pipeline/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="分享链接" data-title-succeed="链接已复制！"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">最近更新</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/accordion_transcribed_March-of-Steel-Torrent/">编配：《钢铁洪流进行曲》手风琴四重奏</a><li><a href="/posts/accordion_transcribed_Baikal-Lake/">编配：《贝加尔湖畔》手风琴独奏</a><li><a href="/posts/accordion_transcribed_Por-una-Cabeza/">编配：《一步之遥》手风琴四重奏</a><li><a href="/posts/accordion_transcribed_Katyusha/">编配：《喀秋莎》手风琴独奏</a><li><a href="/posts/accordion_transcribed_Miyazaki-Hayao-Movie-Themes/">编配：宫崎骏电影主题曲，手风琴二重奏</a></ul></div><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF/">技术</a> <a class="post-tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> <a class="post-tag" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a> <a class="post-tag" href="/tags/%E6%89%8B%E9%A3%8E%E7%90%B4/">手风琴</a> <a class="post-tag" href="/tags/%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0/">持续学习</a> <a class="post-tag" href="/tags/%E4%B9%90%E8%B0%B1/">乐谱</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/pytorch/">PyTorch</a> <a class="post-tag" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">《动手学深度学习》</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80/">语言</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">文章内容</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>相关文章</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/studynotes_PyTorch_nnModule/"><div class="card-body"> <em class="timeago small" data-ts="1644249600" > 2022-02-08 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PyTorch 学习笔记（三）：自定义网络结构（nn.Module）</h3><div class="text-muted small"><p> 本文介绍如何自定义模型，即 nn.Module 模块的逻辑与使用方法。由此可以搭建自己的深度网络结构。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 5.1、5.6 节：层和块，自定义层。 nn.Module 官方文档：https://pytorch.org/docs/stable/generated/torch.nn.Module...</p></div></div></a></div><div class="card"> <a href="/posts/studynotes_PyTorch_computing/"><div class="card-body"> <em class="timeago small" data-ts="1644508800" > 2022-02-11 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PyTorch 学习笔记（五）：计算性能</h3><div class="text-muted small"><p> 本文介绍 PyTorch 与计算性能有关的代码知识，包括如何使用 GPU、并行计算、多服务器计算等等。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 5.6 节：GPU； 第 12 章：计算性能； 深度学习与 GPU 众所周知，深度学习计算可以使用 GPU，往往能极大提高效率。GPU 用于深度学习时与其他任务不同，它更偏向...</p></div></div></a></div><div class="card"> <a href="/posts/studynotes_PyTorch_Dataset_and_Transform/"><div class="card-body"> <em class="timeago small" data-ts="1642867200" > 2022-01-23 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PyTorch 学习笔记（二）：自定义数据集，数据预处理</h3><div class="text-muted small"><p> 本文总结 PyTorch 中与数据集以及对它的预处理的知识。知乎的这篇文章讲得不错，言简意赅。也可参考官方教程。 PyTorch 中的数据集都是定义了一个 torch.utils.data.Dataset 类型，数据集都是这个类型的实例。必须这样做，因为后面构造 Dataloader 只接收 Dataset 类型，而整个训练过程都是对 Dataloader 的操作。我们已经在笔记（一）...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/studynotes_Russian_ABC/" class="btn btn-outline-primary" prompt="上一篇"><p>俄语学习笔记：字母表、语音与文字系统</p></a> <a href="/posts/studynotes_PyTorch_Dataset_and_Transform/" class="btn btn-outline-primary" prompt="下一篇"><p>PyTorch 学习笔记（二）：自定义数据集，数据预处理</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "pengxiang-wang/pengxiang-wang.github.io", "data-repo-id": "R_kgDOHJZRFQ", "data-category": "Announcements", "data-category-id": "DIC_kwDOHJZRFc4COm2c", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "zh-CN", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/pengxiang-wang">Shawn Wang</a>.</p></div><div class="footer-right"><p class="mb-0"> KEEP CALM & CARRY ON</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF/">技术</a> <a class="post-tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> <a class="post-tag" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a> <a class="post-tag" href="/tags/%E6%89%8B%E9%A3%8E%E7%90%B4/">手风琴</a> <a class="post-tag" href="/tags/%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0/">持续学习</a> <a class="post-tag" href="/tags/%E4%B9%90%E8%B0%B1/">乐谱</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/pytorch/">PyTorch</a> <a class="post-tag" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">《动手学深度学习》</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80/">语言</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">搜索结果为空</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/zh-cn.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-H7GH1F7FH5"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-H7GH1F7FH5'); }); </script>
