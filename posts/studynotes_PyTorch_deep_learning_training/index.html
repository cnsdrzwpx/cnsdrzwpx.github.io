<!DOCTYPE html><html lang="zh-CN" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="prefer-datetime-locale" content="zh-cn"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="PyTorch 学习笔记（四）：深度学习的训练" /><meta property="og:locale" content="zh_CN" /><meta name="description" content="本文总结 PyTorch 是如何实现深度学习训练中的各种技巧与细节，包括防止过拟合、参数初始化、优化器、损失函数、超参数优化等等。关于这部分知识，我在这篇笔记中有系统的总结。本文的编排顺序基本与这篇笔记对应（数据预处理部分在介绍 Dataset 类型的笔记中，调参、学习曲线等放在 PyTorch 工程性知识的笔记中）。此外，还有一篇笔记总结了深度学习训练的实践经验，可供参考。" /><meta property="og:description" content="本文总结 PyTorch 是如何实现深度学习训练中的各种技巧与细节，包括防止过拟合、参数初始化、优化器、损失函数、超参数优化等等。关于这部分知识，我在这篇笔记中有系统的总结。本文的编排顺序基本与这篇笔记对应（数据预处理部分在介绍 Dataset 类型的笔记中，调参、学习曲线等放在 PyTorch 工程性知识的笔记中）。此外，还有一篇笔记总结了深度学习训练的实践经验，可供参考。" /><link rel="canonical" href="https://pengxiang-wang.github.io/posts/studynotes_PyTorch_deep_learning_training/" /><meta property="og:url" content="https://pengxiang-wang.github.io/posts/studynotes_PyTorch_deep_learning_training/" /><meta property="og:site_name" content="Shawn Wang" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-02-11T00:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="PyTorch 学习笔记（四）：深度学习的训练" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-05-27T23:26:31+08:00","datePublished":"2022-02-11T00:00:00+08:00","description":"本文总结 PyTorch 是如何实现深度学习训练中的各种技巧与细节，包括防止过拟合、参数初始化、优化器、损失函数、超参数优化等等。关于这部分知识，我在这篇笔记中有系统的总结。本文的编排顺序基本与这篇笔记对应（数据预处理部分在介绍 Dataset 类型的笔记中，调参、学习曲线等放在 PyTorch 工程性知识的笔记中）。此外，还有一篇笔记总结了深度学习训练的实践经验，可供参考。","headline":"PyTorch 学习笔记（四）：深度学习的训练","mainEntityOfPage":{"@type":"WebPage","@id":"https://pengxiang-wang.github.io/posts/studynotes_PyTorch_deep_learning_training/"},"url":"https://pengxiang-wang.github.io/posts/studynotes_PyTorch_deep_learning_training/"}</script><title>PyTorch 学习笔记（四）：深度学习的训练 | Shawn Wang</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Shawn Wang"><meta name="application-name" content="Shawn Wang"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src=" /assets/img/avatar.jpg " alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Shawn Wang</a></div><div class="site-subtitle font-italic">WPX 的个人主页</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>首页</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>分类</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>标签</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>时间表</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>关于本站</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/pengxiang-wang" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://space.bilibili.com/88684674" aria-label="bilibili" target="_blank" rel="noopener"> <i class="fas fa-tv"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['shawn.pxwang','qq.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> 首页 </a> </span> <span>PyTorch 学习笔记（四）：深度学习的训练</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> 文章</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> </span> <span id="search-cancel" >取消</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>PyTorch 学习笔记（四）：深度学习的训练</h1><div class="post-meta text-muted"><div> 作者 <em> <a href="https://github.com/pengxiang-wang">Shawn Wang</a> </em></div><div class="d-flex"><div> <span> 发表于 <em class="timeago" data-ts="1644508800" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-02-11 </em> </span> <span> 更新于 <em class="timeago" data-ts="1685201191" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2023-05-27 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="3632 字"> <em>20 分钟</em>阅读</span></div></div></div><div class="post-content"><p>本文总结 PyTorch 是如何实现深度学习训练中的各种技巧与细节，包括防止过拟合、参数初始化、优化器、损失函数、超参数优化等等。关于这部分知识，我在<a href="">这篇笔记</a>中有系统的总结。本文的编排顺序基本与这篇笔记对应（数据预处理部分在<a href="">介绍 Dataset 类型的笔记</a>中，调参、学习曲线等放在 <a href="">PyTorch 工程性知识的笔记</a>中）。此外，还有一篇<a href="">笔记</a>总结了深度学习训练的实践经验，可供参考。</p><p>本文参考 <a href="https://d2l.ai">Dive into Deep Learning (PyTorch 版)</a> 中的以下内容：</p><ul><li>4.4 节：模型选择、欠拟合与过拟合；<li>4.5 节：权重衰减；<li>4.6 节：暂退法；<li>4.8 节：数值稳定性与模型初始化；<li>4.9 节：环境和分布偏移；<li>第 11 章：优化算法</ul><hr /><h1 id="一激活函数">一、激活函数</h1><p>激活函数在形式上应该是一个 Python 函数，它接受 Tensor 类型的输入并输出相同维度的 Tensor 变量。在<a href="">笔记（一）</a>MLP 的从头开始实现中可以看到：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">relu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">a</span><span class="p">)</span>
</pre></table></code></div></div><p>在 <code class="language-plaintext highlighter-rouge">torch.nn.functional</code> 中有各种预定义的激活函数，见文档：<a href="https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions">https://pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions</a></p><blockquote class="prompt-info"><div><p><code class="language-plaintext highlighter-rouge">torch.nn.functional</code> 提供了各种深度学习可能用到的预定义函数。这里都是用 Python 函数实现的，相对来说封装程度没有那么高，一般用于自己模型设计的零件；下面可以看到，很多这种函数如激活函数、损失函数是实现为一个可调用类的，封装程度更高。</p></div></blockquote><p>在深度学习中，激活函数的用处就是作为 <code class="language-plaintext highlighter-rouge">nn.Module</code> 模型的一个组成部分。使用的方法就是将其套在 forward 函数中。以下是<a href="">笔记（三）</a>自定义块中见到的例子：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">out</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden</span><span class="p">(</span><span class="n">X</span><span class="p">)))</span>
</pre></table></code></div></div><p>因此在实际使用 PyTorch 的高级 API 中，激活函数被看成是一个层，是 <code class="language-plaintext highlighter-rouge">nn.Module</code> 类型。我们也在<a href="">笔记（三）</a>自定义层中见到过：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">ReLULayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></table></code></div></div><p>当然 PyTorch 也预定义了很多这种激活函数层，它们放在 <code class="language-plaintext highlighter-rouge">torch.nn</code> 中（<a href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">文档</a>），与其他预定义的有实际参数的层（如 <code class="language-plaintext highlighter-rouge">nn.Linear</code>）并列。这种预定义层的用法就是放在 <code class="language-plaintext highlighter-rouge">nn.Sequential</code> 容器里，与其他层串联。例如，上述自定义层等价于如下预定义层：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
</pre></table></code></div></div><h1 id="三网络结构">三、网络结构</h1><p>注：“二、数据预处理” 我放到了笔记（二）中，和对 Dataset 的介绍放在了一起。下面从 “三、网络结构” 开始。</p><p>对网络结构下手的一些训练 trick 与 <code class="language-plaintext highlighter-rouge">nn.Module</code> 是兼容的，可以看作一种特殊的层。PyTorch 为 Dropout 和 Batch Normalization 都提供了高级的 API：<code class="language-plaintext highlighter-rouge">nn.Dropout()</code>、<code class="language-plaintext highlighter-rouge">nn.BatchNorm1d()</code>。对于图像等数据，还提供了 2D、3D 等版本。</p><p>这里需要注意的是，Dropout 和 Batch Normalization 都是训练和测试不一样的层（在训练阶段引入随机性，在测试阶段以期望值代替来消除随机性），<strong>所以这些层前向传播时，必须要有指示告诉它们是训练还是测试</strong>。PyTorch 设计了这个指示变量封装在 <code class="language-plaintext highlighter-rouge">nn.Module</code> 类型的实例属性 <code class="language-plaintext highlighter-rouge">training</code> 中（布尔变量）；方法 <code class="language-plaintext highlighter-rouge">.train()</code> 与 <code class="language-plaintext highlighter-rouge">.eval()</code> 可以修改此变量，把它放在整个训练或测试阶段开始前即可。</p><h1 id="四参数管理与初始化">四、参数管理与初始化</h1><p>这节的内容是参数初始化，我会连带讲解 PyTorch 的模型参数管理机制，即与 <code class="language-plaintext highlighter-rouge">nn.Module</code> 对象定义的模型参数有关的操作。</p><p><a href="">笔记（一）的最后一个模型</a>已经看到，<code class="language-plaintext highlighter-rouge">nn.Module</code> 的模型参数都属于 <code class="language-plaintext highlighter-rouge">nn.Parameter</code> 类。这是一个封装模型参数的类（<a href="https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html">文档</a>），将其与普通的 Tensor 区别开，便于训练和管理（我们在以后可以看到封装的优势）。此类在构造时接受两个参数：</p><ul><li><code class="language-plaintext highlighter-rouge">data</code>：参数数据，Tensor 或 <code class="language-plaintext highlighter-rouge">nn.Parameter</code> 类型（允许递归嵌套）；它直接构造了实例属性 <code class="language-plaintext highlighter-rouge">.data</code>，要直接取得 <code class="language-plaintext highlighter-rouge">nn.Parameter</code> 类封装的 Tensor，应访问该实例属性；<li><code class="language-plaintext highlighter-rouge">requires_grad</code>：指定 data 是否需要梯度。</ul><p>当然，参数指的是模型参数，是与模型挂钩的，一般不单独实例化 <code class="language-plaintext highlighter-rouge">nn.Parameter</code> 对象，而是在模型 <code class="language-plaintext highlighter-rouge">nn.Module</code> 实例化时就已经存在了。笔记（一）的最后一个模型只是起了演示的作用。</p><h2 id="访问模型的参数"><span class="mr-2">访问模型的参数</span><a href="#访问模型的参数" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p><img data-src="/assets/img/NestMLP_parameters_called.png" alt="6" data-proofer-ignore></p><p>模型的参数应该是模型的一个属性。上图是<a href="">笔记（三）</a> NestMLP 的参数图（它与笔记（三）图的区别在于，有参数的层外挂了一个绿色的叶子结点）。可以看到参数放在了模型的 <code class="language-plaintext highlighter-rouge">weight</code>、<code class="language-plaintext highlighter-rouge">bias</code> 属性中。</p><p>因此，访问模型某层的参数可以直接按照图中绿色叶子结点的调用方式。</p><p>PyTorch 也设计了访问模型所有参数的方法，注意，在 <code class="language-plaintext highlighter-rouge">nn.Module</code> 类中定义的所有 <code class="language-plaintext highlighter-rouge">nn.Parameter</code> 的实例属性都被视为模型参数（不管它是否参与到 forward 函数）。与 <code class="language-plaintext highlighter-rouge">nn.Module</code> 同理，其算法也是递归地遍历树的叶子。API 有：</p><ul><li><code class="language-plaintext highlighter-rouge">.parameters()</code> 方法：返回一个生成器。这种方式访问通常用于直接传入优化器的 <code class="language-plaintext highlighter-rouge">params</code> 参数，print 无法直接显示，需要遍历其元素 print（或者强制转化为列表）；<li><code class="language-plaintext highlighter-rouge">.named_parameters()</code> 方法，返回生成器生成的是 (参数名字, 参数数据) 对。参数的命名空间与模型一致；<li><code class="language-plaintext highlighter-rouge">.state_dict()</code> 方法：返回一个 <code class="language-plaintext highlighter-rouge">collections.OrderedDict</code> 类型，字典键值为 {参数名字:参数数据}， print 可以显示。</ul><h2 id="参数初始化"><span class="mr-2">参数初始化</span><a href="#参数初始化" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>这里讨论封装在 <code class="language-plaintext highlighter-rouge">nn.Parameter</code> 中的参数的初始化。</p><p>参数初始化当然也可以直接取出 data 属性，对其赋值或修改。但更好用的是能直接对 <code class="language-plaintext highlighter-rouge">nn.Parameter</code> 对象作初始化的修改函数，这也是封装 <code class="language-plaintext highlighter-rouge">nn.Parameter</code> 的意义。PyTorch 提供了很多初始化函数，它们作用在 <code class="language-plaintext highlighter-rouge">nn.Parameter</code> 对象上。这些函数定义在 <code class="language-plaintext highlighter-rouge">nn.init</code> 模块中，以下列举几个常用的，其他的详见文档：<a href="https://pytorch.org/docs/stable/nn.init.html">https://pytorch.org/docs/stable/nn.init.html</a>。</p><ul><li><code class="language-plaintext highlighter-rouge">nn.init.normal(tensor, mean, std)</code>：从正态分布 \(N(mean, std)\) 初始化<li><code class="language-plaintext highlighter-rouge">nn.init.constant(tensor, val)</code>：全部以常量 val 初始化<li><code class="language-plaintext highlighter-rouge">nn.init.uniform(tensor, a, b)</code>：从均匀分布 \(U(a,b)\) 初始化<li><code class="language-plaintext highlighter-rouge">nn.init.xavier_uniform(tensor, gain)</code>, <code class="language-plaintext highlighter-rouge">nn.init.xavier_normal(tensor, gain)</code>：Xavier 初始化<li><code class="language-plaintext highlighter-rouge">nn.init.kaiming_uniform(tensor, a, mode)</code>, <code class="language-plaintext highlighter-rouge">nn.init.kaiming_normal(tensor, a, mode)</code>：何恺明的初始化</ul><p>当然，这些函数内部细节就是取出 data 属性后对 Tensor 的操作，也可以自己按照的方式定义一个初始化函数。</p><blockquote class="prompt-info"><div><p>PyTorch 预定义的 <code class="language-plaintext highlighter-rouge">nn.Module</code> 层带有默认的初始化方法（<code class="language-plaintext highlighter-rouge">reset_parameters()</code>），是一些优秀初始化方法的汇总和精调，在实例化模型时就会调用它。PyTorch 为每种层都涉及了适合它们的默认初始化方法。 因此，除了研究不同初始化方法的需要，在高级 API 搭建深度学习模型的流程中可以省略初始化这一步。</p></div></blockquote><p>在实际中，参数初始化一般是整体地对一整个模型初始化。通常是打包成一个 <code class="language-plaintext highlighter-rouge">init_parameters</code> 函数，通过 <code class="language-plaintext highlighter-rouge">nn.Module</code> 的 <code class="language-plaintext highlighter-rouge">apply</code> 方法（可以将传入的函数递归地作用到它包含的所有层上）作用到模型参数上。以下是[笔记（一）]中出现的例子：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">:</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">init</span><span class="p">.</span><span class="n">normal_</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">weight</span><span class="p">,</span> <span class="n">std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="n">net</span><span class="p">.</span><span class="nb">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
</pre></table></code></div></div><p>可以看到，一个函数也能搞定对不同层的不同初始化，只要加 if 规则判断就行。这样打包成一个函数的好处是方便维护代码。</p><p>对于预训练模型用别人训好的参数作初始化，涉及模型的读写文件，见<a href="">这一篇笔记</a>，不在这里讲。</p><h1 id="五优化器">五、优化器</h1><p>优化器就是一个对参数的修改函数，它接受待更新的参数，利用参数中存储的梯度信息（见<a href="">笔记（一）自动微分</a>部分，计算的梯度存放在参数 Tensor 的 <code class="language-plaintext highlighter-rouge">grad</code> 属性中，因此不需要传梯度参数），在函数主体中完成对参数的一步更新（如梯度下降法）。最后要有一步梯度清零的操作，也是在这里实现的。还要注意这部分代码要用 <code class="language-plaintext highlighter-rouge">with torch.no_grad():</code> 包裹，这些在笔记（一）中都提到了。</p><p>一个优化器函数形式如下：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">optimizer</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">):</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span> <span class="c1"># 遍历模型参数
</span>            <span class="n">g</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="n">grad</span> <span class="c1"># 提取梯度
</span>            <span class="p">...</span> <span class="c1"># 对 p 的修改，是基于 g 的公式
</span>            <span class="n">p</span><span class="p">.</span><span class="n">grad</span><span class="p">.</span><span class="n">zero_</span><span class="p">()</span>
</pre></table></code></div></div><p>其中的超参数除学习率外可能还有很多，为了形式统一，往往打包成字典的形式，在用的时候取字典值。例如动量法传入的超参数形式为 <code class="language-plaintext highlighter-rouge">{'lr':lr, 'momentum':momentum}</code>。</p><p>之前实现的是随机梯度下降（SGD），相对比较简单，只需用一下梯度下降公式即可。更复杂的优化器如 Momentum、AdaGrad、Adam 等，往往需要维护一组状态值，它们随着训练过程也像参数一样进行迭代，且需要初始化。这种状态值的处理也很简单，可以放在全局变量或优化器函数的参数里，不再赘述。</p><p>实际上在 PyTorch 中，优化器并不是简单的修改函数，而是继承的 <code class="language-plaintext highlighter-rouge">torch.optim.Optimizer</code> 类，这样有利于提供更完整、健全的优化器功能，例如设置默认值等，PyTorch 提供的 API 也都是 <code class="language-plaintext highlighter-rouge">Optimizer</code> 类型，详见<a href="https://pytorch.org/docs/stable/optim.html#base-class">文档</a>；对参数的修改定义在 <code class="language-plaintext highlighter-rouge">step()</code> 方法里。自己写优化器时，如果有需求，可以按这种方式写比较复杂的类（但写成函数基本就够用了）：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">MyOptimizer</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">Optimizer</span><span class="p">):</span>

    <span class="n">def__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">hyperparams</span><span class="p">):</span>
        <span class="p">...</span>

    <span class="o">@</span><span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="p">...</span><span class="c1"># 对 self.params 的修改，梯度清零等
</span></pre></table></code></div></div><p>PyTorch 提供了方便的优化器 API，在 <code class="language-plaintext highlighter-rouge">torch.optim</code> 中（文档：<a href="https://pytorch.org/docs/stable/optim.html#algorithms">https://pytorch.org/docs/stable/optim.html#algorithms</a>），在搭建项目时，如无特殊需求，也不必自己写优化器：</p><ul><li><code class="language-plaintext highlighter-rouge">torch.optim.SGD</code>：实现了 SGD、SGD + Momentum；<li><code class="language-plaintext highlighter-rouge">torch.optim.Adagrad</code>、<code class="language-plaintext highlighter-rouge">torch.optim.RMSprop</code>：实现了 AdaGrad、RMSProp；<li><code class="language-plaintext highlighter-rouge">torch.optim.Adam</code>、<code class="language-plaintext highlighter-rouge">torch.optim.AdamW</code>：实现了 Adam 及其一些改进算法。</ul><h2 id="学习率调整器"><span class="mr-2">学习率调整器</span><a href="#学习率调整器" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2></h2><p>上述每个优化器都有学习率超参数，都可以设计学习率调整策略。学习率调整就是在训练过程中不断地改变学习率，这样看的话学习率就类似于一种参数。PyTorch 将学习率看做了一种参数，设计了专门针对这个“参数”的优化器——<strong>学习率调整器</strong>（learning rate scheduler），放在 <code class="language-plaintext highlighter-rouge">torch.optim.lr_scheduler</code> 中。</p><p>它的使用方法与 <code class="language-plaintext highlighter-rouge">Optimizer</code> 类似，但也有一个重要的区别（这也是为什么单独设计一个类的原因）：<code class="language-plaintext highlighter-rouge">lr_scheduler</code> 类传入的不是超参数，而是优化器，以指数衰减调整器为例，它的实例化：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">torch.optim.lr_scheduler</span> <span class="kn">import</span> <span class="n">ExponentialLR</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">scheduler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">lr_scheduler</span><span class="p">.</span><span class="n">ExponentialLR</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
</pre></table></code></div></div><p>这样是为了不想让学习率裸露在训练过程外面，而是始终与优化器捆绑在一起，为了代码的模块化。</p><p>如果不是专门研究学习率，PyTorch 提供的 API 就够用了（见<a href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate">文档</a>），这种东西用的不多，也几乎没有需要自己写的场合，所以知道有这种东西就差不多了。</p><h1 id="六损失函数">六、损失函数</h1><p>损失函数在形式伤也是一个 Python 函数，它接受数据预测值和真实值的 Tensor 类型的输入并输出一个数。在<a href="">笔记（一）</a>MLP 的从头开始实现中可以看到：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">squared_loss</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">y_hat</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">/</span> <span class="mi">2</span>
</pre></table></code></div></div><p>在 <code class="language-plaintext highlighter-rouge">torch.nn.functional</code> 中也有各种预定义的损失函数，见文档：<a href="https://pytorch.org/docs/stable/nn.functional.html#loss-functions">https://pytorch.org/docs/stable/nn.functional.html#loss-functions</a></p><p>与激活函数类似，损失函数也被 PyTorch 的高级 API 实现为一个 <code class="language-plaintext highlighter-rouge">nn.Module</code> 层，它们也放在 <code class="language-plaintext highlighter-rouge">torch.nn</code> 中（<a href="https://pytorch.org/docs/stable/nn.html#loss-functions">文档</a>），与其他预定义的有实际参数的层（如 <code class="language-plaintext highlighter-rouge">nn.Linear</code>）并列。这里预定义的都是最基本、原始的损失函数，使用方法相对来说比较固定，例如：</p><ul><li>交叉熵损失，用于分类问题：<code class="language-plaintext highlighter-rouge">nn.CrossEntropyLoss()</code>；<li>平方损失，用于回归问题：<code class="language-plaintext highlighter-rouge">nn.MSELoss()</code>；<li>…</ul><p>在深度学习中，除了原始的损失函数，还会加正则项，实现其他目的，如防止过拟合、迁移学习的迁移、持续学习的防遗忘等。</p><p>实现正则项的方式是自由的，可以与原始损失函数打包到一个 <code class="language-plaintext highlighter-rouge">nn.Module</code> 类（用 forward 函数嵌套的原理）或 Python 函数，也可以单独写成一个损失函数，在训练过程中计算损失的时候加进来：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">RegLoss</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">factor</span><span class="p">):</span>
        <span class="p">...</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">factor</span> <span class="o">=</span> <span class="n">factor</span> <span class="c1"># 正则化系数
</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="n">reg_loss</span> <span class="o">=</span> <span class="p">...</span> <span class="c1"># 使用 y_hat 和 y 定义的公式
</span>        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">factor</span> <span class="o">*</span> <span class="n">reg_loss</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">RegLoss</span><span class="p">(</span><span class="n">factor</span><span class="o">=</span><span class="n">FACTOR</span><span class="p">)</span>

<span class="c1"># 训练过程
</span><span class="k">for</span> <span class="n">epoch</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">batch</span><span class="p">:</span>
        <span class="p">...</span>
        <span class="n">l</span> <span class="o">=</span> <span class="n">loss</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">reg</span><span class="p">(</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">l</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="p">...</span>
<span class="p">...</span>
</pre></table></code></div></div><p>正则化系数可以放在训练过程外面，也可以像上面这样打包到损失函数里面。具体怎么实现，全看代码模块化程度的需要。</p><p>值得注意的是 L2 正则化，因为它与修改梯度下降公式的权重衰减等价，除了在损失中实现 L2 正则项，还可以直接在优化器中实现。在 PyTorch 的高级 API 中，大部分优化器提供一个超参数：<code class="language-plaintext highlighter-rouge">weight_decay</code>，传入即可实现权重衰减。</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%E7%A7%91%E7%A0%94/'>科研</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/pytorch/" class="post-tag no-text-decoration" >PyTorch</a> <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" class="post-tag no-text-decoration" >读书笔记</a> <a href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-tag no-text-decoration" >《动手学深度学习》</a> <a href="/tags/%E6%8A%80%E6%9C%AF/" class="post-tag no-text-decoration" >技术</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> 本文由作者按照 <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> 进行授权，转载请注明</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="http://service.weibo.com/share/share.php?title=PyTorch 学习笔记（四）：深度学习的训练 - Shawn Wang&amp;url=https://pengxiang-wang.github.io/posts/studynotes_PyTorch_deep_learning_training/" data-toggle="tooltip" data-placement="top" title="Weibo" target="_blank" rel="noopener" aria-label="Weibo"> <i class="fa-fw fab fa-weibo"></i> </a> <a href="https://twitter.com/intent/tweet?text=PyTorch 学习笔记（四）：深度学习的训练 - Shawn Wang&amp;url=https://pengxiang-wang.github.io/posts/studynotes_PyTorch_deep_learning_training/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=PyTorch 学习笔记（四）：深度学习的训练 - Shawn Wang&amp;u=https://pengxiang-wang.github.io/posts/studynotes_PyTorch_deep_learning_training/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="分享链接" data-title-succeed="链接已复制！"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">最近更新</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/accordion_transcribed_March-of-Steel-Torrent/">编配：《钢铁洪流进行曲》手风琴四重奏</a><li><a href="/posts/accordion_transcribed_Baikal-Lake/">编配：《贝加尔湖畔》手风琴独奏</a><li><a href="/posts/accordion_transcribed_Por-una-Cabeza/">编配：《一步之遥》手风琴四重奏</a><li><a href="/posts/accordion_transcribed_Katyusha/">编配：《喀秋莎》手风琴独奏</a><li><a href="/posts/accordion_transcribed_Miyazaki-Hayao-Movie-Themes/">编配：宫崎骏电影主题曲，手风琴二重奏</a></ul></div><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF/">技术</a> <a class="post-tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> <a class="post-tag" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a> <a class="post-tag" href="/tags/%E6%89%8B%E9%A3%8E%E7%90%B4/">手风琴</a> <a class="post-tag" href="/tags/%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0/">持续学习</a> <a class="post-tag" href="/tags/%E4%B9%90%E8%B0%B1/">乐谱</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/pytorch/">PyTorch</a> <a class="post-tag" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">《动手学深度学习》</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80/">语言</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">文章内容</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>相关文章</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/studynotes_PyTorch_autograd_and_pipeline/"><div class="card-body"> <em class="timeago small" data-ts="1642780800" > 2022-01-22 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PyTorch 学习笔记（一）：自动微分，简单模型的实现</h3><div class="text-muted small"><p> 本系列博文是我学习深度学习框架的学习笔记。深度学习框架大同小异，只须学习一种的原理，其他的都可以快速上手。我使用的是 PyTorch。笔记将着重强调代码原理、思想的理解，而不是具体的代码。 PyTorch 官方文档：https://pytorch.org/docs/stable/index.html PyTorch 中文文档：https://pytorch-cn.readthed...</p></div></div></a></div><div class="card"> <a href="/posts/studynotes_PyTorch_nnModule/"><div class="card-body"> <em class="timeago small" data-ts="1644249600" > 2022-02-08 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PyTorch 学习笔记（三）：自定义网络结构（nn.Module）</h3><div class="text-muted small"><p> 本文介绍如何自定义模型，即 nn.Module 模块的逻辑与使用方法。由此可以搭建自己的深度网络结构。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 5.1、5.6 节：层和块，自定义层。 nn.Module 官方文档：https://pytorch.org/docs/stable/generated/torch.nn.Module...</p></div></div></a></div><div class="card"> <a href="/posts/studynotes_PyTorch_computing/"><div class="card-body"> <em class="timeago small" data-ts="1644508800" > 2022-02-11 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PyTorch 学习笔记（五）：计算性能</h3><div class="text-muted small"><p> 本文介绍 PyTorch 与计算性能有关的代码知识，包括如何使用 GPU、并行计算、多服务器计算等等。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 5.6 节：GPU； 第 12 章：计算性能； 深度学习与 GPU 众所周知，深度学习计算可以使用 GPU，往往能极大提高效率。GPU 用于深度学习时与其他任务不同，它更偏向...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/studynotes_PyTorch_computing/" class="btn btn-outline-primary" prompt="上一篇"><p>PyTorch 学习笔记（五）：计算性能</p></a> <a href="/posts/accordion_transcribed_March-of-Steel-Torrent/" class="btn btn-outline-primary" prompt="下一篇"><p>编配：《钢铁洪流进行曲》手风琴四重奏</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "pengxiang-wang/pengxiang-wang.github.io", "data-repo-id": "R_kgDOHJZRFQ", "data-category": "Announcements", "data-category-id": "DIC_kwDOHJZRFc4COm2c", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "zh-CN", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/pengxiang-wang">Shawn Wang</a>.</p></div><div class="footer-right"><p class="mb-0"> KEEP CALM & CARRY ON</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF/">技术</a> <a class="post-tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> <a class="post-tag" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a> <a class="post-tag" href="/tags/%E6%89%8B%E9%A3%8E%E7%90%B4/">手风琴</a> <a class="post-tag" href="/tags/%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0/">持续学习</a> <a class="post-tag" href="/tags/%E4%B9%90%E8%B0%B1/">乐谱</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/pytorch/">PyTorch</a> <a class="post-tag" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">《动手学深度学习》</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80/">语言</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">搜索结果为空</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/zh-cn.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-H7GH1F7FH5"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-H7GH1F7FH5'); }); </script>
