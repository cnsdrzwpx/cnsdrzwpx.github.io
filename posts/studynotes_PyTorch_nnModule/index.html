<!DOCTYPE html><html lang="zh-CN" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="prefer-datetime-locale" content="zh-cn"><meta name="generator" content="Jekyll v4.2.2" /><meta property="og:title" content="PyTorch 学习笔记（三）：自定义网络结构（nn.Module）" /><meta property="og:locale" content="zh_CN" /><meta name="description" content="本文介绍如何自定义模型，即 nn.Module 模块的逻辑与使用方法。由此可以搭建自己的深度网络结构。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 5.1、5.6 节：层和块，自定义层。" /><meta property="og:description" content="本文介绍如何自定义模型，即 nn.Module 模块的逻辑与使用方法。由此可以搭建自己的深度网络结构。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 5.1、5.6 节：层和块，自定义层。" /><link rel="canonical" href="https://pengxiang-wang.github.io/posts/studynotes_PyTorch_nnModule/" /><meta property="og:url" content="https://pengxiang-wang.github.io/posts/studynotes_PyTorch_nnModule/" /><meta property="og:site_name" content="Shawn Wang" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-02-08T00:00:00+08:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="PyTorch 学习笔记（三）：自定义网络结构（nn.Module）" /><meta name="twitter:site" content="@twitter_username" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-05-27T23:26:31+08:00","datePublished":"2022-02-08T00:00:00+08:00","description":"本文介绍如何自定义模型，即 nn.Module 模块的逻辑与使用方法。由此可以搭建自己的深度网络结构。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 5.1、5.6 节：层和块，自定义层。","headline":"PyTorch 学习笔记（三）：自定义网络结构（nn.Module）","mainEntityOfPage":{"@type":"WebPage","@id":"https://pengxiang-wang.github.io/posts/studynotes_PyTorch_nnModule/"},"url":"https://pengxiang-wang.github.io/posts/studynotes_PyTorch_nnModule/"}</script><title>PyTorch 学习笔记（三）：自定义网络结构（nn.Module） | Shawn Wang</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Shawn Wang"><meta name="application-name" content="Shawn Wang"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src=" /assets/img/avatar.jpg " alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Shawn Wang</a></div><div class="site-subtitle font-italic">WPX 的个人主页</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>首页</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>分类</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>标签</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>时间表</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>关于本站</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/pengxiang-wang" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://space.bilibili.com/88684674" aria-label="bilibili" target="_blank" rel="noopener"> <i class="fas fa-tv"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['shawn.pxwang','qq.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper" class="row justify-content-center"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> 首页 </a> </span> <span>PyTorch 学习笔记（三）：自定义网络结构（nn.Module）</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> 文章</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="搜索..."> </span> <span id="search-cancel" >取消</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>PyTorch 学习笔记（三）：自定义网络结构（nn.Module）</h1><div class="post-meta text-muted"><div> 作者 <em> <a href="https://github.com/pengxiang-wang">Shawn Wang</a> </em></div><div class="d-flex"><div> <span> 发表于 <em class="timeago" data-ts="1644249600" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2022-02-08 </em> </span> <span> 更新于 <em class="timeago" data-ts="1685201191" data-toggle="tooltip" data-placement="bottom" data-tooltip-df="llll" > 2023-05-27 </em> </span> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2132 字"> <em>11 分钟</em>阅读</span></div></div></div><div class="post-content"><p>本文介绍如何自定义模型，即 <code class="language-plaintext highlighter-rouge">nn.Module</code> 模块的逻辑与使用方法。由此可以搭建自己的深度网络结构。本文参考 <a href="https://d2l.ai">Dive into Deep Learning (PyTorch 版)</a> 中的以下内容：</p><ul><li>5.1、5.6 节：层和块，自定义层。</ul><p><code class="language-plaintext highlighter-rouge">nn.Module</code> 官方文档：<a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html">https://pytorch.org/docs/stable/generated/torch.nn.Module.html</a></p><hr /><p>之前见到的所有高级 API 定义的模型全是使用 PyTorch 现有的模版：由 <code class="language-plaintext highlighter-rouge">nn.Sequential()</code> 包裹的 <code class="language-plaintext highlighter-rouge">nn.Linear()</code>, <code class="language-plaintext highlighter-rouge">nn.Flatten()</code>，用它们定义出的模型非常默认，不够灵活。设计更复杂的网络结构，这种默认的就无法满足需求了，需要自定义模型。</p><p>在逻辑层面上，所有网络模型都是由<strong>块</strong>（block）组成的，块与块之间可以有各种顺序、嵌套、并列等关系。块中包含一个或多个<strong>层</strong>（layer）。在 PyTorch 的语义中，模型最小单位不是神经元而是层。</p><h1 id="通用-nnmodule-模版">通用 nn.Module 模版</h1><p>在代码写法上，所有块、层都是 <code class="language-plaintext highlighter-rouge">nn.Module</code> 对象，包括 PyTorch 现有的 <code class="language-plaintext highlighter-rouge">nn.Linear()</code>,<code class="language-plaintext highlighter-rouge">nn.Flatten()</code> 甚至 <code class="language-plaintext highlighter-rouge">nn.Sequential()</code>。下面是通用的自定义 <code class="language-plaintext highlighter-rouge">nn.Module</code> 对象的写法。</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">MyModule</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="err">???</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">参数</span> <span class="o">=</span> <span class="p">...</span><span class="err">???</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">...</span> <span class="c1"># X 的表达式
</span></pre></table></code></div></div><p>定义的规则是，只要定义好前向传播 <code class="language-plaintext highlighter-rouge">forward</code> 函数，里面包含的是 torch 运算，再确保把这些运算的参数（即网络参数）封装到此模型的类中即可。<code class="language-plaintext highlighter-rouge">forward</code> 函数是核心，在自定义 <code class="language-plaintext highlighter-rouge">nn.Module</code> 对象时必须要写，<code class="language-plaintext highlighter-rouge">__init__()</code> 函数只是一个将 <code class="language-plaintext highlighter-rouge">forward</code> 函数所需变量绑定于此类的容器。下面做一个小试验，体会其重要性：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="n">M</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">()</span>
<span class="n">M</span><span class="p">()</span>
<span class="n">M</span><span class="p">.</span><span class="n">forward</span><span class="p">()</span>
</pre></table></code></div></div><p>这段代码第二、三行都会报 <code class="language-plaintext highlighter-rouge">NotImplementedError</code>，提示 <code class="language-plaintext highlighter-rouge">forward</code> 函数未定义。为什么会这样？<code class="language-plaintext highlighter-rouge">nn.Module</code> 模块的源代码解释清楚了：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">_forward_unimplemented</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">:</span> <span class="n">Any</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s">"Module [</span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="p">).</span><span class="n">__name__</span><span class="si">}</span><span class="s">] is missing the required </span><span class="se">\"</span><span class="s">forward</span><span class="se">\"</span><span class="s"> function"</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
    <span class="c1">#...
</span>    <span class="n">forward</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[...,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">_forward_unimplemented</span>

    <span class="k">def</span> <span class="nf">_call_impl</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">forward_call</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">_slow_forward</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">_C</span><span class="p">.</span><span class="n">_get_tracing_state</span><span class="p">()</span> <span class="k">else</span> <span class="bp">self</span><span class="p">.</span><span class="n">forward</span><span class="p">)</span>
        <span class="c1">#...
</span>        <span class="n">result</span> <span class="o">=</span> <span class="n">forward_call</span><span class="p">(</span><span class="o">*</span><span class="nb">input</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="c1">#...
</span>        <span class="k">return</span> <span class="n">result</span>

    <span class="n">__call__</span> <span class="p">:</span> <span class="n">Callable</span><span class="p">[...,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">_call_impl</span>
    <span class="c1">#...
</span></pre></table></code></div></div><p><code class="language-plaintext highlighter-rouge">nn.Module</code> 设计的机制就是要求继承时必须重写一个 <code class="language-plaintext highlighter-rouge">forward</code> 函数。此外，可以直接调用 <code class="language-plaintext highlighter-rouge">nn.Module</code> 对象，其实就是给定输入 \(x\) 前向传播一遍得到预测结果，它由 <code class="language-plaintext highlighter-rouge">__call__()</code> 方法定义（见 <a href="">Python 笔记</a>），而实现中可以看到出现了 <code class="language-plaintext highlighter-rouge">self.forward</code>，所以不写 <code class="language-plaintext highlighter-rouge">forward</code> 函数，在训练时前向传播也会报 <code class="language-plaintext highlighter-rouge">forward</code> 函数未定义的错误。</p><p>另外，在构造函数中必须调用父类 <code class="language-plaintext highlighter-rouge">nn.Module</code> 的构造函数：<code class="language-plaintext highlighter-rouge">super().__init__()</code>，为了把 <code class="language-plaintext highlighter-rouge">nn.Module</code> 定义的一些实例属性继承过来，只能这样写。写 <code class="language-plaintext highlighter-rouge">nn.Module</code> 对象时都要调用一下，否则会因缺少里面的属性报变量未定义的错误。感兴趣可以看看这些实例属性是什么：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">Module</span><span class="p">:</span>
    <span class="c1"># ...
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="s">"""
        Initializes internal Module state, shared by both nn.Module and ScriptModule.
        """</span>
        <span class="n">torch</span><span class="p">.</span><span class="n">_C</span><span class="p">.</span><span class="n">_log_api_usage_once</span><span class="p">(</span><span class="s">"python.nn_module"</span><span class="p">)</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">training</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_parameters</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Parameter</span><span class="p">]]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_buffers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_non_persistent_buffers_set</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_backward_hooks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_is_full_backward_hook</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_forward_hooks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_forward_pre_hooks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_state_dict_hooks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_load_state_dict_pre_hooks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_load_state_dict_post_hooks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="n">Callable</span><span class="p">]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">_modules</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="s">'Module'</span><span class="p">]]</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
    <span class="c1">#...
</span></pre></table></code></div></div><p>有了以上模版，我们可以使用 <code class="language-plaintext highlighter-rouge">nn.Module</code> 写一个层，也可以写一个块，乃至块组成的一整个模型，非常灵活。网络的结构取决于 <code class="language-plaintext highlighter-rouge">__init__()</code> 和 <code class="language-plaintext highlighter-rouge">forward</code> 函数怎么写。以下每种情况分别给出两段代码：一段是调用现有的模版，另一段是自己继承 <code class="language-plaintext highlighter-rouge">nn.Module</code> 手写出来的；这两段代码写出来的效果是一样的。</p><h1 id="自定义层">自定义层</h1><p>例 1：全连接层。 现有的模版：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
</pre></table></code></div></div><p>等价于以下自定义的层：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">MyLinear</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_num</span><span class="p">,</span> <span class="n">output_num</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">input_num</span><span class="p">,</span> <span class="n">output_num</span><span class="p">))</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">output_num</span><span class="p">,</span> <span class="p">))</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">weight</span><span class="p">.</span><span class="n">data</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="p">.</span><span class="n">bias</span><span class="p">.</span><span class="n">data</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">MyLinear</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
</pre></table></code></div></div><p>例 2：ReLU 激活函数层，它是一个不带参数的层。 现有的模版：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
</pre></table></code></div></div><p>等价于以下自定义的层：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">ReLULayer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></table></code></div></div><p>参数打包成 <code class="language-plaintext highlighter-rouge">nn.Parameter</code> 类后，直接定义为实例属性，<code class="language-plaintext highlighter-rouge">forward</code> 函数直接拿来用。实际使用时，一般很少自定义层，一般的网络都是使用那些常用层如全连接层、卷积层等，然后按照下面的方式组合成块。</p><h1 id="自定义块">自定义块</h1><p>例：多层感知机（MLP）。现有的模版：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
    <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</pre></table></code></div></div><p>等价于以下自定义的层：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">out</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden</span><span class="p">(</span><span class="n">X</span><span class="p">)))</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">MLP</span><span class="p">()</span>
</pre></table></code></div></div><p>也就是说，一些定义了层的 <code class="language-plaintext highlighter-rouge">nn.Module</code> 对象能以这种方式嵌套进定义了块的对象。</p><p>上面两者还有微小的区别：前者使用 <code class="language-plaintext highlighter-rouge">nn.Sequential</code>，用下标 <code class="language-plaintext highlighter-rouge">net[0]</code>,<code class="language-plaintext highlighter-rouge">net[1]</code>索引各层，还能把激活函数当作层索引到；后者的层是放在实例属性上的，需要用 <code class="language-plaintext highlighter-rouge">.</code> 来索引。</p><p><code class="language-plaintext highlighter-rouge">nn.Sequential</code> 是一种特殊的 <code class="language-plaintext highlighter-rouge">nn.Module</code> 对象，如上所述它能起到顺序连接各层、充当列表的效果。它的原理可以参考书中的简单复现，由此例可以体会到自定义 <code class="language-plaintext highlighter-rouge">nn.Module</code> 对象的灵活性：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">MySequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">_modules</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">idx</span><span class="p">)]</span> <span class="o">=</span> <span class="n">module</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">block</span> <span class="ow">in</span> <span class="bp">self</span><span class="p">.</span><span class="n">_modules</span><span class="p">.</span><span class="n">values</span><span class="p">():</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">block</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">X</span>
</pre></table></code></div></div><p>可以看到，它可以传入任意多个 <code class="language-plaintext highlighter-rouge">nn.Module</code> 对象（可变参数 <code class="language-plaintext highlighter-rouge">args</code>），将其顺序存储在 <code class="language-plaintext highlighter-rouge">_modules</code>（之前见过是 <code class="language-plaintext highlighter-rouge">nn.Module</code> 的实例属性，这里就派上用场了，是一个 <code class="language-plaintext highlighter-rouge">collections.OrderedDict</code> 容器），然后在 <code class="language-plaintext highlighter-rouge">forward</code> 函数中顺序复合到输入 X 上。（注意，这样使得激活函数也可作为可变参数传入。）</p><h1 id="自定义块组成的模型">自定义块组成的模型</h1><p><code class="language-plaintext highlighter-rouge">nn.Module</code> 对象是可以一层一层地递归嵌套地定义的。以下是一个稍复杂的例子：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="k">class</span> <span class="nc">NestMLP</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">().</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
                                 <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">())</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">net</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="n">chimera</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">NestMLP</span><span class="p">(),</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</pre></table></code></div></div><p>现在思考，为什么可以这样嵌套呢？因为在前向传播时，会一层一层递归地调用被嵌套模块的 <code class="language-plaintext highlighter-rouge">forward</code> 函数。例如此例，调用时 <code class="language-plaintext highlighter-rouge">chimera(X)</code> 时，首先会调用 <code class="language-plaintext highlighter-rouge">nn.Sequential</code> 的 <code class="language-plaintext highlighter-rouge">forward</code> 函数，即依次调用 <code class="language-plaintext highlighter-rouge">X = NestMLP()(X)</code>, <code class="language-plaintext highlighter-rouge">X = nn.ReLU()(X)</code>, <code class="language-plaintext highlighter-rouge">X = nn.Linear(16, 10)(X)</code>，前面说过每一层都会调用各自 <code class="language-plaintext highlighter-rouge">forward</code> 函数，例如先调用 <code class="language-plaintext highlighter-rouge">NestMLP()</code> 的 <code class="language-plaintext highlighter-rouge">forward</code>，其中调用了 <code class="language-plaintext highlighter-rouge">self.net()</code>, <code class="language-plaintext highlighter-rouge">self.linear()</code>，它们又会调用里面的 <code class="language-plaintext highlighter-rouge">nn.Sequential(...)</code>,<code class="language-plaintext highlighter-rouge">nn.Linear(32, 16)</code> 的 <code class="language-plaintext highlighter-rouge">forward</code> 函数。如此递归下去，直到遇到真正层里面的参数，例如 <code class="language-plaintext highlighter-rouge">self.linear</code> 里面的 <code class="language-plaintext highlighter-rouge">weight</code>。这种 <code class="language-plaintext highlighter-rouge">forward</code> 函数递归过程会把嵌套的每一块、层的参数都遍历到，从而能反向传播。此递归调用相当于遍历下面这颗树：</p><p><img data-src="/assets/img/NestMLP_called.png" alt="6" data-proofer-ignore></p><p>有人会问，为何不用简单的：</p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="已复制！"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">chimera</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
                        <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">32</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
                        <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="mi">16</span><span class="p">),</span> <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
                        <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
</pre></table></code></div></div><p>这就要从方便性的角度考虑了。这种方便不只在命名上（见下）。在逻辑上前者是将网络分成了几个块，例如本例有点像 NestMLP 是特征提取器，后面的全连接层是分类器的意思，假如以后想换个特征提取器更方便。</p><p>最后讨论一下命名问题。模型的每个块、层都有自己的名字（类似变量的命名空间），且可以通过以这个名字命名的实例属性访问。在这种封装的模型类中，嵌套关系的存在使得各块、层有树形关系。例如上面的 NestMLP 模型各块、层的名字如下：</p><p><img data-src="/assets/img/NestMLP_namespace.png" alt="6" data-proofer-ignore></p><blockquote class="prompt-tip"><div><p><code class="language-plaintext highlighter-rouge">print()</code> 函数能以一种规整的方式打印出网络结构（是由类特殊方法 <code class="language-plaintext highlighter-rouge">__print__()</code> 和 <code class="language-plaintext highlighter-rouge">__repr__()</code> 定义的，见 <a href="">Python 笔记</a>），会显示各块、层的名字、网络结构。也可以使用 Tensorboard 等工具可视化，见 <a href="">Tensorboard 笔记</a>。</p></div></blockquote></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/%E7%A7%91%E7%A0%94/'>科研</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/pytorch/" class="post-tag no-text-decoration" >PyTorch</a> <a href="/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/" class="post-tag no-text-decoration" >读书笔记</a> <a href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" class="post-tag no-text-decoration" >《动手学深度学习》</a> <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" class="post-tag no-text-decoration" >机器学习</a> <a href="/tags/%E6%8A%80%E6%9C%AF/" class="post-tag no-text-decoration" >技术</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> 本文由作者按照 <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> 进行授权，转载请注明</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">分享</span> <span class="share-icons"> <a href="http://service.weibo.com/share/share.php?title=PyTorch 学习笔记（三）：自定义网络结构（nn.Module） - Shawn Wang&amp;url=https://pengxiang-wang.github.io/posts/studynotes_PyTorch_nnModule/" data-toggle="tooltip" data-placement="top" title="Weibo" target="_blank" rel="noopener" aria-label="Weibo"> <i class="fa-fw fab fa-weibo"></i> </a> <a href="https://twitter.com/intent/tweet?text=PyTorch 学习笔记（三）：自定义网络结构（nn.Module） - Shawn Wang&amp;url=https://pengxiang-wang.github.io/posts/studynotes_PyTorch_nnModule/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=PyTorch 学习笔记（三）：自定义网络结构（nn.Module） - Shawn Wang&amp;u=https://pengxiang-wang.github.io/posts/studynotes_PyTorch_nnModule/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="分享链接" data-title-succeed="链接已复制！"> </i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">最近更新</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/accordion_transcribed_March-of-Steel-Torrent/">编配：《钢铁洪流进行曲》手风琴四重奏</a><li><a href="/posts/accordion_transcribed_Baikal-Lake/">编配：《贝加尔湖畔》手风琴独奏</a><li><a href="/posts/accordion_transcribed_Por-una-Cabeza/">编配：《一步之遥》手风琴四重奏</a><li><a href="/posts/accordion_transcribed_Katyusha/">编配：《喀秋莎》手风琴独奏</a><li><a href="/posts/accordion_transcribed_Miyazaki-Hayao-Movie-Themes/">编配：宫崎骏电影主题曲，手风琴二重奏</a></ul></div><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF/">技术</a> <a class="post-tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> <a class="post-tag" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a> <a class="post-tag" href="/tags/%E6%89%8B%E9%A3%8E%E7%90%B4/">手风琴</a> <a class="post-tag" href="/tags/%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0/">持续学习</a> <a class="post-tag" href="/tags/%E4%B9%90%E8%B0%B1/">乐谱</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/pytorch/">PyTorch</a> <a class="post-tag" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">《动手学深度学习》</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80/">语言</a></div></div></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="tail-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>相关文章</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/studynotes_PyTorch_autograd_and_pipeline/"><div class="card-body"> <em class="timeago small" data-ts="1642780800" > 2022-01-22 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PyTorch 学习笔记（一）：自动微分，简单模型的实现</h3><div class="text-muted small"><p> 本系列博文是我学习深度学习框架的学习笔记。深度学习框架大同小异，只须学习一种的原理，其他的都可以快速上手。我使用的是 PyTorch。笔记将着重强调代码原理、思想的理解，而不是具体的代码。 PyTorch 官方文档：https://pytorch.org/docs/stable/index.html PyTorch 中文文档：https://pytorch-cn.readthed...</p></div></div></a></div><div class="card"> <a href="/posts/studynotes_PyTorch_computing/"><div class="card-body"> <em class="timeago small" data-ts="1644508800" > 2022-02-11 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PyTorch 学习笔记（五）：计算性能</h3><div class="text-muted small"><p> 本文介绍 PyTorch 与计算性能有关的代码知识，包括如何使用 GPU、并行计算、多服务器计算等等。本文参考 Dive into Deep Learning (PyTorch 版) 中的以下内容： 5.6 节：GPU； 第 12 章：计算性能； 深度学习与 GPU 众所周知，深度学习计算可以使用 GPU，往往能极大提高效率。GPU 用于深度学习时与其他任务不同，它更偏向...</p></div></div></a></div><div class="card"> <a href="/posts/studynotes_PyTorch_Dataset_and_Transform/"><div class="card-body"> <em class="timeago small" data-ts="1642867200" > 2022-01-23 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>PyTorch 学习笔记（二）：自定义数据集，数据预处理</h3><div class="text-muted small"><p> 本文总结 PyTorch 中与数据集以及对它的预处理的知识。知乎的这篇文章讲得不错，言简意赅。也可参考官方教程。 PyTorch 中的数据集都是定义了一个 torch.utils.data.Dataset 类型，数据集都是这个类型的实例。必须这样做，因为后面构造 Dataloader 只接收 Dataset 类型，而整个训练过程都是对 Dataloader 的操作。我们已经在笔记（一）...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/studynotes_PyTorch_Dataset_and_Transform/" class="btn btn-outline-primary" prompt="上一篇"><p>PyTorch 学习笔记（二）：自定义数据集，数据预处理</p></a> <a href="/posts/studynotes_Minnan_pinyin/" class="btn btn-outline-primary" prompt="下一篇"><p>闽南语学习笔记：语音系统</p></a></div><script type="text/javascript"> $(function () { const origin = "https://giscus.app"; const iframe = "iframe.giscus-frame"; const lightTheme = "light"; const darkTheme = "dark_dimmed"; let initTheme = lightTheme; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches)) { initTheme = darkTheme; } let giscusAttributes = { "src": "https://giscus.app/client.js", "data-repo": "pengxiang-wang/pengxiang-wang.github.io", "data-repo-id": "R_kgDOHJZRFQ", "data-category": "Announcements", "data-category-id": "DIC_kwDOHJZRFc4COm2c", "data-mapping": "pathname", "data-reactions-enabled": "1", "data-emit-metadata": "0", "data-theme": initTheme, "data-input-position": "bottom", "data-lang": "zh-CN", "crossorigin": "anonymous", "async": "" }; let giscusScript = document.createElement("script"); Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value)); document.getElementById("tail-wrapper").appendChild(giscusScript); addEventListener("message", (event) => { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { /* global theme mode changed */ const mode = event.data.message; const theme = (mode === ModeToggle.DARK_MODE ? darkTheme : lightTheme); const message = { setConfig: { theme: theme } }; const giscus = document.querySelector(iframe).contentWindow; giscus.postMessage({ giscus: message }, origin); } }); }); </script></div></div></div><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center text-muted"><div class="footer-left"><p class="mb-0"> © 2024 <a href="https://github.com/pengxiang-wang">Shawn Wang</a>.</p></div><div class="footer-right"><p class="mb-0"> KEEP CALM & CARRY ON</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">热门标签</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/%E6%8A%80%E6%9C%AF/">技术</a> <a class="post-tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a> <a class="post-tag" href="/tags/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">学习笔记</a> <a class="post-tag" href="/tags/%E6%89%8B%E9%A3%8E%E7%90%B4/">手风琴</a> <a class="post-tag" href="/tags/%E6%8C%81%E7%BB%AD%E5%AD%A6%E4%B9%A0/">持续学习</a> <a class="post-tag" href="/tags/%E4%B9%90%E8%B0%B1/">乐谱</a> <a class="post-tag" href="/tags/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/">论文笔记</a> <a class="post-tag" href="/tags/pytorch/">PyTorch</a> <a class="post-tag" href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">《动手学深度学习》</a> <a class="post-tag" href="/tags/%E8%AF%AD%E8%A8%80/">语言</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">搜索结果为空</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/zh-cn.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-H7GH1F7FH5"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-H7GH1F7FH5'); }); </script>
